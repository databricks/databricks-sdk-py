openapi: 3.0.0
info:
  title: Databricks Compute API
  description: API for Databricks compute service
  version: 1.0.0
  contact:
    name: Databricks
    url: https://databricks.com
servers:
- url: https://{deployment_name}.cloud.databricks.com
  description: Databricks Workspace API
  variables:
    deployment_name:
      default: your-deployment
      description: Databricks workspace deployment name
paths:
  /api/2.0/policies/clusters/create:
    post:
      operationId: create
      summary: Creates a new policy with prescribed settings.
      description: "Creates a new policy with prescribed settings.\n\n:param definition:\
        \ str (optional)\n  Policy definition document expressed in [Databricks Cluster\
        \ Policy Definition Language].\n\n  [Databricks Cluster Policy Definition\
        \ Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n\
        :param description: str (optional)\n  Additional human-readable description\
        \ of the cluster policy.\n:param libraries: List[:class:`Library`] (optional)\n\
        \  A list of libraries to be installed on the next cluster restart that uses\
        \ this policy. The maximum\n  number of libraries is 500.\n:param max_clusters_per_user:\
        \ int (optional)\n  Max number of clusters per user that can be active using\
        \ this policy. If not present, there is no\n  max limit.\n:param name: str\
        \ (optional)\n  Cluster Policy name requested by the user. This has to be\
        \ unique. Length must be between 1 and 100\n  characters.\n:param policy_family_definition_overrides:\
        \ str (optional)\n  Policy definition JSON document expressed in [Databricks\
        \ Policy Definition Language]. The JSON\n  document must be passed as a string\
        \ and cannot be embedded in the requests.\n\n  You can use this to customize\
        \ the policy definition inherited from the policy family. Policy rules\n \
        \ specified here are merged into the inherited policy definition.\n\n  [Databricks\
        \ Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n\
        :param policy_family_id: str (optional)\n  ID of the policy family. The cluster\
        \ policy's policy definition inherits the policy family's policy\n  definition.\n\
        \n  Cannot be used with `definition`. Use `policy_family_definition_overrides`\
        \ instead to customize the\n  policy definition.\n\n:returns: :class:`CreatePolicyResponse`"
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                definition:
                  type: string
                  description: 'str (optional) Policy definition document expressed
                    in [Databricks Cluster Policy Definition Language]. [Databricks
                    Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
                description:
                  type: string
                  description: str (optional) Additional human-readable description
                    of the cluster policy.
                libraries:
                  type: array
                  items:
                    $ref: '#/components/schemas/Library'
                  description: List[:class:`Library`] (optional) A list of libraries
                    to be installed on the next cluster restart that uses this policy.
                    The maximum number of libraries is 500.
                max_clusters_per_user:
                  type: integer
                  description: int (optional) Max number of clusters per user that
                    can be active using this policy. If not present, there is no max
                    limit.
                name:
                  type: string
                  description: str (optional) Cluster Policy name requested by the
                    user. This has to be unique. Length must be between 1 and 100
                    characters.
                policy_family_definition_overrides:
                  type: string
                  description: 'str (optional) Policy definition JSON document expressed
                    in [Databricks Policy Definition Language]. The JSON document
                    must be passed as a string and cannot be embedded in the requests.
                    You can use this to customize the policy definition inherited
                    from the policy family. Policy rules specified here are merged
                    into the inherited policy definition. [Databricks Policy Definition
                    Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
                policy_family_id:
                  type: string
                  description: str (optional) ID of the policy family. The cluster
                    policy's policy definition inherits the policy family's policy
                    definition. Cannot be used with `definition`. Use `policy_family_definition_overrides`
                    instead to customize the policy definition.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreatePolicyResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policies/clusters/delete:
    post:
      operationId: delete
      summary: Delete a policy for a cluster. Clusters governed by this policy can
        still run, but cannot be edited.
      description: "Delete a policy for a cluster. Clusters governed by this policy\
        \ can still run, but cannot be edited.\n\n:param policy_id: str\n  The ID\
        \ of the policy to delete."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                policy_id:
                  type: string
                  description: str The ID of the policy to delete.
      responses:
        '200':
          description: Success
  /api/2.0/policies/clusters/edit:
    post:
      operationId: edit
      summary: 'Update an existing policy for cluster. This operation may make some
        clusters governed by the previous

        policy invalid.'
      description: "Update an existing policy for cluster. This operation may make\
        \ some clusters governed by the previous\npolicy invalid.\n\n:param policy_id:\
        \ str\n  The ID of the policy to update.\n:param definition: str (optional)\n\
        \  Policy definition document expressed in [Databricks Cluster Policy Definition\
        \ Language].\n\n  [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n\
        :param description: str (optional)\n  Additional human-readable description\
        \ of the cluster policy.\n:param libraries: List[:class:`Library`] (optional)\n\
        \  A list of libraries to be installed on the next cluster restart that uses\
        \ this policy. The maximum\n  number of libraries is 500.\n:param max_clusters_per_user:\
        \ int (optional)\n  Max number of clusters per user that can be active using\
        \ this policy. If not present, there is no\n  max limit.\n:param name: str\
        \ (optional)\n  Cluster Policy name requested by the user. This has to be\
        \ unique. Length must be between 1 and 100\n  characters.\n:param policy_family_definition_overrides:\
        \ str (optional)\n  Policy definition JSON document expressed in [Databricks\
        \ Policy Definition Language]. The JSON\n  document must be passed as a string\
        \ and cannot be embedded in the requests.\n\n  You can use this to customize\
        \ the policy definition inherited from the policy family. Policy rules\n \
        \ specified here are merged into the inherited policy definition.\n\n  [Databricks\
        \ Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n\
        :param policy_family_id: str (optional)\n  ID of the policy family. The cluster\
        \ policy's policy definition inherits the policy family's policy\n  definition.\n\
        \n  Cannot be used with `definition`. Use `policy_family_definition_overrides`\
        \ instead to customize the\n  policy definition."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                policy_id:
                  type: string
                  description: str The ID of the policy to update.
                definition:
                  type: string
                  description: 'str (optional) Policy definition document expressed
                    in [Databricks Cluster Policy Definition Language]. [Databricks
                    Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
                description:
                  type: string
                  description: str (optional) Additional human-readable description
                    of the cluster policy.
                libraries:
                  type: array
                  items:
                    $ref: '#/components/schemas/Library'
                  description: List[:class:`Library`] (optional) A list of libraries
                    to be installed on the next cluster restart that uses this policy.
                    The maximum number of libraries is 500.
                max_clusters_per_user:
                  type: integer
                  description: int (optional) Max number of clusters per user that
                    can be active using this policy. If not present, there is no max
                    limit.
                name:
                  type: string
                  description: str (optional) Cluster Policy name requested by the
                    user. This has to be unique. Length must be between 1 and 100
                    characters.
                policy_family_definition_overrides:
                  type: string
                  description: 'str (optional) Policy definition JSON document expressed
                    in [Databricks Policy Definition Language]. The JSON document
                    must be passed as a string and cannot be embedded in the requests.
                    You can use this to customize the policy definition inherited
                    from the policy family. Policy rules specified here are merged
                    into the inherited policy definition. [Databricks Policy Definition
                    Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
                policy_family_id:
                  type: string
                  description: str (optional) ID of the policy family. The cluster
                    policy's policy definition inherits the policy family's policy
                    definition. Cannot be used with `definition`. Use `policy_family_definition_overrides`
                    instead to customize the policy definition.
      responses:
        '200':
          description: Success
  /api/2.0/policies/clusters/get:
    get:
      operationId: get
      summary: Get a cluster policy entity. Creation and editing is available to admins
        only.
      description: "Get a cluster policy entity. Creation and editing is available\
        \ to admins only.\n\n:param policy_id: str\n  Canonical unique identifier\
        \ for the Cluster Policy.\n\n:returns: :class:`Policy`"
      tags:
      - compute
      parameters:
      - name: policy_id
        description: str Canonical unique identifier for the Cluster Policy.
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/permissions/cluster-policies/{cluster_policy_id}:
    get:
      operationId: get_permissions
      summary: 'Gets the permissions of a cluster policy. Cluster policies can inherit
        permissions from their root

        object.'
      description: "Gets the permissions of a cluster policy. Cluster policies can\
        \ inherit permissions from their root\nobject.\n\n:param cluster_policy_id:\
        \ str\n  The cluster policy for which to get or manage permissions.\n\n:returns:\
        \ :class:`ClusterPolicyPermissions`"
      tags:
      - compute
      parameters:
      - name: cluster_policy_id
        description: str The cluster policy for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPolicyPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policies/clusters/list:
    get:
      operationId: list
      summary: Returns a list of policies accessible by the requesting user.
      description: "Returns a list of policies accessible by the requesting user.\n\
        \n:param sort_column: :class:`ListSortColumn` (optional)\n  The cluster policy\
        \ attribute to sort by. * `POLICY_CREATION_TIME` - Sort result list by policy\n\
        \  creation time. * `POLICY_NAME` - Sort result list by policy name.\n:param\
        \ sort_order: :class:`ListSortOrder` (optional)\n  The order in which the\
        \ policies get listed. * `DESC` - Sort result list in descending order. *\
        \ `ASC`\n  - Sort result list in ascending order.\n\n:returns: Iterator over\
        \ :class:`Policy`"
      tags:
      - compute
      parameters:
      - name: sort_column
        description: :class:`ListSortColumn` (optional) The cluster policy attribute
          to sort by. * `POLICY_CREATION_TIME` - Sort result list by policy creation
          time. * `POLICY_NAME` - Sort result list by policy name.
        required: false
        schema:
          type: string
          enum:
          - POLICY_CREATION_TIME
          - POLICY_NAME
        in: query
      - name: sort_order
        description: :class:`ListSortOrder` (optional) The order in which the policies
          get listed. * `DESC` - Sort result list in descending order. * `ASC` - Sort
          result list in ascending order.
        required: false
        schema:
          type: string
          enum:
          - ASC
          - DESC
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/change-owner:
    post:
      operationId: change_owner
      summary: 'Change the owner of the cluster. You must be an admin and the cluster
        must be terminated to perform

        this operation. The service principal application ID can be supplied as an
        argument to

        `owner_username`.'
      description: "Change the owner of the cluster. You must be an admin and the\
        \ cluster must be terminated to perform\nthis operation. The service principal\
        \ application ID can be supplied as an argument to\n`owner_username`.\n\n\
        :param cluster_id: str\n:param owner_username: str\n  New owner of the cluster_id\
        \ after this RPC."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str
                owner_username:
                  type: string
                  description: str New owner of the cluster_id after this RPC.
      responses:
        '200':
          description: Success
  /api/2.1/clusters/create:
    post:
      operationId: create
      summary: 'Creates a new Spark cluster. This method will acquire new instances
        from the cloud provider if

        necessary. This method is asynchronous; the returned ``cluster_id`` can be
        used to poll the cluster

        status. When this method returns, the cluster will be in a ``PENDING`` state.
        The cluster will be

        usable once it enters a ``RUNNING`` state. Note: Databricks may not be able
        to acquire some of the

        requested nodes, due to cloud provider limitations (account limits, spot price,
        etc.) or transient

        network issues.'
      description: "Creates a new Spark cluster. This method will acquire new instances\
        \ from the cloud provider if\nnecessary. This method is asynchronous; the\
        \ returned ``cluster_id`` can be used to poll the cluster\nstatus. When this\
        \ method returns, the cluster will be in a ``PENDING`` state. The cluster\
        \ will be\nusable once it enters a ``RUNNING`` state. Note: Databricks may\
        \ not be able to acquire some of the\nrequested nodes, due to cloud provider\
        \ limitations (account limits, spot price, etc.) or transient\nnetwork issues.\n\
        \nIf Databricks acquires at least 85% of the requested on-demand nodes, cluster\
        \ creation will succeed.\nOtherwise the cluster will terminate with an informative\
        \ error message.\n\nRather than authoring the cluster's JSON definition from\
        \ scratch, Databricks recommends filling out\nthe [create compute UI] and\
        \ then copying the generated JSON definition from the UI.\n\n[create compute\
        \ UI]: https://docs.databricks.com/compute/configure.html\n\n:param spark_version:\
        \ str\n  The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list\
        \ of available Spark versions can be\n  retrieved by using the :method:clusters/sparkVersions\
        \ API call.\n:param apply_policy_default_values: bool (optional)\n  When set\
        \ to true, fixed and default values from the policy will be used for fields\
        \ that are omitted.\n  When set to false, only fixed values from the policy\
        \ will be applied.\n:param autoscale: :class:`AutoScale` (optional)\n  Parameters\
        \ needed in order to automatically scale clusters up and down based on load.\
        \ Note:\n  autoscaling works best with DB runtime versions 3.0 or later.\n\
        :param autotermination_minutes: int (optional)\n  Automatically terminates\
        \ the cluster after it is inactive for this time in minutes. If not set, this\n\
        \  cluster will not be automatically terminated. If specified, the threshold\
        \ must be between 10 and\n  10000 minutes. Users can also set this value to\
        \ 0 to explicitly disable automatic termination.\n:param aws_attributes: :class:`AwsAttributes`\
        \ (optional)\n  Attributes related to clusters running on Amazon Web Services.\
        \ If not specified at cluster creation,\n  a set of default values will be\
        \ used.\n:param azure_attributes: :class:`AzureAttributes` (optional)\n  Attributes\
        \ related to clusters running on Microsoft Azure. If not specified at cluster\
        \ creation, a\n  set of default values will be used.\n:param clone_from: :class:`CloneCluster`\
        \ (optional)\n  When specified, this clones libraries from a source cluster\
        \ during the creation of a new cluster.\n:param cluster_log_conf: :class:`ClusterLogConf`\
        \ (optional)\n  The configuration for delivering spark logs to a long-term\
        \ storage destination. Three kinds of\n  destinations (DBFS, S3 and Unity\
        \ Catalog volumes) are supported. Only one destination can be\n  specified\
        \ for one cluster. If the conf is given, the logs will be delivered to the\
        \ destination every\n  `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`,\
        \ while the destination\n  of executor logs is `$destination/$clusterId/executor`.\n\
        :param cluster_name: str (optional)\n  Cluster name requested by the user.\
        \ This doesn't have to be unique. If not specified at creation,\n  the cluster\
        \ name will be an empty string. For job clusters, the cluster name is automatically\
        \ set\n  based on the job and job run IDs.\n:param custom_tags: Dict[str,str]\
        \ (optional)\n  Additional tags for cluster resources. Databricks will tag\
        \ all cluster resources (e.g., AWS\n  instances and EBS volumes) with these\
        \ tags in addition to `default_tags`. Notes:\n\n  - Currently, Databricks\
        \ allows at most 45 custom tags\n\n  - Clusters can only reuse cloud resources\
        \ if the resources' tags are a subset of the cluster tags\n:param data_security_mode:\
        \ :class:`DataSecurityMode` (optional)\n:param docker_image: :class:`DockerImage`\
        \ (optional)\n  Custom docker image BYOC\n:param driver_instance_pool_id:\
        \ str (optional)\n  The optional ID of the instance pool for the driver of\
        \ the cluster belongs. The pool cluster uses\n  the instance pool with id\
        \ (instance_pool_id) if the driver pool is not assigned.\n:param driver_node_type_id:\
        \ str (optional)\n  The node type of the Spark driver. Note that this field\
        \ is optional; if unset, the driver node type\n  will be set as the same value\
        \ as `node_type_id` defined above.\n\n  This field, along with node_type_id,\
        \ should not be set if virtual_cluster_size is set. If both\n  driver_node_type_id,\
        \ node_type_id, and virtual_cluster_size are specified, driver_node_type_id\
        \ and\n  node_type_id take precedence.\n:param enable_elastic_disk: bool (optional)\n\
        \  Autoscaling Local Storage: when enabled, this cluster will dynamically\
        \ acquire additional disk space\n  when its Spark workers are running low\
        \ on disk space. This feature requires specific AWS permissions\n  to function\
        \ correctly - refer to the User Guide for more details.\n:param enable_local_disk_encryption:\
        \ bool (optional)\n  Whether to enable LUKS on cluster VMs' local disks\n\
        :param gcp_attributes: :class:`GcpAttributes` (optional)\n  Attributes related\
        \ to clusters running on Google Cloud Platform. If not specified at cluster\n\
        \  creation, a set of default values will be used.\n:param init_scripts: List[:class:`InitScriptInfo`]\
        \ (optional)\n  The configuration for storing init scripts. Any number of\
        \ destinations can be specified. The scripts\n  are executed sequentially\
        \ in the order provided. If `cluster_log_conf` is specified, init script\n\
        \  logs are sent to `<destination>/<cluster-ID>/init_scripts`.\n:param instance_pool_id:\
        \ str (optional)\n  The optional ID of the instance pool to which the cluster\
        \ belongs.\n:param is_single_node: bool (optional)\n  This field can only\
        \ be used when `kind = CLASSIC_PREVIEW`.\n\n  When set to true, Databricks\
        \ will automatically set single node related `custom_tags`, `spark_conf`,\n\
        \  and `num_workers`\n:param kind: :class:`Kind` (optional)\n:param node_type_id:\
        \ str (optional)\n  This field encodes, through a single value, the resources\
        \ available to each of the Spark nodes in\n  this cluster. For example, the\
        \ Spark nodes can be provisioned and optimized for memory or compute\n  intensive\
        \ workloads. A list of available node types can be retrieved by using the\n\
        \  :method:clusters/listNodeTypes API call.\n:param num_workers: int (optional)\n\
        \  Number of worker nodes that this cluster should have. A cluster has one\
        \ Spark Driver and\n  `num_workers` Executors for a total of `num_workers`\
        \ + 1 Spark nodes.\n\n  Note: When reading the properties of a cluster, this\
        \ field reflects the desired number of workers\n  rather than the actual current\
        \ number of workers. For instance, if a cluster is resized from 5 to 10\n\
        \  workers, this field will immediately be updated to reflect the target size\
        \ of 10 workers, whereas\n  the workers listed in `spark_info` will gradually\
        \ increase from 5 to 10 as the new nodes are\n  provisioned.\n:param policy_id:\
        \ str (optional)\n  The ID of the cluster policy used to create the cluster\
        \ if applicable.\n:param remote_disk_throughput: int (optional)\n  If set,\
        \ what the configurable throughput (in Mb/s) for the remote disk is. Currently\
        \ only supported\n  for GCP HYPERDISK_BALANCED disks.\n:param runtime_engine:\
        \ :class:`RuntimeEngine` (optional)\n  Determines the cluster's runtime engine,\
        \ either standard or Photon.\n\n  This field is not compatible with legacy\
        \ `spark_version` values that contain `-photon-`. Remove\n  `-photon-` from\
        \ the `spark_version` and set `runtime_engine` to `PHOTON`.\n\n  If left unspecified,\
        \ the runtime engine defaults to standard unless the spark_version contains\n\
        \  -photon-, in which case Photon will be used.\n:param single_user_name:\
        \ str (optional)\n  Single user name if data_security_mode is `SINGLE_USER`\n\
        :param spark_conf: Dict[str,str] (optional)\n  An object containing a set\
        \ of optional, user-specified Spark configuration key-value pairs. Users\n\
        \  can also pass in a string of extra JVM options to the driver and the executors\
        \ via\n  `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`\
        \ respectively.\n:param spark_env_vars: Dict[str,str] (optional)\n  An object\
        \ containing a set of optional, user-specified environment variable key-value\
        \ pairs. Please\n  note that key-value pair of the form (X,Y) will be exported\
        \ as is (i.e., `export X='Y'`) while\n  launching the driver and workers.\n\
        \n  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we\
        \ recommend appending them to\n  `$SPARK_DAEMON_JAVA_OPTS` as shown in the\
        \ example below. This ensures that all default databricks\n  managed environmental\
        \ variables are included as well.\n\n  Example Spark environment variables:\
        \ `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\":\n  \"/local_disk0\"\
        }` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS\n  -Dspark.shuffle.service.enabled=true\"\
        }`\n:param ssh_public_keys: List[str] (optional)\n  SSH public key contents\
        \ that will be added to each Spark node in this cluster. The corresponding\n\
        \  private keys can be used to login with the user name `ubuntu` on port `2200`.\
        \ Up to 10 keys can be\n  specified.\n:param total_initial_remote_disk_size:\
        \ int (optional)\n  If set, what the total initial volume size (in GB) of\
        \ the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED\
        \ disks.\n:param use_ml_runtime: bool (optional)\n  This field can only be\
        \ used when `kind = CLASSIC_PREVIEW`.\n\n  `effective_spark_version` is determined\
        \ by `spark_version` (DBR release), this field\n  `use_ml_runtime`, and whether\
        \ `node_type_id` is gpu node or not.\n:param workload_type: :class:`WorkloadType`\
        \ (optional)\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n\
        \  See :method:wait_get_cluster_running for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                spark_version:
                  type: string
                  description: str The Spark version of the cluster, e.g. `3.3.x-scala2.11`.
                    A list of available Spark versions can be retrieved by using the
                    :method:clusters/sparkVersions API call.
                apply_policy_default_values:
                  type: boolean
                  description: bool (optional) When set to true, fixed and default
                    values from the policy will be used for fields that are omitted.
                    When set to false, only fixed values from the policy will be applied.
                autoscale:
                  $ref: '#/components/schemas/AutoScale'
                autotermination_minutes:
                  type: integer
                  description: int (optional) Automatically terminates the cluster
                    after it is inactive for this time in minutes. If not set, this
                    cluster will not be automatically terminated. If specified, the
                    threshold must be between 10 and 10000 minutes. Users can also
                    set this value to 0 to explicitly disable automatic termination.
                aws_attributes:
                  $ref: '#/components/schemas/AwsAttributes'
                azure_attributes:
                  $ref: '#/components/schemas/AzureAttributes'
                clone_from:
                  $ref: '#/components/schemas/CloneCluster'
                cluster_log_conf:
                  $ref: '#/components/schemas/ClusterLogConf'
                cluster_name:
                  type: string
                  description: str (optional) Cluster name requested by the user.
                    This doesn't have to be unique. If not specified at creation,
                    the cluster name will be an empty string. For job clusters, the
                    cluster name is automatically set based on the job and job run
                    IDs.
                custom_tags:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) Additional tags for cluster
                    resources. Databricks will tag all cluster resources (e.g., AWS
                    instances and EBS volumes) with these tags in addition to `default_tags`.
                    Notes: - Currently, Databricks allows at most 45 custom tags -
                    Clusters can only reuse cloud resources if the resources'' tags
                    are a subset of the cluster tags'
                data_security_mode:
                  type: string
                  enum:
                  - DATA_SECURITY_MODE_AUTO
                  - DATA_SECURITY_MODE_DEDICATED
                  - DATA_SECURITY_MODE_STANDARD
                  - LEGACY_PASSTHROUGH
                  - LEGACY_SINGLE_USER
                  - LEGACY_SINGLE_USER_STANDARD
                  - LEGACY_TABLE_ACL
                  - NONE
                  - SINGLE_USER
                  - USER_ISOLATION
                  description: :class:`DataSecurityMode` (optional)
                docker_image:
                  $ref: '#/components/schemas/DockerImage'
                driver_instance_pool_id:
                  type: string
                  description: str (optional) The optional ID of the instance pool
                    for the driver of the cluster belongs. The pool cluster uses the
                    instance pool with id (instance_pool_id) if the driver pool is
                    not assigned.
                driver_node_type_id:
                  type: string
                  description: str (optional) The node type of the Spark driver. Note
                    that this field is optional; if unset, the driver node type will
                    be set as the same value as `node_type_id` defined above. This
                    field, along with node_type_id, should not be set if virtual_cluster_size
                    is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size
                    are specified, driver_node_type_id and node_type_id take precedence.
                enable_elastic_disk:
                  type: boolean
                  description: 'bool (optional) Autoscaling Local Storage: when enabled,
                    this cluster will dynamically acquire additional disk space when
                    its Spark workers are running low on disk space. This feature
                    requires specific AWS permissions to function correctly - refer
                    to the User Guide for more details.'
                enable_local_disk_encryption:
                  type: boolean
                  description: bool (optional) Whether to enable LUKS on cluster VMs'
                    local disks
                gcp_attributes:
                  $ref: '#/components/schemas/GcpAttributes'
                init_scripts:
                  type: array
                  items:
                    $ref: '#/components/schemas/InitScriptInfo'
                  description: List[:class:`InitScriptInfo`] (optional) The configuration
                    for storing init scripts. Any number of destinations can be specified.
                    The scripts are executed sequentially in the order provided. If
                    `cluster_log_conf` is specified, init script logs are sent to
                    `<destination>/<cluster-ID>/init_scripts`.
                instance_pool_id:
                  type: string
                  description: str (optional) The optional ID of the instance pool
                    to which the cluster belongs.
                is_single_node:
                  type: boolean
                  description: bool (optional) This field can only be used when `kind
                    = CLASSIC_PREVIEW`. When set to true, Databricks will automatically
                    set single node related `custom_tags`, `spark_conf`, and `num_workers`
                kind:
                  type: string
                  enum:
                  - CLASSIC_PREVIEW
                  description: :class:`Kind` (optional)
                node_type_id:
                  type: string
                  description: str (optional) This field encodes, through a single
                    value, the resources available to each of the Spark nodes in this
                    cluster. For example, the Spark nodes can be provisioned and optimized
                    for memory or compute intensive workloads. A list of available
                    node types can be retrieved by using the
                num_workers:
                  type: integer
                  description: 'int (optional) Number of worker nodes that this cluster
                    should have. A cluster has one Spark Driver and `num_workers`
                    Executors for a total of `num_workers` + 1 Spark nodes. Note:
                    When reading the properties of a cluster, this field reflects
                    the desired number of workers rather than the actual current number
                    of workers. For instance, if a cluster is resized from 5 to 10
                    workers, this field will immediately be updated to reflect the
                    target size of 10 workers, whereas the workers listed in `spark_info`
                    will gradually increase from 5 to 10 as the new nodes are provisioned.'
                policy_id:
                  type: string
                  description: str (optional) The ID of the cluster policy used to
                    create the cluster if applicable.
                remote_disk_throughput:
                  type: integer
                  description: int (optional) If set, what the configurable throughput
                    (in Mb/s) for the remote disk is. Currently only supported for
                    GCP HYPERDISK_BALANCED disks.
                runtime_engine:
                  type: string
                  enum:
                  - 'NULL'
                  - PHOTON
                  - STANDARD
                  description: :class:`RuntimeEngine` (optional) Determines the cluster's
                    runtime engine, either standard or Photon. This field is not compatible
                    with legacy `spark_version` values that contain `-photon-`. Remove
                    `-photon-` from the `spark_version` and set `runtime_engine` to
                    `PHOTON`. If left unspecified, the runtime engine defaults to
                    standard unless the spark_version contains -photon-, in which
                    case Photon will be used.
                single_user_name:
                  type: string
                  description: str (optional) Single user name if data_security_mode
                    is `SINGLE_USER`
                spark_conf:
                  type: object
                  additionalProperties: true
                  description: Dict[str,str] (optional) An object containing a set
                    of optional, user-specified Spark configuration key-value pairs.
                    Users can also pass in a string of extra JVM options to the driver
                    and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`
                    respectively.
                spark_env_vars:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) An object containing a set
                    of optional, user-specified environment variable key-value pairs.
                    Please note that key-value pair of the form (X,Y) will be exported
                    as is (i.e., `export X=''Y''`) while launching the driver and
                    workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`,
                    we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown
                    in the example below. This ensures that all default databricks
                    managed environmental variables are included as well. Example
                    Spark environment variables: `{"SPARK_WORKER_MEMORY": "28000m",
                    "SPARK_LOCAL_DIRS": "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS":
                    "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`'
                ssh_public_keys:
                  type: array
                  items:
                    type: string
                  description: List[str] (optional) SSH public key contents that will
                    be added to each Spark node in this cluster. The corresponding
                    private keys can be used to login with the user name `ubuntu`
                    on port `2200`. Up to 10 keys can be specified.
                total_initial_remote_disk_size:
                  type: integer
                  description: int (optional) If set, what the total initial volume
                    size (in GB) of the remote disks should be. Currently only supported
                    for GCP HYPERDISK_BALANCED disks.
                use_ml_runtime:
                  type: boolean
                  description: bool (optional) This field can only be used when `kind
                    = CLASSIC_PREVIEW`. `effective_spark_version` is determined by
                    `spark_version` (DBR release), this field `use_ml_runtime`, and
                    whether `node_type_id` is gpu node or not.
                workload_type:
                  $ref: '#/components/schemas/WorkloadType'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/delete:
    post:
      operationId: delete
      summary: 'Terminates the Spark cluster with the specified ID. The cluster is
        removed asynchronously. Once the

        termination has completed, the cluster will be in a `TERMINATED` state. If
        the cluster is already in a

        `TERMINATING` or `TERMINATED` state, nothing will happen.'
      description: "Terminates the Spark cluster with the specified ID. The cluster\
        \ is removed asynchronously. Once the\ntermination has completed, the cluster\
        \ will be in a `TERMINATED` state. If the cluster is already in a\n`TERMINATING`\
        \ or `TERMINATED` state, nothing will happen.\n\n:param cluster_id: str\n\
        \  The cluster to be terminated.\n\n:returns:\n  Long-running operation waiter\
        \ for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_terminated\
        \ for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The cluster to be terminated.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/edit:
    post:
      operationId: edit
      summary: 'Updates the configuration of a cluster to match the provided attributes
        and size. A cluster can be

        updated if it is in a `RUNNING` or `TERMINATED` state.'
      description: "Updates the configuration of a cluster to match the provided attributes\
        \ and size. A cluster can be\nupdated if it is in a `RUNNING` or `TERMINATED`\
        \ state.\n\nIf a cluster is updated while in a `RUNNING` state, it will be\
        \ restarted so that the new attributes\ncan take effect.\n\nIf a cluster is\
        \ updated while in a `TERMINATED` state, it will remain `TERMINATED`. The\
        \ next time it\nis started using the `clusters/start` API, the new attributes\
        \ will take effect. Any attempt to update\na cluster in any other state will\
        \ be rejected with an `INVALID_STATE` error code.\n\nClusters created by the\
        \ Databricks Jobs service cannot be edited.\n\n:param cluster_id: str\n  ID\
        \ of the cluster\n:param spark_version: str\n  The Spark version of the cluster,\
        \ e.g. `3.3.x-scala2.11`. A list of available Spark versions can be\n  retrieved\
        \ by using the :method:clusters/sparkVersions API call.\n:param apply_policy_default_values:\
        \ bool (optional)\n  When set to true, fixed and default values from the policy\
        \ will be used for fields that are omitted.\n  When set to false, only fixed\
        \ values from the policy will be applied.\n:param autoscale: :class:`AutoScale`\
        \ (optional)\n  Parameters needed in order to automatically scale clusters\
        \ up and down based on load. Note:\n  autoscaling works best with DB runtime\
        \ versions 3.0 or later.\n:param autotermination_minutes: int (optional)\n\
        \  Automatically terminates the cluster after it is inactive for this time\
        \ in minutes. If not set, this\n  cluster will not be automatically terminated.\
        \ If specified, the threshold must be between 10 and\n  10000 minutes. Users\
        \ can also set this value to 0 to explicitly disable automatic termination.\n\
        :param aws_attributes: :class:`AwsAttributes` (optional)\n  Attributes related\
        \ to clusters running on Amazon Web Services. If not specified at cluster\
        \ creation,\n  a set of default values will be used.\n:param azure_attributes:\
        \ :class:`AzureAttributes` (optional)\n  Attributes related to clusters running\
        \ on Microsoft Azure. If not specified at cluster creation, a\n  set of default\
        \ values will be used.\n:param cluster_log_conf: :class:`ClusterLogConf` (optional)\n\
        \  The configuration for delivering spark logs to a long-term storage destination.\
        \ Three kinds of\n  destinations (DBFS, S3 and Unity Catalog volumes) are\
        \ supported. Only one destination can be\n  specified for one cluster. If\
        \ the conf is given, the logs will be delivered to the destination every\n\
        \  `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`,\
        \ while the destination\n  of executor logs is `$destination/$clusterId/executor`.\n\
        :param cluster_name: str (optional)\n  Cluster name requested by the user.\
        \ This doesn't have to be unique. If not specified at creation,\n  the cluster\
        \ name will be an empty string. For job clusters, the cluster name is automatically\
        \ set\n  based on the job and job run IDs.\n:param custom_tags: Dict[str,str]\
        \ (optional)\n  Additional tags for cluster resources. Databricks will tag\
        \ all cluster resources (e.g., AWS\n  instances and EBS volumes) with these\
        \ tags in addition to `default_tags`. Notes:\n\n  - Currently, Databricks\
        \ allows at most 45 custom tags\n\n  - Clusters can only reuse cloud resources\
        \ if the resources' tags are a subset of the cluster tags\n:param data_security_mode:\
        \ :class:`DataSecurityMode` (optional)\n:param docker_image: :class:`DockerImage`\
        \ (optional)\n  Custom docker image BYOC\n:param driver_instance_pool_id:\
        \ str (optional)\n  The optional ID of the instance pool for the driver of\
        \ the cluster belongs. The pool cluster uses\n  the instance pool with id\
        \ (instance_pool_id) if the driver pool is not assigned.\n:param driver_node_type_id:\
        \ str (optional)\n  The node type of the Spark driver. Note that this field\
        \ is optional; if unset, the driver node type\n  will be set as the same value\
        \ as `node_type_id` defined above.\n\n  This field, along with node_type_id,\
        \ should not be set if virtual_cluster_size is set. If both\n  driver_node_type_id,\
        \ node_type_id, and virtual_cluster_size are specified, driver_node_type_id\
        \ and\n  node_type_id take precedence.\n:param enable_elastic_disk: bool (optional)\n\
        \  Autoscaling Local Storage: when enabled, this cluster will dynamically\
        \ acquire additional disk space\n  when its Spark workers are running low\
        \ on disk space. This feature requires specific AWS permissions\n  to function\
        \ correctly - refer to the User Guide for more details.\n:param enable_local_disk_encryption:\
        \ bool (optional)\n  Whether to enable LUKS on cluster VMs' local disks\n\
        :param gcp_attributes: :class:`GcpAttributes` (optional)\n  Attributes related\
        \ to clusters running on Google Cloud Platform. If not specified at cluster\n\
        \  creation, a set of default values will be used.\n:param init_scripts: List[:class:`InitScriptInfo`]\
        \ (optional)\n  The configuration for storing init scripts. Any number of\
        \ destinations can be specified. The scripts\n  are executed sequentially\
        \ in the order provided. If `cluster_log_conf` is specified, init script\n\
        \  logs are sent to `<destination>/<cluster-ID>/init_scripts`.\n:param instance_pool_id:\
        \ str (optional)\n  The optional ID of the instance pool to which the cluster\
        \ belongs.\n:param is_single_node: bool (optional)\n  This field can only\
        \ be used when `kind = CLASSIC_PREVIEW`.\n\n  When set to true, Databricks\
        \ will automatically set single node related `custom_tags`, `spark_conf`,\n\
        \  and `num_workers`\n:param kind: :class:`Kind` (optional)\n:param node_type_id:\
        \ str (optional)\n  This field encodes, through a single value, the resources\
        \ available to each of the Spark nodes in\n  this cluster. For example, the\
        \ Spark nodes can be provisioned and optimized for memory or compute\n  intensive\
        \ workloads. A list of available node types can be retrieved by using the\n\
        \  :method:clusters/listNodeTypes API call.\n:param num_workers: int (optional)\n\
        \  Number of worker nodes that this cluster should have. A cluster has one\
        \ Spark Driver and\n  `num_workers` Executors for a total of `num_workers`\
        \ + 1 Spark nodes.\n\n  Note: When reading the properties of a cluster, this\
        \ field reflects the desired number of workers\n  rather than the actual current\
        \ number of workers. For instance, if a cluster is resized from 5 to 10\n\
        \  workers, this field will immediately be updated to reflect the target size\
        \ of 10 workers, whereas\n  the workers listed in `spark_info` will gradually\
        \ increase from 5 to 10 as the new nodes are\n  provisioned.\n:param policy_id:\
        \ str (optional)\n  The ID of the cluster policy used to create the cluster\
        \ if applicable.\n:param remote_disk_throughput: int (optional)\n  If set,\
        \ what the configurable throughput (in Mb/s) for the remote disk is. Currently\
        \ only supported\n  for GCP HYPERDISK_BALANCED disks.\n:param runtime_engine:\
        \ :class:`RuntimeEngine` (optional)\n  Determines the cluster's runtime engine,\
        \ either standard or Photon.\n\n  This field is not compatible with legacy\
        \ `spark_version` values that contain `-photon-`. Remove\n  `-photon-` from\
        \ the `spark_version` and set `runtime_engine` to `PHOTON`.\n\n  If left unspecified,\
        \ the runtime engine defaults to standard unless the spark_version contains\n\
        \  -photon-, in which case Photon will be used.\n:param single_user_name:\
        \ str (optional)\n  Single user name if data_security_mode is `SINGLE_USER`\n\
        :param spark_conf: Dict[str,str] (optional)\n  An object containing a set\
        \ of optional, user-specified Spark configuration key-value pairs. Users\n\
        \  can also pass in a string of extra JVM options to the driver and the executors\
        \ via\n  `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`\
        \ respectively.\n:param spark_env_vars: Dict[str,str] (optional)\n  An object\
        \ containing a set of optional, user-specified environment variable key-value\
        \ pairs. Please\n  note that key-value pair of the form (X,Y) will be exported\
        \ as is (i.e., `export X='Y'`) while\n  launching the driver and workers.\n\
        \n  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we\
        \ recommend appending them to\n  `$SPARK_DAEMON_JAVA_OPTS` as shown in the\
        \ example below. This ensures that all default databricks\n  managed environmental\
        \ variables are included as well.\n\n  Example Spark environment variables:\
        \ `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\":\n  \"/local_disk0\"\
        }` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS\n  -Dspark.shuffle.service.enabled=true\"\
        }`\n:param ssh_public_keys: List[str] (optional)\n  SSH public key contents\
        \ that will be added to each Spark node in this cluster. The corresponding\n\
        \  private keys can be used to login with the user name `ubuntu` on port `2200`.\
        \ Up to 10 keys can be\n  specified.\n:param total_initial_remote_disk_size:\
        \ int (optional)\n  If set, what the total initial volume size (in GB) of\
        \ the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED\
        \ disks.\n:param use_ml_runtime: bool (optional)\n  This field can only be\
        \ used when `kind = CLASSIC_PREVIEW`.\n\n  `effective_spark_version` is determined\
        \ by `spark_version` (DBR release), this field\n  `use_ml_runtime`, and whether\
        \ `node_type_id` is gpu node or not.\n:param workload_type: :class:`WorkloadType`\
        \ (optional)\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n\
        \  See :method:wait_get_cluster_running for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str ID of the cluster
                spark_version:
                  type: string
                  description: str The Spark version of the cluster, e.g. `3.3.x-scala2.11`.
                    A list of available Spark versions can be retrieved by using the
                    :method:clusters/sparkVersions API call.
                apply_policy_default_values:
                  type: boolean
                  description: bool (optional) When set to true, fixed and default
                    values from the policy will be used for fields that are omitted.
                    When set to false, only fixed values from the policy will be applied.
                autoscale:
                  $ref: '#/components/schemas/AutoScale'
                autotermination_minutes:
                  type: integer
                  description: int (optional) Automatically terminates the cluster
                    after it is inactive for this time in minutes. If not set, this
                    cluster will not be automatically terminated. If specified, the
                    threshold must be between 10 and 10000 minutes. Users can also
                    set this value to 0 to explicitly disable automatic termination.
                aws_attributes:
                  $ref: '#/components/schemas/AwsAttributes'
                azure_attributes:
                  $ref: '#/components/schemas/AzureAttributes'
                cluster_log_conf:
                  $ref: '#/components/schemas/ClusterLogConf'
                cluster_name:
                  type: string
                  description: str (optional) Cluster name requested by the user.
                    This doesn't have to be unique. If not specified at creation,
                    the cluster name will be an empty string. For job clusters, the
                    cluster name is automatically set based on the job and job run
                    IDs.
                custom_tags:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) Additional tags for cluster
                    resources. Databricks will tag all cluster resources (e.g., AWS
                    instances and EBS volumes) with these tags in addition to `default_tags`.
                    Notes: - Currently, Databricks allows at most 45 custom tags -
                    Clusters can only reuse cloud resources if the resources'' tags
                    are a subset of the cluster tags'
                data_security_mode:
                  type: string
                  enum:
                  - DATA_SECURITY_MODE_AUTO
                  - DATA_SECURITY_MODE_DEDICATED
                  - DATA_SECURITY_MODE_STANDARD
                  - LEGACY_PASSTHROUGH
                  - LEGACY_SINGLE_USER
                  - LEGACY_SINGLE_USER_STANDARD
                  - LEGACY_TABLE_ACL
                  - NONE
                  - SINGLE_USER
                  - USER_ISOLATION
                  description: :class:`DataSecurityMode` (optional)
                docker_image:
                  $ref: '#/components/schemas/DockerImage'
                driver_instance_pool_id:
                  type: string
                  description: str (optional) The optional ID of the instance pool
                    for the driver of the cluster belongs. The pool cluster uses the
                    instance pool with id (instance_pool_id) if the driver pool is
                    not assigned.
                driver_node_type_id:
                  type: string
                  description: str (optional) The node type of the Spark driver. Note
                    that this field is optional; if unset, the driver node type will
                    be set as the same value as `node_type_id` defined above. This
                    field, along with node_type_id, should not be set if virtual_cluster_size
                    is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size
                    are specified, driver_node_type_id and node_type_id take precedence.
                enable_elastic_disk:
                  type: boolean
                  description: 'bool (optional) Autoscaling Local Storage: when enabled,
                    this cluster will dynamically acquire additional disk space when
                    its Spark workers are running low on disk space. This feature
                    requires specific AWS permissions to function correctly - refer
                    to the User Guide for more details.'
                enable_local_disk_encryption:
                  type: boolean
                  description: bool (optional) Whether to enable LUKS on cluster VMs'
                    local disks
                gcp_attributes:
                  $ref: '#/components/schemas/GcpAttributes'
                init_scripts:
                  type: array
                  items:
                    $ref: '#/components/schemas/InitScriptInfo'
                  description: List[:class:`InitScriptInfo`] (optional) The configuration
                    for storing init scripts. Any number of destinations can be specified.
                    The scripts are executed sequentially in the order provided. If
                    `cluster_log_conf` is specified, init script logs are sent to
                    `<destination>/<cluster-ID>/init_scripts`.
                instance_pool_id:
                  type: string
                  description: str (optional) The optional ID of the instance pool
                    to which the cluster belongs.
                is_single_node:
                  type: boolean
                  description: bool (optional) This field can only be used when `kind
                    = CLASSIC_PREVIEW`. When set to true, Databricks will automatically
                    set single node related `custom_tags`, `spark_conf`, and `num_workers`
                kind:
                  type: string
                  enum:
                  - CLASSIC_PREVIEW
                  description: :class:`Kind` (optional)
                node_type_id:
                  type: string
                  description: str (optional) This field encodes, through a single
                    value, the resources available to each of the Spark nodes in this
                    cluster. For example, the Spark nodes can be provisioned and optimized
                    for memory or compute intensive workloads. A list of available
                    node types can be retrieved by using the
                num_workers:
                  type: integer
                  description: 'int (optional) Number of worker nodes that this cluster
                    should have. A cluster has one Spark Driver and `num_workers`
                    Executors for a total of `num_workers` + 1 Spark nodes. Note:
                    When reading the properties of a cluster, this field reflects
                    the desired number of workers rather than the actual current number
                    of workers. For instance, if a cluster is resized from 5 to 10
                    workers, this field will immediately be updated to reflect the
                    target size of 10 workers, whereas the workers listed in `spark_info`
                    will gradually increase from 5 to 10 as the new nodes are provisioned.'
                policy_id:
                  type: string
                  description: str (optional) The ID of the cluster policy used to
                    create the cluster if applicable.
                remote_disk_throughput:
                  type: integer
                  description: int (optional) If set, what the configurable throughput
                    (in Mb/s) for the remote disk is. Currently only supported for
                    GCP HYPERDISK_BALANCED disks.
                runtime_engine:
                  type: string
                  enum:
                  - 'NULL'
                  - PHOTON
                  - STANDARD
                  description: :class:`RuntimeEngine` (optional) Determines the cluster's
                    runtime engine, either standard or Photon. This field is not compatible
                    with legacy `spark_version` values that contain `-photon-`. Remove
                    `-photon-` from the `spark_version` and set `runtime_engine` to
                    `PHOTON`. If left unspecified, the runtime engine defaults to
                    standard unless the spark_version contains -photon-, in which
                    case Photon will be used.
                single_user_name:
                  type: string
                  description: str (optional) Single user name if data_security_mode
                    is `SINGLE_USER`
                spark_conf:
                  type: object
                  additionalProperties: true
                  description: Dict[str,str] (optional) An object containing a set
                    of optional, user-specified Spark configuration key-value pairs.
                    Users can also pass in a string of extra JVM options to the driver
                    and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`
                    respectively.
                spark_env_vars:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) An object containing a set
                    of optional, user-specified environment variable key-value pairs.
                    Please note that key-value pair of the form (X,Y) will be exported
                    as is (i.e., `export X=''Y''`) while launching the driver and
                    workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`,
                    we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown
                    in the example below. This ensures that all default databricks
                    managed environmental variables are included as well. Example
                    Spark environment variables: `{"SPARK_WORKER_MEMORY": "28000m",
                    "SPARK_LOCAL_DIRS": "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS":
                    "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`'
                ssh_public_keys:
                  type: array
                  items:
                    type: string
                  description: List[str] (optional) SSH public key contents that will
                    be added to each Spark node in this cluster. The corresponding
                    private keys can be used to login with the user name `ubuntu`
                    on port `2200`. Up to 10 keys can be specified.
                total_initial_remote_disk_size:
                  type: integer
                  description: int (optional) If set, what the total initial volume
                    size (in GB) of the remote disks should be. Currently only supported
                    for GCP HYPERDISK_BALANCED disks.
                use_ml_runtime:
                  type: boolean
                  description: bool (optional) This field can only be used when `kind
                    = CLASSIC_PREVIEW`. `effective_spark_version` is determined by
                    `spark_version` (DBR release), this field `use_ml_runtime`, and
                    whether `node_type_id` is gpu node or not.
                workload_type:
                  $ref: '#/components/schemas/WorkloadType'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/events:
    post:
      operationId: events
      summary: 'Retrieves a list of events about the activity of a cluster. This API
        is paginated. If there are more

        events to read, the response includes all the parameters necessary to request
        the next page of events.'
      description: "Retrieves a list of events about the activity of a cluster. This\
        \ API is paginated. If there are more\nevents to read, the response includes\
        \ all the parameters necessary to request the next page of events.\n\n:param\
        \ cluster_id: str\n  The ID of the cluster to retrieve events about.\n:param\
        \ end_time: int (optional)\n  The end time in epoch milliseconds. If empty,\
        \ returns events up to the current time.\n:param event_types: List[:class:`EventType`]\
        \ (optional)\n  An optional set of event types to filter on. If empty, all\
        \ event types are returned.\n:param limit: int (optional)\n  Deprecated: use\
        \ page_token in combination with page_size instead.\n\n  The maximum number\
        \ of events to include in a page of events. Defaults to 50, and maximum allowed\n\
        \  value is 500.\n:param offset: int (optional)\n  Deprecated: use page_token\
        \ in combination with page_size instead.\n\n  The offset in the result set.\
        \ Defaults to 0 (no offset). When an offset is specified and the results\n\
        \  are requested in descending order, the end_time field is required.\n:param\
        \ order: :class:`GetEventsOrder` (optional)\n  The order to list events in;\
        \ either \"ASC\" or \"DESC\". Defaults to \"DESC\".\n:param page_size: int\
        \ (optional)\n  The maximum number of events to include in a page of events.\
        \ The server may further constrain the\n  maximum number of results returned\
        \ in a single page. If the page_size is empty or 0, the server will\n  decide\
        \ the number of results to be returned. The field has to be in the range [0,500].\
        \ If the value\n  is outside the range, the server enforces 0 or 500.\n:param\
        \ page_token: str (optional)\n  Use next_page_token or prev_page_token returned\
        \ from the previous request to list the next or\n  previous page of events\
        \ respectively. If page_token is empty, the first page is returned.\n:param\
        \ start_time: int (optional)\n  The start time in epoch milliseconds. If empty,\
        \ returns events starting from the beginning of time.\n\n:returns: Iterator\
        \ over :class:`ClusterEvent`"
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The ID of the cluster to retrieve events about.
                end_time:
                  type: integer
                  description: int (optional) The end time in epoch milliseconds.
                    If empty, returns events up to the current time.
                event_types:
                  type: array
                  items:
                    type: string
                    enum:
                    - ADD_NODES_FAILED
                    - AUTOMATIC_CLUSTER_UPDATE
                    - AUTOSCALING_BACKOFF
                    - AUTOSCALING_FAILED
                    - AUTOSCALING_STATS_REPORT
                    - CLUSTER_MIGRATED
                    - CREATING
                    - DBFS_DOWN
                    - DID_NOT_EXPAND_DISK
                    - DRIVER_HEALTHY
                    - DRIVER_NOT_RESPONDING
                    - DRIVER_UNAVAILABLE
                    - EDITED
                    - EXPANDED_DISK
                    - FAILED_TO_EXPAND_DISK
                    - INIT_SCRIPTS_FINISHED
                    - INIT_SCRIPTS_STARTED
                    - METASTORE_DOWN
                    - NODES_LOST
                    - NODE_BLACKLISTED
                    - NODE_EXCLUDED_DECOMMISSIONED
                    - PINNED
                    - RESIZING
                    - RESTARTING
                    - RUNNING
                    - SPARK_EXCEPTION
                    - STARTING
                    - TERMINATING
                    - UNPINNED
                    - UPSIZE_COMPLETED
                  description: List[:class:`EventType`] (optional) An optional set
                    of event types to filter on. If empty, all event types are returned.
                limit:
                  type: integer
                  description: 'int (optional) Deprecated: use page_token in combination
                    with page_size instead. The maximum number of events to include
                    in a page of events. Defaults to 50, and maximum allowed value
                    is 500.'
                offset:
                  type: integer
                  description: 'int (optional) Deprecated: use page_token in combination
                    with page_size instead. The offset in the result set. Defaults
                    to 0 (no offset). When an offset is specified and the results
                    are requested in descending order, the end_time field is required.'
                order:
                  type: string
                  enum:
                  - ASC
                  - DESC
                  description: :class:`GetEventsOrder` (optional) The order to list
                    events in; either "ASC" or "DESC". Defaults to "DESC".
                page_size:
                  type: integer
                  description: int (optional) The maximum number of events to include
                    in a page of events. The server may further constrain the maximum
                    number of results returned in a single page. If the page_size
                    is empty or 0, the server will decide the number of results to
                    be returned. The field has to be in the range [0,500]. If the
                    value is outside the range, the server enforces 0 or 500.
                page_token:
                  type: string
                  description: str (optional) Use next_page_token or prev_page_token
                    returned from the previous request to list the next or previous
                    page of events respectively. If page_token is empty, the first
                    page is returned.
                start_time:
                  type: integer
                  description: int (optional) The start time in epoch milliseconds.
                    If empty, returns events starting from the beginning of time.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterEvent'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/get:
    get:
      operationId: get
      summary: 'Retrieves the information for a cluster given its identifier. Clusters
        can be described while they are

        running, or up to 60 days after they are terminated.'
      description: "Retrieves the information for a cluster given its identifier.\
        \ Clusters can be described while they are\nrunning, or up to 60 days after\
        \ they are terminated.\n\n:param cluster_id: str\n  The cluster about which\
        \ to retrieve information.\n\n:returns: :class:`ClusterDetails`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str The cluster about which to retrieve information.
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/permissions/clusters/{cluster_id}/permissionLevels:
    get:
      operationId: get_permission_levels
      summary: Gets the permission levels that a user can have on an object.
      description: "Gets the permission levels that a user can have on an object.\n\
        \n:param cluster_id: str\n  The cluster for which to get or manage permissions.\n\
        \n:returns: :class:`GetClusterPermissionLevelsResponse`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str The cluster for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetClusterPermissionLevelsResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/permissions/clusters/{cluster_id}:
    get:
      operationId: get_permissions
      summary: Gets the permissions of a cluster. Clusters can inherit permissions
        from their root object.
      description: "Gets the permissions of a cluster. Clusters can inherit permissions\
        \ from their root object.\n\n:param cluster_id: str\n  The cluster for which\
        \ to get or manage permissions.\n\n:returns: :class:`ClusterPermissions`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str The cluster for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    put:
      operationId: set_permissions
      summary: 'Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from their
        root object.'
      description: "Sets permissions on an object, replacing existing permissions\
        \ if they exist. Deletes all direct\npermissions if none are specified. Objects\
        \ can inherit permissions from their root object.\n\n:param cluster_id: str\n\
        \  The cluster for which to get or manage permissions.\n:param access_control_list:\
        \ List[:class:`ClusterAccessControlRequest`] (optional)\n\n:returns: :class:`ClusterPermissions`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str The cluster for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/ClusterAccessControlRequest'
                  description: List[:class:`ClusterAccessControlRequest`] (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    patch:
      operationId: update_permissions
      summary: Updates the permissions on a cluster. Clusters can inherit permissions
        from their root object.
      description: "Updates the permissions on a cluster. Clusters can inherit permissions\
        \ from their root object.\n\n:param cluster_id: str\n  The cluster for which\
        \ to get or manage permissions.\n:param access_control_list: List[:class:`ClusterAccessControlRequest`]\
        \ (optional)\n\n:returns: :class:`ClusterPermissions`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str The cluster for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/ClusterAccessControlRequest'
                  description: List[:class:`ClusterAccessControlRequest`] (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/list:
    get:
      operationId: list
      summary: 'Return information about all pinned and active clusters, and all clusters
        terminated within the last

        30 days. Clusters terminated prior to this period are not included.'
      description: "Return information about all pinned and active clusters, and all\
        \ clusters terminated within the last\n30 days. Clusters terminated prior\
        \ to this period are not included.\n\n:param filter_by: :class:`ListClustersFilterBy`\
        \ (optional)\n  Filters to apply to the list of clusters.\n:param page_size:\
        \ int (optional)\n  Use this field to specify the maximum number of results\
        \ to be returned by the server. The server may\n  further constrain the maximum\
        \ number of results returned in a single page.\n:param page_token: str (optional)\n\
        \  Use next_page_token or prev_page_token returned from the previous request\
        \ to list the next or\n  previous page of clusters respectively.\n:param sort_by:\
        \ :class:`ListClustersSortBy` (optional)\n  Sort the list of clusters by a\
        \ specific criteria.\n\n:returns: Iterator over :class:`ClusterDetails`"
      tags:
      - compute
      parameters:
      - name: filter_by
        description: :class:`ListClustersFilterBy` (optional) Filters to apply to
          the list of clusters.
        required: false
        schema:
          $ref: '#/components/schemas/ListClustersFilterBy'
        in: query
      - name: page_size
        description: int (optional) Use this field to specify the maximum number of
          results to be returned by the server. The server may further constrain the
          maximum number of results returned in a single page.
        required: false
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) Use next_page_token or prev_page_token returned
          from the previous request to list the next or previous page of clusters
          respectively.
        required: false
        schema:
          type: string
        in: query
      - name: sort_by
        description: :class:`ListClustersSortBy` (optional) Sort the list of clusters
          by a specific criteria.
        required: false
        schema:
          $ref: '#/components/schemas/ListClustersSortBy'
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/list-node-types:
    get:
      operationId: list_node_types
      summary: Returns a list of supported Spark node types. These node types can
        be used to launch a cluster.
      description: 'Returns a list of supported Spark node types. These node types
        can be used to launch a cluster.



        :returns: :class:`ListNodeTypesResponse`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListNodeTypesResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/list-zones:
    get:
      operationId: list_zones
      summary: 'Returns a list of availability zones where clusters can be created
        in (For example, us-west-2a). These

        zones can be used to launch a cluster.'
      description: 'Returns a list of availability zones where clusters can be created
        in (For example, us-west-2a). These

        zones can be used to launch a cluster.



        :returns: :class:`ListAvailableZonesResponse`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListAvailableZonesResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/permanent-delete:
    post:
      operationId: permanent_delete
      summary: 'Permanently deletes a Spark cluster. This cluster is terminated and
        resources are asynchronously

        removed.'
      description: "Permanently deletes a Spark cluster. This cluster is terminated\
        \ and resources are asynchronously\nremoved.\n\nIn addition, users will no\
        \ longer see permanently deleted clusters in the cluster list, and API users\n\
        can no longer perform any action on permanently deleted clusters.\n\n:param\
        \ cluster_id: str\n  The cluster to be deleted."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The cluster to be deleted.
      responses:
        '200':
          description: Success
  /api/2.1/clusters/pin:
    post:
      operationId: pin
      summary: 'Pinning a cluster ensures that the cluster will always be returned
        by the ListClusters API. Pinning a

        cluster that is already pinned will have no effect. This API can only be called
        by workspace admins.'
      description: 'Pinning a cluster ensures that the cluster will always be returned
        by the ListClusters API. Pinning a

        cluster that is already pinned will have no effect. This API can only be called
        by workspace admins.


        :param cluster_id: str'
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str
      responses:
        '200':
          description: Success
  /api/2.1/clusters/resize:
    post:
      operationId: resize
      summary: 'Resizes a cluster to have a desired number of workers. This will fail
        unless the cluster is in a

        `RUNNING` state.'
      description: "Resizes a cluster to have a desired number of workers. This will\
        \ fail unless the cluster is in a\n`RUNNING` state.\n\n:param cluster_id:\
        \ str\n  The cluster to be resized.\n:param autoscale: :class:`AutoScale`\
        \ (optional)\n  Parameters needed in order to automatically scale clusters\
        \ up and down based on load. Note:\n  autoscaling works best with DB runtime\
        \ versions 3.0 or later.\n:param num_workers: int (optional)\n  Number of\
        \ worker nodes that this cluster should have. A cluster has one Spark Driver\
        \ and\n  `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\
        \n  Note: When reading the properties of a cluster, this field reflects the\
        \ desired number of workers\n  rather than the actual current number of workers.\
        \ For instance, if a cluster is resized from 5 to 10\n  workers, this field\
        \ will immediately be updated to reflect the target size of 10 workers, whereas\n\
        \  the workers listed in `spark_info` will gradually increase from 5 to 10\
        \ as the new nodes are\n  provisioned.\n\n:returns:\n  Long-running operation\
        \ waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running\
        \ for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The cluster to be resized.
                autoscale:
                  $ref: '#/components/schemas/AutoScale'
                num_workers:
                  type: integer
                  description: 'int (optional) Number of worker nodes that this cluster
                    should have. A cluster has one Spark Driver and `num_workers`
                    Executors for a total of `num_workers` + 1 Spark nodes. Note:
                    When reading the properties of a cluster, this field reflects
                    the desired number of workers rather than the actual current number
                    of workers. For instance, if a cluster is resized from 5 to 10
                    workers, this field will immediately be updated to reflect the
                    target size of 10 workers, whereas the workers listed in `spark_info`
                    will gradually increase from 5 to 10 as the new nodes are provisioned.'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/restart:
    post:
      operationId: restart
      summary: 'Restarts a Spark cluster with the supplied ID. If the cluster is not
        currently in a `RUNNING` state,

        nothing will happen.'
      description: "Restarts a Spark cluster with the supplied ID. If the cluster\
        \ is not currently in a `RUNNING` state,\nnothing will happen.\n\n:param cluster_id:\
        \ str\n  The cluster to be started.\n:param restart_user: str (optional)\n\
        \n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n\
        \  See :method:wait_get_cluster_running for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The cluster to be started.
                restart_user:
                  type: string
                  description: str (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/spark-versions:
    get:
      operationId: spark_versions
      summary: Returns the list of available Spark versions. These versions can be
        used to launch a cluster.
      description: 'Returns the list of available Spark versions. These versions can
        be used to launch a cluster.



        :returns: :class:`GetSparkVersionsResponse`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetSparkVersionsResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/start:
    post:
      operationId: start
      summary: 'Starts a terminated Spark cluster with the supplied ID. This works
        similar to `createCluster` except:

        - The previous cluster id and attributes are preserved. - The cluster starts
        with the last specified

        cluster size. - If the previous cluster was an autoscaling cluster, the current
        cluster starts with

        the minimum number of nodes. - If the cluster is not currently in a ``TERMINATED``
        state, nothing will

        happen. - Clusters launched to run a job cannot be started.'
      description: "Starts a terminated Spark cluster with the supplied ID. This works\
        \ similar to `createCluster` except:\n- The previous cluster id and attributes\
        \ are preserved. - The cluster starts with the last specified\ncluster size.\
        \ - If the previous cluster was an autoscaling cluster, the current cluster\
        \ starts with\nthe minimum number of nodes. - If the cluster is not currently\
        \ in a ``TERMINATED`` state, nothing will\nhappen. - Clusters launched to\
        \ run a job cannot be started.\n\n:param cluster_id: str\n  The cluster to\
        \ be started.\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n\
        \  See :method:wait_get_cluster_running for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The cluster to be started.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.1/clusters/unpin:
    post:
      operationId: unpin
      summary: 'Unpinning a cluster will allow the cluster to eventually be removed
        from the ListClusters API.

        Unpinning a cluster that is not pinned will have no effect. This API can only
        be called by workspace

        admins.'
      description: 'Unpinning a cluster will allow the cluster to eventually be removed
        from the ListClusters API.

        Unpinning a cluster that is not pinned will have no effect. This API can only
        be called by workspace

        admins.


        :param cluster_id: str'
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str
      responses:
        '200':
          description: Success
  /api/2.1/clusters/update:
    post:
      operationId: update
      summary: 'Updates the configuration of a cluster to match the partial set of
        attributes and size. Denote which

        fields to update using the `update_mask` field in the request body. A cluster
        can be updated if it is

        in a `RUNNING` or `TERMINATED` state. If a cluster is updated while in a `RUNNING`
        state, it will be

        restarted so that the new attributes can take effect. If a cluster is updated
        while in a `TERMINATED`

        state, it will remain `TERMINATED`. The updated attributes will take effect
        the next time the cluster

        is started using the `clusters/start` API. Attempts to update a cluster in
        any other state will be

        rejected with an `INVALID_STATE` error code. Clusters created by the Databricks
        Jobs service cannot be

        updated.'
      description: "Updates the configuration of a cluster to match the partial set\
        \ of attributes and size. Denote which\nfields to update using the `update_mask`\
        \ field in the request body. A cluster can be updated if it is\nin a `RUNNING`\
        \ or `TERMINATED` state. If a cluster is updated while in a `RUNNING` state,\
        \ it will be\nrestarted so that the new attributes can take effect. If a cluster\
        \ is updated while in a `TERMINATED`\nstate, it will remain `TERMINATED`.\
        \ The updated attributes will take effect the next time the cluster\nis started\
        \ using the `clusters/start` API. Attempts to update a cluster in any other\
        \ state will be\nrejected with an `INVALID_STATE` error code. Clusters created\
        \ by the Databricks Jobs service cannot be\nupdated.\n\n:param cluster_id:\
        \ str\n  ID of the cluster.\n:param update_mask: str\n  Used to specify which\
        \ cluster attributes and size fields to update. See https://google.aip.dev/161\n\
        \  for more details.\n\n  The field mask must be a single string, with multiple\
        \ fields separated by commas (no spaces). The\n  field path is relative to\
        \ the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n \
        \ `author.given_name`). Specification of elements in sequence or map fields\
        \ is not allowed, as only\n  the entire collection field can be specified.\
        \ Field names must exactly match the resource field\n  names.\n\n  A field\
        \ mask of `*` indicates full replacement. Its recommended to always explicitly\
        \ list the\n  fields being updated and avoid using `*` wildcards, as it can\
        \ lead to unintended results if the API\n  changes in the future.\n:param\
        \ cluster: :class:`UpdateClusterResource` (optional)\n  The cluster to be\
        \ updated.\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n\
        \  See :method:wait_get_cluster_running for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str ID of the cluster.
                update_mask:
                  type: string
                  description: str Used to specify which cluster attributes and size
                    fields to update. See https://google.aip.dev/161 for more details.
                    The field mask must be a single string, with multiple fields separated
                    by commas (no spaces). The field path is relative to the resource
                    object, using a dot (`.`) to navigate sub-fields (e.g., `author.given_name`).
                    Specification of elements in sequence or map fields is not allowed,
                    as only the entire collection field can be specified. Field names
                    must exactly match the resource field names. A field mask of `*`
                    indicates full replacement. Its recommended to always explicitly
                    list the fields being updated and avoid using `*` wildcards, as
                    it can lead to unintended results if the API changes in the future.
                cluster:
                  $ref: '#/components/schemas/UpdateClusterResource'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/1.2/commands/cancel:
    post:
      operationId: cancel
      summary: Cancels a currently running command within an execution context.
      description: "Cancels a currently running command within an execution context.\n\
        \nThe command ID is obtained from a prior successful call to __execute__.\n\
        \n:param cluster_id: str (optional)\n:param command_id: str (optional)\n:param\
        \ context_id: str (optional)\n\n:returns:\n  Long-running operation waiter\
        \ for :class:`CommandStatusResponse`.\n  See :method:wait_command_status_command_execution_cancelled\
        \ for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str (optional)
                command_id:
                  type: string
                  description: str (optional)
                context_id:
                  type: string
                  description: str (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CommandStatusResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/1.2/commands/status:
    get:
      operationId: command_status
      summary: Gets the status of and, if available, the results from a currently
        executing command.
      description: 'Gets the status of and, if available, the results from a currently
        executing command.


        The command ID is obtained from a prior successful call to __execute__.


        :param cluster_id: str

        :param context_id: str

        :param command_id: str


        :returns: :class:`CommandStatusResponse`'
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str
        required: true
        schema:
          type: string
        in: query
      - name: context_id
        description: str
        required: true
        schema:
          type: string
        in: query
      - name: command_id
        description: str
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CommandStatusResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/1.2/contexts/status:
    get:
      operationId: context_status
      summary: Gets the status for an execution context.
      description: 'Gets the status for an execution context.


        :param cluster_id: str

        :param context_id: str


        :returns: :class:`ContextStatusResponse`'
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str
        required: true
        schema:
          type: string
        in: query
      - name: context_id
        description: str
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ContextStatusResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/1.2/contexts/create:
    post:
      operationId: create
      summary: Creates an execution context for running cluster commands.
      description: "Creates an execution context for running cluster commands.\n\n\
        If successful, this method returns the ID of the new execution context.\n\n\
        :param cluster_id: str (optional)\n  Running cluster id\n:param language:\
        \ :class:`Language` (optional)\n\n:returns:\n  Long-running operation waiter\
        \ for :class:`ContextStatusResponse`.\n  See :method:wait_context_status_command_execution_running\
        \ for more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str (optional) Running cluster id
                language:
                  type: string
                  enum:
                  - python
                  - r
                  - scala
                  - sql
                  description: :class:`Language` (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ContextStatusResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/1.2/contexts/destroy:
    post:
      operationId: destroy
      summary: Deletes an execution context.
      description: 'Deletes an execution context.


        :param cluster_id: str

        :param context_id: str'
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str
                context_id:
                  type: string
                  description: str
      responses:
        '200':
          description: Success
  /api/1.2/commands/execute:
    post:
      operationId: execute
      summary: Runs a cluster command in the given execution context, using the provided
        language.
      description: "Runs a cluster command in the given execution context, using the\
        \ provided language.\n\nIf successful, it returns an ID for tracking the status\
        \ of the command's execution.\n\n:param cluster_id: str (optional)\n  Running\
        \ cluster id\n:param command: str (optional)\n  Executable code\n:param context_id:\
        \ str (optional)\n  Running context id\n:param language: :class:`Language`\
        \ (optional)\n\n:returns:\n  Long-running operation waiter for :class:`CommandStatusResponse`.\n\
        \  See :method:wait_command_status_command_execution_finished_or_error for\
        \ more details."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str (optional) Running cluster id
                command:
                  type: string
                  description: str (optional) Executable code
                context_id:
                  type: string
                  description: str (optional) Running context id
                language:
                  type: string
                  enum:
                  - python
                  - r
                  - scala
                  - sql
                  description: :class:`Language` (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CommandStatusResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/global-init-scripts:
    post:
      operationId: create
      summary: Creates a new global init script in this workspace.
      description: "Creates a new global init script in this workspace.\n\n:param\
        \ name: str\n  The name of the script\n:param script: str\n  The Base64-encoded\
        \ content of the script.\n:param enabled: bool (optional)\n  Specifies whether\
        \ the script is enabled. The script runs only if enabled.\n:param position:\
        \ int (optional)\n  The position of a global init script, where 0 represents\
        \ the first script to run, 1 is the second\n  script to run, in ascending\
        \ order.\n\n  If you omit the numeric position for a new global init script,\
        \ it defaults to last position. It will\n  run after all current scripts.\
        \ Setting any value greater than the position of the last script is\n  equivalent\
        \ to the last position. Example: Take three existing scripts with positions\
        \ 0, 1, and 2.\n  Any position of (3) or greater puts the script in the last\
        \ position. If an explicit position value\n  conflicts with an existing script\
        \ value, your request succeeds, but the original script at that\n  position\
        \ and all later scripts have their positions incremented by 1.\n\n:returns:\
        \ :class:`CreateResponse`"
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                name:
                  type: string
                  description: str The name of the script
                script:
                  type: string
                  description: str The Base64-encoded content of the script.
                enabled:
                  type: boolean
                  description: bool (optional) Specifies whether the script is enabled.
                    The script runs only if enabled.
                position:
                  type: integer
                  description: 'int (optional) The position of a global init script,
                    where 0 represents the first script to run, 1 is the second script
                    to run, in ascending order. If you omit the numeric position for
                    a new global init script, it defaults to last position. It will
                    run after all current scripts. Setting any value greater than
                    the position of the last script is equivalent to the last position.
                    Example: Take three existing scripts with positions 0, 1, and
                    2. Any position of (3) or greater puts the script in the last
                    position. If an explicit position value conflicts with an existing
                    script value, your request succeeds, but the original script at
                    that position and all later scripts have their positions incremented
                    by 1.'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    get:
      operationId: list
      summary: 'Get a list of all global init scripts for this workspace. This returns
        all properties for each script

        but **not** the script contents. To retrieve the contents of a script, use
        the [get a global init

        script](:method:globalinitscripts/get) operation.'
      description: 'Get a list of all global init scripts for this workspace. This
        returns all properties for each script

        but **not** the script contents. To retrieve the contents of a script, use
        the [get a global init

        script](:method:globalinitscripts/get) operation.



        :returns: Iterator over :class:`GlobalInitScriptDetails`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GlobalInitScriptDetails'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/global-init-scripts/{script_id}:
    delete:
      operationId: delete
      summary: Deletes a global init script.
      description: "Deletes a global init script.\n\n:param script_id: str\n  The\
        \ ID of the global init script."
      tags:
      - compute
      parameters:
      - name: script_id
        description: str The ID of the global init script.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Success
    get:
      operationId: get
      summary: Gets all the details of a script, including its Base64-encoded contents.
      description: "Gets all the details of a script, including its Base64-encoded\
        \ contents.\n\n:param script_id: str\n  The ID of the global init script.\n\
        \n:returns: :class:`GlobalInitScriptDetailsWithContent`"
      tags:
      - compute
      parameters:
      - name: script_id
        description: str The ID of the global init script.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GlobalInitScriptDetailsWithContent'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    patch:
      operationId: update
      summary: 'Updates a global init script, specifying only the fields to change.
        All fields are optional.

        Unspecified fields retain their current value.'
      description: "Updates a global init script, specifying only the fields to change.\
        \ All fields are optional.\nUnspecified fields retain their current value.\n\
        \n:param script_id: str\n  The ID of the global init script.\n:param name:\
        \ str\n  The name of the script\n:param script: str\n  The Base64-encoded\
        \ content of the script.\n:param enabled: bool (optional)\n  Specifies whether\
        \ the script is enabled. The script runs only if enabled.\n:param position:\
        \ int (optional)\n  The position of a script, where 0 represents the first\
        \ script to run, 1 is the second script to run,\n  in ascending order. To\
        \ move the script to run first, set its position to 0.\n\n  To move the script\
        \ to the end, set its position to any value greater or equal to the position\
        \ of the\n  last script. Example, three existing scripts with positions 0,\
        \ 1, and 2. Any position value of 2 or\n  greater puts the script in the last\
        \ position (2).\n\n  If an explicit position value conflicts with an existing\
        \ script, your request succeeds, but the\n  original script at that position\
        \ and all later scripts have their positions incremented by 1."
      tags:
      - compute
      parameters:
      - name: script_id
        description: str The ID of the global init script.
        required: true
        schema:
          type: string
        in: path
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                name:
                  type: string
                  description: str The name of the script
                script:
                  type: string
                  description: str The Base64-encoded content of the script.
                enabled:
                  type: boolean
                  description: bool (optional) Specifies whether the script is enabled.
                    The script runs only if enabled.
                position:
                  type: integer
                  description: int (optional) The position of a script, where 0 represents
                    the first script to run, 1 is the second script to run, in ascending
                    order. To move the script to run first, set its position to 0.
                    To move the script to the end, set its position to any value greater
                    or equal to the position of the last script. Example, three existing
                    scripts with positions 0, 1, and 2. Any position value of 2 or
                    greater puts the script in the last position (2). If an explicit
                    position value conflicts with an existing script, your request
                    succeeds, but the original script at that position and all later
                    scripts have their positions incremented by 1.
      responses:
        '200':
          description: Success
  /api/2.0/instance-pools/create:
    post:
      operationId: create
      summary: Creates a new instance pool using idle and ready-to-use cloud instances.
      description: "Creates a new instance pool using idle and ready-to-use cloud\
        \ instances.\n\n:param instance_pool_name: str\n  Pool name requested by the\
        \ user. Pool name must be unique. Length must be between 1 and 100\n  characters.\n\
        :param node_type_id: str\n  This field encodes, through a single value, the\
        \ resources available to each of the Spark nodes in\n  this cluster. For example,\
        \ the Spark nodes can be provisioned and optimized for memory or compute\n\
        \  intensive workloads. A list of available node types can be retrieved by\
        \ using the\n  :method:clusters/listNodeTypes API call.\n:param aws_attributes:\
        \ :class:`InstancePoolAwsAttributes` (optional)\n  Attributes related to instance\
        \ pools running on Amazon Web Services. If not specified at pool\n  creation,\
        \ a set of default values will be used.\n:param azure_attributes: :class:`InstancePoolAzureAttributes`\
        \ (optional)\n  Attributes related to instance pools running on Azure. If\
        \ not specified at pool creation, a set of\n  default values will be used.\n\
        :param custom_tags: Dict[str,str] (optional)\n  Additional tags for pool resources.\
        \ Databricks will tag all pool resources (e.g., AWS instances and\n  EBS volumes)\
        \ with these tags in addition to `default_tags`. Notes:\n\n  - Currently,\
        \ Databricks allows at most 45 custom tags\n:param disk_spec: :class:`DiskSpec`\
        \ (optional)\n  Defines the specification of the disks that will be attached\
        \ to all spark containers.\n:param enable_elastic_disk: bool (optional)\n\
        \  Autoscaling Local Storage: when enabled, this instances in this pool will\
        \ dynamically acquire\n  additional disk space when its Spark workers are\
        \ running low on disk space. In AWS, this feature\n  requires specific AWS\
        \ permissions to function correctly - refer to the User Guide for more details.\n\
        :param gcp_attributes: :class:`InstancePoolGcpAttributes` (optional)\n  Attributes\
        \ related to instance pools running on Google Cloud Platform. If not specified\
        \ at pool\n  creation, a set of default values will be used.\n:param idle_instance_autotermination_minutes:\
        \ int (optional)\n  Automatically terminates the extra instances in the pool\
        \ cache after they are inactive for this time\n  in minutes if min_idle_instances\
        \ requirement is already met. If not set, the extra pool instances\n  will\
        \ be automatically terminated after a default timeout. If specified, the threshold\
        \ must be\n  between 0 and 10000 minutes. Users can also set this value to\
        \ 0 to instantly remove idle instances\n  from the cache if min cache size\
        \ could still hold.\n:param max_capacity: int (optional)\n  Maximum number\
        \ of outstanding instances to keep in the pool, including both instances used\
        \ by\n  clusters and idle instances. Clusters that require further instance\
        \ provisioning will fail during\n  upsize requests.\n:param min_idle_instances:\
        \ int (optional)\n  Minimum number of idle instances to keep in the instance\
        \ pool\n:param preloaded_docker_images: List[:class:`DockerImage`] (optional)\n\
        \  Custom Docker Image BYOC\n:param preloaded_spark_versions: List[str] (optional)\n\
        \  A list containing at most one preloaded Spark image version for the pool.\
        \ Pool-backed clusters\n  started with the preloaded Spark version will start\
        \ faster. A list of available Spark versions can\n  be retrieved by using\
        \ the :method:clusters/sparkVersions API call.\n:param remote_disk_throughput:\
        \ int (optional)\n  If set, what the configurable throughput (in Mb/s) for\
        \ the remote disk is. Currently only supported\n  for GCP HYPERDISK_BALANCED\
        \ types.\n:param total_initial_remote_disk_size: int (optional)\n  If set,\
        \ what the total initial volume size (in GB) of the remote disks should be.\
        \ Currently only\n  supported for GCP HYPERDISK_BALANCED types.\n\n:returns:\
        \ :class:`CreateInstancePoolResponse`"
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_pool_name:
                  type: string
                  description: str Pool name requested by the user. Pool name must
                    be unique. Length must be between 1 and 100 characters.
                node_type_id:
                  type: string
                  description: str This field encodes, through a single value, the
                    resources available to each of the Spark nodes in this cluster.
                    For example, the Spark nodes can be provisioned and optimized
                    for memory or compute intensive workloads. A list of available
                    node types can be retrieved by using the
                aws_attributes:
                  $ref: '#/components/schemas/InstancePoolAwsAttributes'
                azure_attributes:
                  $ref: '#/components/schemas/InstancePoolAzureAttributes'
                custom_tags:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) Additional tags for pool
                    resources. Databricks will tag all pool resources (e.g., AWS instances
                    and EBS volumes) with these tags in addition to `default_tags`.
                    Notes: - Currently, Databricks allows at most 45 custom tags'
                disk_spec:
                  $ref: '#/components/schemas/DiskSpec'
                enable_elastic_disk:
                  type: boolean
                  description: 'bool (optional) Autoscaling Local Storage: when enabled,
                    this instances in this pool will dynamically acquire additional
                    disk space when its Spark workers are running low on disk space.
                    In AWS, this feature requires specific AWS permissions to function
                    correctly - refer to the User Guide for more details.'
                gcp_attributes:
                  $ref: '#/components/schemas/InstancePoolGcpAttributes'
                idle_instance_autotermination_minutes:
                  type: integer
                  description: int (optional) Automatically terminates the extra instances
                    in the pool cache after they are inactive for this time in minutes
                    if min_idle_instances requirement is already met. If not set,
                    the extra pool instances will be automatically terminated after
                    a default timeout. If specified, the threshold must be between
                    0 and 10000 minutes. Users can also set this value to 0 to instantly
                    remove idle instances from the cache if min cache size could still
                    hold.
                max_capacity:
                  type: integer
                  description: int (optional) Maximum number of outstanding instances
                    to keep in the pool, including both instances used by clusters
                    and idle instances. Clusters that require further instance provisioning
                    will fail during upsize requests.
                min_idle_instances:
                  type: integer
                  description: int (optional) Minimum number of idle instances to
                    keep in the instance pool
                preloaded_docker_images:
                  type: array
                  items:
                    $ref: '#/components/schemas/DockerImage'
                  description: List[:class:`DockerImage`] (optional) Custom Docker
                    Image BYOC
                preloaded_spark_versions:
                  type: array
                  items:
                    type: string
                  description: List[str] (optional) A list containing at most one
                    preloaded Spark image version for the pool. Pool-backed clusters
                    started with the preloaded Spark version will start faster. A
                    list of available Spark versions can be retrieved by using the
                    :method:clusters/sparkVersions API call.
                remote_disk_throughput:
                  type: integer
                  description: int (optional) If set, what the configurable throughput
                    (in Mb/s) for the remote disk is. Currently only supported for
                    GCP HYPERDISK_BALANCED types.
                total_initial_remote_disk_size:
                  type: integer
                  description: int (optional) If set, what the total initial volume
                    size (in GB) of the remote disks should be. Currently only supported
                    for GCP HYPERDISK_BALANCED types.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateInstancePoolResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/instance-pools/delete:
    post:
      operationId: delete
      summary: Deletes the instance pool permanently. The idle instances in the pool
        are terminated asynchronously.
      description: "Deletes the instance pool permanently. The idle instances in the\
        \ pool are terminated asynchronously.\n\n:param instance_pool_id: str\n  The\
        \ instance pool to be terminated."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_pool_id:
                  type: string
                  description: str The instance pool to be terminated.
      responses:
        '200':
          description: Success
  /api/2.0/instance-pools/edit:
    post:
      operationId: edit
      summary: Modifies the configuration of an existing instance pool.
      description: "Modifies the configuration of an existing instance pool.\n\n:param\
        \ instance_pool_id: str\n  Instance pool ID\n:param instance_pool_name: str\n\
        \  Pool name requested by the user. Pool name must be unique. Length must\
        \ be between 1 and 100\n  characters.\n:param node_type_id: str\n  This field\
        \ encodes, through a single value, the resources available to each of the\
        \ Spark nodes in\n  this cluster. For example, the Spark nodes can be provisioned\
        \ and optimized for memory or compute\n  intensive workloads. A list of available\
        \ node types can be retrieved by using the\n  :method:clusters/listNodeTypes\
        \ API call.\n:param custom_tags: Dict[str,str] (optional)\n  Additional tags\
        \ for pool resources. Databricks will tag all pool resources (e.g., AWS instances\
        \ and\n  EBS volumes) with these tags in addition to `default_tags`. Notes:\n\
        \n  - Currently, Databricks allows at most 45 custom tags\n:param idle_instance_autotermination_minutes:\
        \ int (optional)\n  Automatically terminates the extra instances in the pool\
        \ cache after they are inactive for this time\n  in minutes if min_idle_instances\
        \ requirement is already met. If not set, the extra pool instances\n  will\
        \ be automatically terminated after a default timeout. If specified, the threshold\
        \ must be\n  between 0 and 10000 minutes. Users can also set this value to\
        \ 0 to instantly remove idle instances\n  from the cache if min cache size\
        \ could still hold.\n:param max_capacity: int (optional)\n  Maximum number\
        \ of outstanding instances to keep in the pool, including both instances used\
        \ by\n  clusters and idle instances. Clusters that require further instance\
        \ provisioning will fail during\n  upsize requests.\n:param min_idle_instances:\
        \ int (optional)\n  Minimum number of idle instances to keep in the instance\
        \ pool\n:param remote_disk_throughput: int (optional)\n  If set, what the\
        \ configurable throughput (in Mb/s) for the remote disk is. Currently only\
        \ supported\n  for GCP HYPERDISK_BALANCED types.\n:param total_initial_remote_disk_size:\
        \ int (optional)\n  If set, what the total initial volume size (in GB) of\
        \ the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED\
        \ types."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_pool_id:
                  type: string
                  description: str Instance pool ID
                instance_pool_name:
                  type: string
                  description: str Pool name requested by the user. Pool name must
                    be unique. Length must be between 1 and 100 characters.
                node_type_id:
                  type: string
                  description: str This field encodes, through a single value, the
                    resources available to each of the Spark nodes in this cluster.
                    For example, the Spark nodes can be provisioned and optimized
                    for memory or compute intensive workloads. A list of available
                    node types can be retrieved by using the
                custom_tags:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) Additional tags for pool
                    resources. Databricks will tag all pool resources (e.g., AWS instances
                    and EBS volumes) with these tags in addition to `default_tags`.
                    Notes: - Currently, Databricks allows at most 45 custom tags'
                idle_instance_autotermination_minutes:
                  type: integer
                  description: int (optional) Automatically terminates the extra instances
                    in the pool cache after they are inactive for this time in minutes
                    if min_idle_instances requirement is already met. If not set,
                    the extra pool instances will be automatically terminated after
                    a default timeout. If specified, the threshold must be between
                    0 and 10000 minutes. Users can also set this value to 0 to instantly
                    remove idle instances from the cache if min cache size could still
                    hold.
                max_capacity:
                  type: integer
                  description: int (optional) Maximum number of outstanding instances
                    to keep in the pool, including both instances used by clusters
                    and idle instances. Clusters that require further instance provisioning
                    will fail during upsize requests.
                min_idle_instances:
                  type: integer
                  description: int (optional) Minimum number of idle instances to
                    keep in the instance pool
                remote_disk_throughput:
                  type: integer
                  description: int (optional) If set, what the configurable throughput
                    (in Mb/s) for the remote disk is. Currently only supported for
                    GCP HYPERDISK_BALANCED types.
                total_initial_remote_disk_size:
                  type: integer
                  description: int (optional) If set, what the total initial volume
                    size (in GB) of the remote disks should be. Currently only supported
                    for GCP HYPERDISK_BALANCED types.
      responses:
        '200':
          description: Success
  /api/2.0/instance-pools/get:
    get:
      operationId: get
      summary: Retrieve the information for an instance pool based on its identifier.
      description: "Retrieve the information for an instance pool based on its identifier.\n\
        \n:param instance_pool_id: str\n  The canonical unique identifier for the\
        \ instance pool.\n\n:returns: :class:`GetInstancePool`"
      tags:
      - compute
      parameters:
      - name: instance_pool_id
        description: str The canonical unique identifier for the instance pool.
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetInstancePool'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/permissions/instance-pools/{instance_pool_id}:
    get:
      operationId: get_permissions
      summary: 'Gets the permissions of an instance pool. Instance pools can inherit
        permissions from their root

        object.'
      description: "Gets the permissions of an instance pool. Instance pools can inherit\
        \ permissions from their root\nobject.\n\n:param instance_pool_id: str\n \
        \ The instance pool for which to get or manage permissions.\n\n:returns: :class:`InstancePoolPermissions`"
      tags:
      - compute
      parameters:
      - name: instance_pool_id
        description: str The instance pool for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstancePoolPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    put:
      operationId: set_permissions
      summary: 'Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from their
        root object.'
      description: "Sets permissions on an object, replacing existing permissions\
        \ if they exist. Deletes all direct\npermissions if none are specified. Objects\
        \ can inherit permissions from their root object.\n\n:param instance_pool_id:\
        \ str\n  The instance pool for which to get or manage permissions.\n:param\
        \ access_control_list: List[:class:`InstancePoolAccessControlRequest`] (optional)\n\
        \n:returns: :class:`InstancePoolPermissions`"
      tags:
      - compute
      parameters:
      - name: instance_pool_id
        description: str The instance pool for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/InstancePoolAccessControlRequest'
                  description: List[:class:`InstancePoolAccessControlRequest`] (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstancePoolPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/instance-pools/list:
    get:
      operationId: list
      summary: Gets a list of instance pools with their statistics.
      description: 'Gets a list of instance pools with their statistics.



        :returns: Iterator over :class:`InstancePoolAndStats`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstancePoolAndStats'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/instance-profiles/add:
    post:
      operationId: add
      summary: 'Registers an instance profile in Databricks. In the UI, you can then
        give users the permission to use

        this instance profile when launching clusters.'
      description: "Registers an instance profile in Databricks. In the UI, you can\
        \ then give users the permission to use\nthis instance profile when launching\
        \ clusters.\n\nThis API is only available to admin users.\n\n:param instance_profile_arn:\
        \ str\n  The AWS ARN of the instance profile to register with Databricks.\
        \ This field is required.\n:param iam_role_arn: str (optional)\n  The AWS\
        \ IAM role ARN of the role associated with the instance profile. This field\
        \ is required if\n  your role name and instance profile name do not match\
        \ and you want to use the instance profile with\n  [Databricks SQL Serverless].\n\
        \n  Otherwise, this field is optional.\n\n  [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html\n\
        :param is_meta_instance_profile: bool (optional)\n  Boolean flag indicating\
        \ whether the instance profile should only be used in credential passthrough\n\
        \  scenarios. If true, it means the instance profile contains an meta IAM\
        \ role which could assume a\n  wide range of roles. Therefore it should always\
        \ be used with authorization. This field is optional,\n  the default value\
        \ is `false`.\n:param skip_validation: bool (optional)\n  By default, Databricks\
        \ validates that it has sufficient permissions to launch instances with the\n\
        \  instance profile. This validation uses AWS dry-run mode for the RunInstances\
        \ API. If validation\n  fails with an error message that does not indicate\
        \ an IAM related permission issue, (e.g. Your\n  requested instance type\
        \ is not supported in your requested availability zone), you can pass this\n\
        \  flag to skip the validation and forcibly add the instance profile."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_profile_arn:
                  type: string
                  description: str The AWS ARN of the instance profile to register
                    with Databricks. This field is required.
                iam_role_arn:
                  type: string
                  description: 'str (optional) The AWS IAM role ARN of the role associated
                    with the instance profile. This field is required if your role
                    name and instance profile name do not match and you want to use
                    the instance profile with [Databricks SQL Serverless]. Otherwise,
                    this field is optional. [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html'
                is_meta_instance_profile:
                  type: boolean
                  description: bool (optional) Boolean flag indicating whether the
                    instance profile should only be used in credential passthrough
                    scenarios. If true, it means the instance profile contains an
                    meta IAM role which could assume a wide range of roles. Therefore
                    it should always be used with authorization. This field is optional,
                    the default value is `false`.
                skip_validation:
                  type: boolean
                  description: bool (optional) By default, Databricks validates that
                    it has sufficient permissions to launch instances with the instance
                    profile. This validation uses AWS dry-run mode for the RunInstances
                    API. If validation fails with an error message that does not indicate
                    an IAM related permission issue, (e.g. Your requested instance
                    type is not supported in your requested availability zone), you
                    can pass this flag to skip the validation and forcibly add the
                    instance profile.
      responses:
        '200':
          description: Success
  /api/2.0/instance-profiles/edit:
    post:
      operationId: edit
      summary: 'The only supported field to change is the optional IAM role ARN associated
        with the instance profile.

        It is required to specify the IAM role ARN if both of the following are true:'
      description: "The only supported field to change is the optional IAM role ARN\
        \ associated with the instance profile.\nIt is required to specify the IAM\
        \ role ARN if both of the following are true:\n\n* Your role name and instance\
        \ profile name do not match. The name is the part after the last slash in\n\
        each ARN. * You want to use the instance profile with [Databricks SQL Serverless].\n\
        \nTo understand where these fields are in the AWS console, see [Enable serverless\
        \ SQL warehouses].\n\nThis API is only available to admin users.\n\n[Databricks\
        \ SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html\n\
        [Enable serverless SQL warehouses]: https://docs.databricks.com/sql/admin/serverless.html\n\
        \n:param instance_profile_arn: str\n  The AWS ARN of the instance profile\
        \ to register with Databricks. This field is required.\n:param iam_role_arn:\
        \ str (optional)\n  The AWS IAM role ARN of the role associated with the instance\
        \ profile. This field is required if\n  your role name and instance profile\
        \ name do not match and you want to use the instance profile with\n  [Databricks\
        \ SQL Serverless].\n\n  Otherwise, this field is optional.\n\n  [Databricks\
        \ SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html\n\
        :param is_meta_instance_profile: bool (optional)\n  Boolean flag indicating\
        \ whether the instance profile should only be used in credential passthrough\n\
        \  scenarios. If true, it means the instance profile contains an meta IAM\
        \ role which could assume a\n  wide range of roles. Therefore it should always\
        \ be used with authorization. This field is optional,\n  the default value\
        \ is `false`."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_profile_arn:
                  type: string
                  description: str The AWS ARN of the instance profile to register
                    with Databricks. This field is required.
                iam_role_arn:
                  type: string
                  description: 'str (optional) The AWS IAM role ARN of the role associated
                    with the instance profile. This field is required if your role
                    name and instance profile name do not match and you want to use
                    the instance profile with [Databricks SQL Serverless]. Otherwise,
                    this field is optional. [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html'
                is_meta_instance_profile:
                  type: boolean
                  description: bool (optional) Boolean flag indicating whether the
                    instance profile should only be used in credential passthrough
                    scenarios. If true, it means the instance profile contains an
                    meta IAM role which could assume a wide range of roles. Therefore
                    it should always be used with authorization. This field is optional,
                    the default value is `false`.
      responses:
        '200':
          description: Success
  /api/2.0/instance-profiles/list:
    get:
      operationId: list
      summary: List the instance profiles that the calling user can use to launch
        a cluster.
      description: 'List the instance profiles that the calling user can use to launch
        a cluster.


        This API is available to all users.



        :returns: Iterator over :class:`InstanceProfile`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstanceProfile'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/instance-profiles/remove:
    post:
      operationId: remove
      summary: 'Remove the instance profile with the provided ARN. Existing clusters
        with this instance profile will

        continue to function.'
      description: "Remove the instance profile with the provided ARN. Existing clusters\
        \ with this instance profile will\ncontinue to function.\n\nThis API is only\
        \ accessible to admin users.\n\n:param instance_profile_arn: str\n  The ARN\
        \ of the instance profile to remove. This field is required."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_profile_arn:
                  type: string
                  description: str The ARN of the instance profile to remove. This
                    field is required.
      responses:
        '200':
          description: Success
  /api/2.0/libraries/all-cluster-statuses:
    get:
      operationId: all_cluster_statuses
      summary: 'Get the status of all libraries on all clusters. A status is returned
        for all libraries installed on

        this cluster via the API or the libraries UI.'
      description: 'Get the status of all libraries on all clusters. A status is returned
        for all libraries installed on

        this cluster via the API or the libraries UI.



        :returns: Iterator over :class:`ClusterLibraryStatuses`'
      tags:
      - compute
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterLibraryStatuses'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/libraries/cluster-status:
    get:
      operationId: cluster_status
      summary: 'Get the status of libraries on a cluster. A status is returned for
        all libraries installed on this

        cluster via the API or the libraries UI. The order of returned libraries is
        as follows: 1. Libraries

        set to be installed on this cluster, in the order that the libraries were
        added to the cluster, are

        returned first. 2. Libraries that were previously requested to be installed
        on this cluster or, but

        are now marked for removal, in no particular order, are returned last.'
      description: "Get the status of libraries on a cluster. A status is returned\
        \ for all libraries installed on this\ncluster via the API or the libraries\
        \ UI. The order of returned libraries is as follows: 1. Libraries\nset to\
        \ be installed on this cluster, in the order that the libraries were added\
        \ to the cluster, are\nreturned first. 2. Libraries that were previously requested\
        \ to be installed on this cluster or, but\nare now marked for removal, in\
        \ no particular order, are returned last.\n\n:param cluster_id: str\n  Unique\
        \ identifier of the cluster whose status should be retrieved.\n\n:returns:\
        \ Iterator over :class:`LibraryFullStatus`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str Unique identifier of the cluster whose status should be retrieved.
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/LibraryFullStatus'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/libraries/install:
    post:
      operationId: install
      summary: 'Add libraries to install on a cluster. The installation is asynchronous;
        it happens in the background

        after the completion of this request.'
      description: "Add libraries to install on a cluster. The installation is asynchronous;\
        \ it happens in the background\nafter the completion of this request.\n\n\
        :param cluster_id: str\n  Unique identifier for the cluster on which to install\
        \ these libraries.\n:param libraries: List[:class:`Library`]\n  The libraries\
        \ to install."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str Unique identifier for the cluster on which to install
                    these libraries.
                libraries:
                  type: array
                  items:
                    $ref: '#/components/schemas/Library'
                  description: List[:class:`Library`] The libraries to install.
      responses:
        '200':
          description: Success
  /api/2.0/libraries/uninstall:
    post:
      operationId: uninstall
      summary: 'Set libraries to uninstall from a cluster. The libraries won''t be
        uninstalled until the cluster is

        restarted. A request to uninstall a library that is not currently installed
        is ignored.'
      description: "Set libraries to uninstall from a cluster. The libraries won't\
        \ be uninstalled until the cluster is\nrestarted. A request to uninstall a\
        \ library that is not currently installed is ignored.\n\n:param cluster_id:\
        \ str\n  Unique identifier for the cluster on which to uninstall these libraries.\n\
        :param libraries: List[:class:`Library`]\n  The libraries to uninstall."
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str Unique identifier for the cluster on which to uninstall
                    these libraries.
                libraries:
                  type: array
                  items:
                    $ref: '#/components/schemas/Library'
                  description: List[:class:`Library`] The libraries to uninstall.
      responses:
        '200':
          description: Success
  /api/2.0/policies/clusters/enforce-compliance:
    post:
      operationId: enforce_compliance
      summary: 'Updates a cluster to be compliant with the current version of its
        policy. A cluster can be updated if

        it is in a `RUNNING` or `TERMINATED` state.'
      description: "Updates a cluster to be compliant with the current version of\
        \ its policy. A cluster can be updated if\nit is in a `RUNNING` or `TERMINATED`\
        \ state.\n\nIf a cluster is updated while in a `RUNNING` state, it will be\
        \ restarted so that the new attributes\ncan take effect.\n\nIf a cluster is\
        \ updated while in a `TERMINATED` state, it will remain `TERMINATED`. The\
        \ next time the\ncluster is started, the new attributes will take effect.\n\
        \nClusters created by the Databricks Jobs, DLT, or Models services cannot\
        \ be enforced by this API.\nInstead, use the \"Enforce job policy compliance\"\
        \ API to enforce policy compliance on jobs.\n\n:param cluster_id: str\n  The\
        \ ID of the cluster you want to enforce policy compliance on.\n:param validate_only:\
        \ bool (optional)\n  If set, previews the changes that would be made to a\
        \ cluster to enforce compliance but does not\n  update the cluster.\n\n:returns:\
        \ :class:`EnforceClusterComplianceResponse`"
      tags:
      - compute
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str The ID of the cluster you want to enforce policy
                    compliance on.
                validate_only:
                  type: boolean
                  description: bool (optional) If set, previews the changes that would
                    be made to a cluster to enforce compliance but does not update
                    the cluster.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/EnforceClusterComplianceResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policies/clusters/get-compliance:
    get:
      operationId: get_compliance
      summary: 'Returns the policy compliance status of a cluster. Clusters could
        be out of compliance if their policy

        was updated after the cluster was last edited.'
      description: "Returns the policy compliance status of a cluster. Clusters could\
        \ be out of compliance if their policy\nwas updated after the cluster was\
        \ last edited.\n\n:param cluster_id: str\n  The ID of the cluster to get the\
        \ compliance status\n\n:returns: :class:`GetClusterComplianceResponse`"
      tags:
      - compute
      parameters:
      - name: cluster_id
        description: str The ID of the cluster to get the compliance status
        required: true
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetClusterComplianceResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policies/clusters/list-compliance:
    get:
      operationId: list_compliance
      summary: 'Returns the policy compliance status of all clusters that use a given
        policy. Clusters could be out of

        compliance if their policy was updated after the cluster was last edited.'
      description: "Returns the policy compliance status of all clusters that use\
        \ a given policy. Clusters could be out of\ncompliance if their policy was\
        \ updated after the cluster was last edited.\n\n:param policy_id: str\n  Canonical\
        \ unique identifier for the cluster policy.\n:param page_size: int (optional)\n\
        \  Use this field to specify the maximum number of results to be returned\
        \ by the server. The server may\n  further constrain the maximum number of\
        \ results returned in a single page.\n:param page_token: str (optional)\n\
        \  A page token that can be used to navigate to the next page or previous\
        \ page as returned by\n  `next_page_token` or `prev_page_token`.\n\n:returns:\
        \ Iterator over :class:`ClusterCompliance`"
      tags:
      - compute
      parameters:
      - name: policy_id
        description: str Canonical unique identifier for the cluster policy.
        required: true
        schema:
          type: string
        in: query
      - name: page_size
        description: int (optional) Use this field to specify the maximum number of
          results to be returned by the server. The server may further constrain the
          maximum number of results returned in a single page.
        required: false
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) A page token that can be used to navigate to the
          next page or previous page as returned by `next_page_token` or `prev_page_token`.
        required: false
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterCompliance'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policy-families/{policy_family_id}:
    get:
      operationId: get
      summary: Retrieve the information for an policy family based on its identifier
        and version
      description: "Retrieve the information for an policy family based on its identifier\
        \ and version\n\n:param policy_family_id: str\n  The family ID about which\
        \ to retrieve information.\n:param version: int (optional)\n  The version\
        \ number for the family to fetch. Defaults to the latest version.\n\n:returns:\
        \ :class:`PolicyFamily`"
      tags:
      - compute
      parameters:
      - name: policy_family_id
        description: str The family ID about which to retrieve information.
        required: true
        schema:
          type: string
        in: path
      - name: version
        description: int (optional) The version number for the family to fetch. Defaults
          to the latest version.
        required: false
        schema:
          type: integer
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PolicyFamily'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policy-families:
    get:
      operationId: list
      summary: 'Returns the list of policy definition types available to use at their
        latest version. This API is

        paginated.'
      description: "Returns the list of policy definition types available to use at\
        \ their latest version. This API is\npaginated.\n\n:param max_results: int\
        \ (optional)\n  Maximum number of policy families to return.\n:param page_token:\
        \ str (optional)\n  A token that can be used to get the next page of results.\n\
        \n:returns: Iterator over :class:`PolicyFamily`"
      tags:
      - compute
      parameters:
      - name: max_results
        description: int (optional) Maximum number of policy families to return.
        required: false
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) A token that can be used to get the next page
          of results.
        required: false
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PolicyFamily'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
components:
  schemas:
    AddResponse:
      type: object
      description: AddResponse()
      properties: {}
    Adlsgen2Info:
      type: object
      description: A storage location in Adls Gen2
      properties:
        destination:
          type: string
          description: abfss destination, e.g. `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`.
    AutoScale:
      type: object
      description: 'AutoScale(max_workers: ''Optional[int]'' = None, min_workers:
        ''Optional[int]'' = None)'
      properties:
        max_workers:
          type: string
          description: The maximum number of workers to which the cluster can scale
            up when overloaded. Note that `max_workers` must be strictly greater than
            `min_workers`.
        min_workers:
          type: string
          description: The minimum number of workers to which the cluster can scale
            down when underutilized. It is also the initial number of workers the
            cluster will have after creation.
    AwsAttributes:
      type: object
      description: Attributes set during cluster creation which are related to Amazon
        Web Services.
      properties:
        availability:
          type: string
          description: ''
        ebs_volume_count:
          type: string
          description: The number of volumes launched for each instance. Users can
            choose up to 10 volumes. This feature is only enabled for supported node
            types. Legacy node types cannot specify custom EBS volumes. For node types
            with no instance store, at least one EBS volume needs to be specified;
            otherwise, cluster creation will fail.  These EBS volumes will be mounted
            at `/ebs0`, `/ebs1`, and etc. Instance store volumes will be mounted at
            `/local_disk0`, `/local_disk1`, and etc.  If EBS volumes are attached,
            Databricks will configure Spark to use only the EBS volumes for scratch
            storage because heterogenously sized scratch devices can lead to inefficient
            disk utilization. If no EBS volumes are attached, Databricks will configure
            Spark to use instance store volumes.  Please note that if EBS volumes
            are specified, then the Spark configuration `spark.local.dir` will be
            overridden.
        ebs_volume_iops:
          type: string
          description: If using gp3 volumes, what IOPS to use for the disk. If this
            is not set, the maximum performance of a gp2 volume with the same volume
            size will be used.
        ebs_volume_size:
          type: string
          description: The size of each EBS volume (in GiB) launched for each instance.
            For general purpose SSD, this value must be within the range 100 - 4096.
            For throughput optimized HDD, this value must be within the range 500
            - 4096.
        ebs_volume_throughput:
          type: string
          description: If using gp3 volumes, what throughput to use for the disk.
            If this is not set, the maximum performance of a gp2 volume with the same
            volume size will be used.
        ebs_volume_type:
          type: string
          description: 'The type of EBS volumes that will be launched with this cluster.  first_on_demand:
            Optional[int] = None """The first `first_on_demand` nodes of the cluster
            will be placed on on-demand instances. If this value is greater than 0,
            the cluster driver node in particular will be placed on an on-demand instance.
            If this value is greater than or equal to the current cluster size, all
            nodes will be placed on on-demand instances. If this value is less than
            the current cluster size, `first_on_demand` nodes will be placed on on-demand
            instances and the remainder will be placed on `availability` instances.
            Note that this value does not affect cluster size and cannot currently
            be mutated over the lifetime of a cluster.'
        first_on_demand:
          type: string
          description: The first `first_on_demand` nodes of the cluster will be placed
            on on-demand instances. If this value is greater than 0, the cluster driver
            node in particular will be placed on an on-demand instance. If this value
            is greater than or equal to the current cluster size, all nodes will be
            placed on on-demand instances. If this value is less than the current
            cluster size, `first_on_demand` nodes will be placed on on-demand instances
            and the remainder will be placed on `availability` instances. Note that
            this value does not affect cluster size and cannot currently be mutated
            over the lifetime of a cluster.
        instance_profile_arn:
          type: string
          description: Nodes for this cluster will only be placed on AWS instances
            with this instance profile. If ommitted, nodes will be placed on instances
            without an IAM instance profile. The instance profile must have previously
            been added to the Databricks environment by an account administrator.  This
            feature may only be available to certain customer plans.
        spot_bid_price_percent:
          type: string
          description: The bid price for AWS spot instances, as a percentage of the
            corresponding instance type's on-demand price. For example, if this field
            is set to 50, and the cluster needs a new `r3.xlarge` spot instance, then
            the bid price is half of the price of on-demand `r3.xlarge` instances.
            Similarly, if this field is set to 200, the bid price is twice the price
            of on-demand `r3.xlarge` instances. If not specified, the default value
            is 100. When spot instances are requested for this cluster, only spot
            instances whose bid price percentage matches this field will be considered.
            Note that, for safety, we enforce this field to be no more than 10000.
        zone_id:
          type: string
          description: Identifier for the availability zone/datacenter in which the
            cluster resides. This string will be of a form like "us-west-2a". The
            provided availability zone must be in the same region as the Databricks
            deployment. For example, "us-west-2a" is not a valid zone id if the Databricks
            deployment resides in the "us-east-1" region. This is an optional field
            at cluster creation, and if not specified, a default zone will be used.
            If the zone specified is "auto", will try to place cluster in a zone with
            high availability, and will retry placement in a different AZ if there
            is not enough capacity.  The list of available zones as well as the default
            value can be found by using the `List Zones` method.
    AwsAvailability:
      type: string
      description: 'Availability type used for all subsequent nodes past the `first_on_demand`
        ones.


        Note: If `first_on_demand` is zero, this availability type will be used for
        the entire cluster.'
      enum:
      - ON_DEMAND
      - SPOT
      - SPOT_WITH_FALLBACK
    AzureAttributes:
      type: object
      description: Attributes set during cluster creation which are related to Microsoft
        Azure.
      properties:
        availability:
          type: string
          description: 'Availability type used for all subsequent nodes past the `first_on_demand`
            ones. Note: If `first_on_demand` is zero, this availability type will
            be used for the entire cluster.'
        first_on_demand:
          type: string
          description: The first `first_on_demand` nodes of the cluster will be placed
            on on-demand instances. This value should be greater than 0, to make sure
            the cluster driver node is placed on an on-demand instance. If this value
            is greater than or equal to the current cluster size, all nodes will be
            placed on on-demand instances. If this value is less than the current
            cluster size, `first_on_demand` nodes will be placed on on-demand instances
            and the remainder will be placed on `availability` instances. Note that
            this value does not affect cluster size and cannot currently be mutated
            over the lifetime of a cluster.
        log_analytics_info:
          type: string
          description: 'Defines values necessary to configure and run Azure Log Analytics
            agent  spot_bid_max_price: Optional[float] = None """The max bid price
            to be used for Azure spot instances. The Max price for the bid cannot
            be higher than the on-demand price of the instance. If not specified,
            the default value is -1, which specifies that the instance cannot be evicted
            on the basis of price, and only on the basis of availability. Further,
            the value should > 0 or -1.'
        spot_bid_max_price:
          type: string
          description: The max bid price to be used for Azure spot instances. The
            Max price for the bid cannot be higher than the on-demand price of the
            instance. If not specified, the default value is -1, which specifies that
            the instance cannot be evicted on the basis of price, and only on the
            basis of availability. Further, the value should > 0 or -1.
    AzureAvailability:
      type: string
      description: 'Availability type used for all subsequent nodes past the `first_on_demand`
        ones. Note: If

        `first_on_demand` is zero, this availability type will be used for the entire
        cluster.'
      enum:
      - ON_DEMAND_AZURE
      - SPOT_AZURE
      - SPOT_WITH_FALLBACK_AZURE
    CancelResponse:
      type: object
      description: CancelResponse()
      properties: {}
    ChangeClusterOwnerResponse:
      type: object
      description: ChangeClusterOwnerResponse()
      properties: {}
    ClientsTypes:
      type: object
      description: 'ClientsTypes(jobs: ''Optional[bool]'' = None, notebooks: ''Optional[bool]''
        = None)'
      properties:
        jobs:
          type: string
          description: 'With jobs set, the cluster can be used for jobs  notebooks:
            Optional[bool] = None With notebooks set, this cluster can be used for
            notebooks'
        notebooks:
          type: string
          description: 'With notebooks set, this cluster can be used for notebooks  def
            as_dict(self) -> dict: Serializes the ClientsTypes into a dictionary suitable
            for use as a JSON request body.'
    CloneCluster:
      type: object
      description: 'CloneCluster(source_cluster_id: ''str'')'
      properties:
        source_cluster_id:
          type: string
          description: 'The cluster that is being cloned.  def as_dict(self) -> dict:
            Serializes the CloneCluster into a dictionary suitable for use as a JSON
            request body.'
    CloudProviderNodeInfo:
      type: object
      description: 'CloudProviderNodeInfo(status: ''Optional[List[CloudProviderNodeStatus]]''
        = None)'
      properties:
        status:
          type: string
          description: 'Status as reported by the cloud provider  def as_dict(self)
            -> dict: Serializes the CloudProviderNodeInfo into a dictionary suitable
            for use as a JSON request body.'
    CloudProviderNodeStatus:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - NotAvailableInRegion
      - NotEnabledOnSubscription
    ClusterAccessControlRequest:
      type: object
      description: 'ClusterAccessControlRequest(group_name: ''Optional[str]'' = None,
        permission_level: ''Optional[ClusterPermissionLevel]'' = None, service_principal_name:
        ''Optional[str]'' = None, user_name: ''Optional[str]'' = None)'
      properties:
        group_name:
          type: string
          description: 'name of the group  permission_level: Optional[ClusterPermissionLevel]
            = None  service_principal_name: Optional[str] = None application ID of
            a service principal'
        permission_level:
          type: string
          description: ''
        service_principal_name:
          type: string
          description: 'application ID of a service principal  user_name: Optional[str]
            = None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            ClusterAccessControlRequest into a dictionary suitable for use as a JSON
            request body.'
    ClusterAccessControlResponse:
      type: object
      description: 'ClusterAccessControlResponse(all_permissions: ''Optional[List[ClusterPermission]]''
        = None, display_name: ''Optional[str]'' = None, group_name: ''Optional[str]''
        = None, service_principal_name: ''Optional[str]'' = None, user_name: ''Optional[str]''
        = None)'
      properties:
        all_permissions:
          type: string
          description: 'All permissions.  display_name: Optional[str] = None Display
            name of the user or service principal.'
        display_name:
          type: string
          description: 'Display name of the user or service principal.  group_name:
            Optional[str] = None name of the group'
        group_name:
          type: string
          description: 'name of the group  service_principal_name: Optional[str] =
            None Name of the service principal.'
        service_principal_name:
          type: string
          description: 'Name of the service principal.  user_name: Optional[str] =
            None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            ClusterAccessControlResponse into a dictionary suitable for use as a JSON
            request body.'
    ClusterAttributes:
      type: object
      description: 'Common set of attributes set during cluster creation. These attributes
        cannot be changed over

        the lifetime of a cluster.'
      properties:
        spark_version:
          type: string
          description: The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A
            list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions
            API call.
        autotermination_minutes:
          type: string
          description: Automatically terminates the cluster after it is inactive for
            this time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000 minutes.
            Users can also set this value to 0 to explicitly disable automatic termination.
        aws_attributes:
          type: string
          description: Attributes related to clusters running on Amazon Web Services.
            If not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          type: string
          description: Attributes related to clusters running on Microsoft Azure.
            If not specified at cluster creation, a set of default values will be
            used.
        cluster_log_conf:
          type: string
          description: The configuration for delivering spark logs to a long-term
            storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for one
            cluster. If the conf is given, the logs will be delivered to the destination
            every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`,
            while the destination of executor logs is `$destination/$clusterId/executor`.
        cluster_name:
          type: string
          description: Cluster name requested by the user. This doesn't have to be
            unique. If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set based
            on the job and job run IDs.
        custom_tags:
          type: string
          description: 'Additional tags for cluster resources. Databricks will tag
            all cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes:  - Currently, Databricks allows
            at most 45 custom tags  - Clusters can only reuse cloud resources if the
            resources'' tags are a subset of the cluster tags'
        data_security_mode:
          type: string
          description: ''
        docker_image:
          type: string
          description: 'Custom docker image BYOC  driver_instance_pool_id: Optional[str]
            = None """The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.'
        driver_instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        driver_node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        enable_elastic_disk:
          type: string
          description: 'Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are running
            low on disk space. This feature requires specific AWS permissions to function
            correctly - refer to the User Guide for more details.'
        enable_local_disk_encryption:
          type: string
          description: 'Whether to enable LUKS on cluster VMs'' local disks  gcp_attributes:
            Optional[GcpAttributes] = None """Attributes related to clusters running
            on Google Cloud Platform. If not specified at cluster creation, a set
            of default values will be used.'
        gcp_attributes:
          type: string
          description: Attributes related to clusters running on Google Cloud Platform.
            If not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: string
          description: The configuration for storing init scripts. Any number of destinations
            can be specified. The scripts are executed sequentially in the order provided.
            If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        is_single_node:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  When
            set to true, Databricks will automatically set single node related `custom_tags`,
            `spark_conf`, and `num_workers`
        kind:
          type: string
          description: ''
        node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        policy_id:
          type: string
          description: 'The ID of the cluster policy used to create the cluster if
            applicable.  remote_disk_throughput: Optional[int] = None """If set, what
            the configurable throughput (in Mb/s) for the remote disk is. Currently
            only supported for GCP HYPERDISK_BALANCED disks.'
        remote_disk_throughput:
          type: string
          description: If set, what the configurable throughput (in Mb/s) for the
            remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          type: string
          description: Determines the cluster's runtime engine, either standard or
            Photon.  This field is not compatible with legacy `spark_version` values
            that contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`.  If left unspecified, the runtime engine
            defaults to standard unless the spark_version contains -photon-, in which
            case Photon will be used.
        single_user_name:
          type: string
          description: 'Single user name if data_security_mode is `SINGLE_USER`  spark_conf:
            Optional[Dict[str, str]] = None """An object containing a set of optional,
            user-specified Spark configuration key-value pairs. Users can also pass
            in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.'
        spark_conf:
          type: string
          description: An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of extra
            JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.
        spark_env_vars:
          type: string
          description: 'An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the form
            (X,Y) will be exported as is (i.e., `export X=''Y''`) while launching
            the driver and workers.  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`,
            we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the
            example below. This ensures that all default databricks managed environmental
            variables are included as well.  Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS": "/local_disk0"}`
            or `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`'
        ssh_public_keys:
          type: string
          description: SSH public key contents that will be added to each Spark node
            in this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
        total_initial_remote_disk_size:
          type: string
          description: If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks.
        use_ml_runtime:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  `effective_spark_version`
            is determined by `spark_version` (DBR release), this field `use_ml_runtime`,
            and whether `node_type_id` is gpu node or not.
        workload_type:
          type: string
          description: ''
    ClusterCompliance:
      type: object
      description: 'ClusterCompliance(cluster_id: ''str'', is_compliant: ''Optional[bool]''
        = None, violations: ''Optional[Dict[str, str]]'' = None)'
      properties:
        cluster_id:
          type: string
          description: 'Canonical unique identifier for a cluster.  is_compliant:
            Optional[bool] = None Whether this cluster is in compliance with the latest
            version of its policy.'
        is_compliant:
          type: string
          description: 'Whether this cluster is in compliance with the latest version
            of its policy.  violations: Optional[Dict[str, str]] = None """An object
            containing key-value mappings representing the first 200 policy validation
            errors. The keys indicate the path where the policy validation error is
            occurring. The values indicate an error message describing the policy
            validation error.'
        violations:
          type: string
          description: An object containing key-value mappings representing the first
            200 policy validation errors. The keys indicate the path where the policy
            validation error is occurring. The values indicate an error message describing
            the policy validation error.
    ClusterDetails:
      type: object
      description: Describes all of the metadata about a single Spark cluster in Databricks.
      properties:
        autoscale:
          type: string
          description: 'Parameters needed in order to automatically scale clusters
            up and down based on load. Note: autoscaling works best with DB runtime
            versions 3.0 or later.'
        autotermination_minutes:
          type: string
          description: Automatically terminates the cluster after it is inactive for
            this time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000 minutes.
            Users can also set this value to 0 to explicitly disable automatic termination.
        aws_attributes:
          type: string
          description: Attributes related to clusters running on Amazon Web Services.
            If not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          type: string
          description: Attributes related to clusters running on Microsoft Azure.
            If not specified at cluster creation, a set of default values will be
            used.
        cluster_cores:
          type: string
          description: Number of CPU cores available for this cluster. Note that this
            can be fractional, e.g. 7.5 cores, since certain node types are configured
            to share cores between Spark nodes on the same instance.
        cluster_id:
          type: string
          description: Canonical identifier for the cluster. This id is retained during
            cluster restarts and resizes, while each new cluster has a globally unique
            id.
        cluster_log_conf:
          type: string
          description: The configuration for delivering spark logs to a long-term
            storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for one
            cluster. If the conf is given, the logs will be delivered to the destination
            every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`,
            while the destination of executor logs is `$destination/$clusterId/executor`.
        cluster_log_status:
          type: string
          description: 'Cluster log delivery status.  cluster_memory_mb: Optional[int]
            = None Total amount of cluster memory, in megabytes'
        cluster_memory_mb:
          type: string
          description: 'Total amount of cluster memory, in megabytes  cluster_name:
            Optional[str] = None """Cluster name requested by the user. This doesn''t
            have to be unique. If not specified at creation, the cluster name will
            be an empty string. For job clusters, the cluster name is automatically
            set based on the job and job run IDs.'
        cluster_name:
          type: string
          description: Cluster name requested by the user. This doesn't have to be
            unique. If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set based
            on the job and job run IDs.
        cluster_source:
          type: string
          description: Determines whether the cluster was created by a user through
            the UI, created by the Databricks Jobs Scheduler, or through an API request.
        creator_user_name:
          type: string
          description: Creator user name. The field won't be included in the response
            if the user has already been deleted.
        custom_tags:
          type: string
          description: 'Additional tags for cluster resources. Databricks will tag
            all cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes:  - Currently, Databricks allows
            at most 45 custom tags  - Clusters can only reuse cloud resources if the
            resources'' tags are a subset of the cluster tags'
        data_security_mode:
          type: string
          description: ''
        default_tags:
          type: string
          description: 'Tags that are added by Databricks regardless of any `custom_tags`,
            including:  - Vendor: Databricks  - Creator: <username_of_creator>  -
            ClusterName: <name_of_cluster>  - ClusterId: <id_of_cluster>  - Name:
            <Databricks internal use>'
        docker_image:
          type: string
          description: 'Custom docker image BYOC  driver: Optional[SparkNode] = None
            """Node on which the Spark driver resides. The driver node contains the
            Spark master and the Databricks application that manages the per-notebook
            Spark REPLs.'
        driver:
          type: string
          description: Node on which the Spark driver resides. The driver node contains
            the Spark master and the Databricks application that manages the per-notebook
            Spark REPLs.
        driver_instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        driver_node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        enable_elastic_disk:
          type: string
          description: 'Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are running
            low on disk space. This feature requires specific AWS permissions to function
            correctly - refer to the User Guide for more details.'
        enable_local_disk_encryption:
          type: string
          description: 'Whether to enable LUKS on cluster VMs'' local disks  executors:
            Optional[List[SparkNode]] = None Nodes on which the Spark executors reside.'
        executors:
          type: string
          description: 'Nodes on which the Spark executors reside.  gcp_attributes:
            Optional[GcpAttributes] = None """Attributes related to clusters running
            on Google Cloud Platform. If not specified at cluster creation, a set
            of default values will be used.'
        gcp_attributes:
          type: string
          description: Attributes related to clusters running on Google Cloud Platform.
            If not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: string
          description: The configuration for storing init scripts. Any number of destinations
            can be specified. The scripts are executed sequentially in the order provided.
            If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        is_single_node:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  When
            set to true, Databricks will automatically set single node related `custom_tags`,
            `spark_conf`, and `num_workers`
        jdbc_port:
          type: string
          description: Port on which Spark JDBC server is listening, in the driver
            nod. No service will be listeningon on this port in executor nodes.
        kind:
          type: string
          description: ''
        last_restarted_time:
          type: string
          description: 'the timestamp that the cluster was started/restarted  last_state_loss_time:
            Optional[int] = None Time when the cluster driver last lost its state
            (due to a restart or driver failure).'
        last_state_loss_time:
          type: string
          description: 'Time when the cluster driver last lost its state (due to a
            restart or driver failure).  node_type_id: Optional[str] = None """This
            field encodes, through a single value, the resources available to each
            of the Spark nodes in this cluster. For example, the Spark nodes can be
            provisioned and optimized for memory or compute intensive workloads. A
            list of available node types can be retrieved by using the :method:clusters/listNodeTypes
            API call.'
        node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        num_workers:
          type: string
          description: 'Number of worker nodes that this cluster should have. A cluster
            has one Spark Driver and `num_workers` Executors for a total of `num_workers`
            + 1 Spark nodes.  Note: When reading the properties of a cluster, this
            field reflects the desired number of workers rather than the actual current
            number of workers. For instance, if a cluster is resized from 5 to 10
            workers, this field will immediately be updated to reflect the target
            size of 10 workers, whereas the workers listed in `spark_info` will gradually
            increase from 5 to 10 as the new nodes are provisioned.'
        policy_id:
          type: string
          description: 'The ID of the cluster policy used to create the cluster if
            applicable.  remote_disk_throughput: Optional[int] = None """If set, what
            the configurable throughput (in Mb/s) for the remote disk is. Currently
            only supported for GCP HYPERDISK_BALANCED disks.'
        remote_disk_throughput:
          type: string
          description: If set, what the configurable throughput (in Mb/s) for the
            remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          type: string
          description: Determines the cluster's runtime engine, either standard or
            Photon.  This field is not compatible with legacy `spark_version` values
            that contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`.  If left unspecified, the runtime engine
            defaults to standard unless the spark_version contains -photon-, in which
            case Photon will be used.
        single_user_name:
          type: string
          description: 'Single user name if data_security_mode is `SINGLE_USER`  spark_conf:
            Optional[Dict[str, str]] = None """An object containing a set of optional,
            user-specified Spark configuration key-value pairs. Users can also pass
            in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.'
        spark_conf:
          type: string
          description: An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of extra
            JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.
        spark_context_id:
          type: string
          description: A canonical SparkContext identifier. This value *does* change
            when the Spark driver restarts. The pair `(cluster_id, spark_context_id)`
            is a globally unique identifier over all Spark contexts.
        spark_env_vars:
          type: string
          description: 'An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the form
            (X,Y) will be exported as is (i.e., `export X=''Y''`) while launching
            the driver and workers.  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`,
            we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the
            example below. This ensures that all default databricks managed environmental
            variables are included as well.  Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS": "/local_disk0"}`
            or `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`'
        spark_version:
          type: string
          description: The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A
            list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions
            API call.
        spec:
          type: string
          description: 'The spec contains a snapshot of the latest user specified
            settings that were used to create/edit the cluster. Note: not included
            in the response of the ListClusters API.'
        ssh_public_keys:
          type: string
          description: SSH public key contents that will be added to each Spark node
            in this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
        start_time:
          type: string
          description: Time (in epoch milliseconds) when the cluster creation request
            was received (when the cluster entered a `PENDING` state).
        state:
          type: string
          description: 'Current state of the cluster.  state_message: Optional[str]
            = None """A message associated with the most recent state transition (e.g.,
            the reason why the cluster entered a `TERMINATED` state).'
        state_message:
          type: string
          description: A message associated with the most recent state transition
            (e.g., the reason why the cluster entered a `TERMINATED` state).
        terminated_time:
          type: string
          description: 'Time (in epoch milliseconds) when the cluster was terminated,
            if applicable.  termination_reason: Optional[TerminationReason] = None
            """Information about why the cluster was terminated. This field only appears
            when the cluster is in a `TERMINATING` or `TERMINATED` state.'
        termination_reason:
          type: string
          description: Information about why the cluster was terminated. This field
            only appears when the cluster is in a `TERMINATING` or `TERMINATED` state.
        total_initial_remote_disk_size:
          type: string
          description: If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks.
        use_ml_runtime:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  `effective_spark_version`
            is determined by `spark_version` (DBR release), this field `use_ml_runtime`,
            and whether `node_type_id` is gpu node or not.
        workload_type:
          type: string
          description: ''
    ClusterEvent:
      type: object
      description: 'ClusterEvent(cluster_id: ''str'', data_plane_event_details: ''Optional[DataPlaneEventDetails]''
        = None, details: ''Optional[EventDetails]'' = None, timestamp: ''Optional[int]''
        = None, type: ''Optional[EventType]'' = None)'
      properties:
        cluster_id:
          type: string
          description: ''
        data_plane_event_details:
          type: string
          description: ''
        details:
          type: string
          description: ''
        timestamp:
          type: string
          description: The timestamp when the event occurred, stored as the number
            of milliseconds since the Unix epoch. If not provided, this will be assigned
            by the Timeline service.
        type:
          type: string
          description: ''
    ClusterLibraryStatuses:
      type: object
      description: 'ClusterLibraryStatuses(cluster_id: ''Optional[str]'' = None, library_statuses:
        ''Optional[List[LibraryFullStatus]]'' = None)'
      properties:
        cluster_id:
          type: string
          description: 'Unique identifier for the cluster.  library_statuses: Optional[List[LibraryFullStatus]]
            = None Status of all libraries on the cluster.'
        library_statuses:
          type: string
          description: 'Status of all libraries on the cluster.  def as_dict(self)
            -> dict: Serializes the ClusterLibraryStatuses into a dictionary suitable
            for use as a JSON request body.'
    ClusterLogConf:
      type: object
      description: Cluster log delivery config
      properties:
        dbfs:
          type: string
          description: 'destination needs to be provided. e.g. `{ "dbfs" : { "destination"
            : "dbfs:/home/cluster_log" } }`'
        s3:
          type: string
          description: 'destination and either the region or endpoint need to be provided.
            e.g. `{ "s3": { "destination : "s3://cluster_log_bucket/prefix", "region"
            : "us-west-2" } }` Cluster iam role is used to access s3, please make
            sure the cluster iam role in `instance_profile_arn` has permission to
            write data to the s3 destination.'
        volumes:
          type: string
          description: 'destination needs to be provided, e.g. `{ "volumes": { "destination":
            /Volumes/catalog/schema/volume/cluster_log" } }`'
    ClusterPermission:
      type: object
      description: 'ClusterPermission(inherited: ''Optional[bool]'' = None, inherited_from_object:
        ''Optional[List[str]]'' = None, permission_level: ''Optional[ClusterPermissionLevel]''
        = None)'
      properties:
        inherited:
          type: string
          description: ''
        inherited_from_object:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    ClusterPermissionLevel:
      type: string
      description: Permission level
      enum:
      - CAN_ATTACH_TO
      - CAN_MANAGE
      - CAN_RESTART
    ClusterPermissions:
      type: object
      description: 'ClusterPermissions(access_control_list: ''Optional[List[ClusterAccessControlResponse]]''
        = None, object_id: ''Optional[str]'' = None, object_type: ''Optional[str]''
        = None)'
      properties:
        access_control_list:
          type: string
          description: ''
        object_id:
          type: string
          description: ''
        object_type:
          type: string
          description: ''
    ClusterPermissionsDescription:
      type: object
      description: 'ClusterPermissionsDescription(description: ''Optional[str]'' =
        None, permission_level: ''Optional[ClusterPermissionLevel]'' = None)'
      properties:
        description:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    ClusterPolicyAccessControlRequest:
      type: object
      description: 'ClusterPolicyAccessControlRequest(group_name: ''Optional[str]''
        = None, permission_level: ''Optional[ClusterPolicyPermissionLevel]'' = None,
        service_principal_name: ''Optional[str]'' = None, user_name: ''Optional[str]''
        = None)'
      properties:
        group_name:
          type: string
          description: 'name of the group  permission_level: Optional[ClusterPolicyPermissionLevel]
            = None  service_principal_name: Optional[str] = None application ID of
            a service principal'
        permission_level:
          type: string
          description: ''
        service_principal_name:
          type: string
          description: 'application ID of a service principal  user_name: Optional[str]
            = None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            ClusterPolicyAccessControlRequest into a dictionary suitable for use as
            a JSON request body.'
    ClusterPolicyAccessControlResponse:
      type: object
      description: 'ClusterPolicyAccessControlResponse(all_permissions: ''Optional[List[ClusterPolicyPermission]]''
        = None, display_name: ''Optional[str]'' = None, group_name: ''Optional[str]''
        = None, service_principal_name: ''Optional[str]'' = None, user_name: ''Optional[str]''
        = None)'
      properties:
        all_permissions:
          type: string
          description: 'All permissions.  display_name: Optional[str] = None Display
            name of the user or service principal.'
        display_name:
          type: string
          description: 'Display name of the user or service principal.  group_name:
            Optional[str] = None name of the group'
        group_name:
          type: string
          description: 'name of the group  service_principal_name: Optional[str] =
            None Name of the service principal.'
        service_principal_name:
          type: string
          description: 'Name of the service principal.  user_name: Optional[str] =
            None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            ClusterPolicyAccessControlResponse into a dictionary suitable for use
            as a JSON request body.'
    ClusterPolicyPermission:
      type: object
      description: 'ClusterPolicyPermission(inherited: ''Optional[bool]'' = None,
        inherited_from_object: ''Optional[List[str]]'' = None, permission_level: ''Optional[ClusterPolicyPermissionLevel]''
        = None)'
      properties:
        inherited:
          type: string
          description: ''
        inherited_from_object:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    ClusterPolicyPermissionLevel:
      type: string
      description: Permission level
      enum:
      - CAN_USE
    ClusterPolicyPermissions:
      type: object
      description: 'ClusterPolicyPermissions(access_control_list: ''Optional[List[ClusterPolicyAccessControlResponse]]''
        = None, object_id: ''Optional[str]'' = None, object_type: ''Optional[str]''
        = None)'
      properties:
        access_control_list:
          type: string
          description: ''
        object_id:
          type: string
          description: ''
        object_type:
          type: string
          description: ''
    ClusterPolicyPermissionsDescription:
      type: object
      description: 'ClusterPolicyPermissionsDescription(description: ''Optional[str]''
        = None, permission_level: ''Optional[ClusterPolicyPermissionLevel]'' = None)'
      properties:
        description:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    ClusterSettingsChange:
      type: object
      description: 'Represents a change to the cluster settings required for the cluster
        to become compliant with

        its policy.'
      properties:
        field:
          type: string
          description: 'The field where this change would be made.  new_value: Optional[str]
            = None """The new value of this field after enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string. This
            is intended to be read by a human. The typed new value of this field can
            be retrieved by reading the settings field in the API response.'
        new_value:
          type: string
          description: The new value of this field after enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string. This
            is intended to be read by a human. The typed new value of this field can
            be retrieved by reading the settings field in the API response.
        previous_value:
          type: string
          description: The previous value of this field before enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string. This
            is intended to be read by a human. The type of the field can be retrieved
            by reading the settings field in the API response.
    ClusterSize:
      type: object
      description: 'ClusterSize(autoscale: ''Optional[AutoScale]'' = None, num_workers:
        ''Optional[int]'' = None)'
      properties:
        autoscale:
          type: string
          description: 'Parameters needed in order to automatically scale clusters
            up and down based on load. Note: autoscaling works best with DB runtime
            versions 3.0 or later.'
        num_workers:
          type: string
          description: 'Number of worker nodes that this cluster should have. A cluster
            has one Spark Driver and `num_workers` Executors for a total of `num_workers`
            + 1 Spark nodes.  Note: When reading the properties of a cluster, this
            field reflects the desired number of workers rather than the actual current
            number of workers. For instance, if a cluster is resized from 5 to 10
            workers, this field will immediately be updated to reflect the target
            size of 10 workers, whereas the workers listed in `spark_info` will gradually
            increase from 5 to 10 as the new nodes are provisioned.'
    ClusterSource:
      type: string
      description: 'Determines whether the cluster was created by a user through the
        UI, created by the Databricks

        Jobs Scheduler, or through an API request. This is the same as cluster_creator,
        but read only.'
      enum:
      - API
      - JOB
      - MODELS
      - PIPELINE
      - PIPELINE_MAINTENANCE
      - SQL
      - UI
    ClusterSpec:
      type: object
      description: 'Contains a snapshot of the latest user specified settings that
        were used to create/edit the

        cluster.'
      properties:
        apply_policy_default_values:
          type: string
          description: When set to true, fixed and default values from the policy
            will be used for fields that are omitted. When set to false, only fixed
            values from the policy will be applied.
        autoscale:
          type: string
          description: 'Parameters needed in order to automatically scale clusters
            up and down based on load. Note: autoscaling works best with DB runtime
            versions 3.0 or later.'
        autotermination_minutes:
          type: string
          description: Automatically terminates the cluster after it is inactive for
            this time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000 minutes.
            Users can also set this value to 0 to explicitly disable automatic termination.
        aws_attributes:
          type: string
          description: Attributes related to clusters running on Amazon Web Services.
            If not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          type: string
          description: Attributes related to clusters running on Microsoft Azure.
            If not specified at cluster creation, a set of default values will be
            used.
        cluster_log_conf:
          type: string
          description: The configuration for delivering spark logs to a long-term
            storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for one
            cluster. If the conf is given, the logs will be delivered to the destination
            every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`,
            while the destination of executor logs is `$destination/$clusterId/executor`.
        cluster_name:
          type: string
          description: Cluster name requested by the user. This doesn't have to be
            unique. If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set based
            on the job and job run IDs.
        custom_tags:
          type: string
          description: 'Additional tags for cluster resources. Databricks will tag
            all cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes:  - Currently, Databricks allows
            at most 45 custom tags  - Clusters can only reuse cloud resources if the
            resources'' tags are a subset of the cluster tags'
        data_security_mode:
          type: string
          description: ''
        docker_image:
          type: string
          description: 'Custom docker image BYOC  driver_instance_pool_id: Optional[str]
            = None """The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.'
        driver_instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        driver_node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        enable_elastic_disk:
          type: string
          description: 'Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are running
            low on disk space. This feature requires specific AWS permissions to function
            correctly - refer to the User Guide for more details.'
        enable_local_disk_encryption:
          type: string
          description: 'Whether to enable LUKS on cluster VMs'' local disks  gcp_attributes:
            Optional[GcpAttributes] = None """Attributes related to clusters running
            on Google Cloud Platform. If not specified at cluster creation, a set
            of default values will be used.'
        gcp_attributes:
          type: string
          description: Attributes related to clusters running on Google Cloud Platform.
            If not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: string
          description: The configuration for storing init scripts. Any number of destinations
            can be specified. The scripts are executed sequentially in the order provided.
            If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        is_single_node:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  When
            set to true, Databricks will automatically set single node related `custom_tags`,
            `spark_conf`, and `num_workers`
        kind:
          type: string
          description: ''
        node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        num_workers:
          type: string
          description: 'Number of worker nodes that this cluster should have. A cluster
            has one Spark Driver and `num_workers` Executors for a total of `num_workers`
            + 1 Spark nodes.  Note: When reading the properties of a cluster, this
            field reflects the desired number of workers rather than the actual current
            number of workers. For instance, if a cluster is resized from 5 to 10
            workers, this field will immediately be updated to reflect the target
            size of 10 workers, whereas the workers listed in `spark_info` will gradually
            increase from 5 to 10 as the new nodes are provisioned.'
        policy_id:
          type: string
          description: 'The ID of the cluster policy used to create the cluster if
            applicable.  remote_disk_throughput: Optional[int] = None """If set, what
            the configurable throughput (in Mb/s) for the remote disk is. Currently
            only supported for GCP HYPERDISK_BALANCED disks.'
        remote_disk_throughput:
          type: string
          description: If set, what the configurable throughput (in Mb/s) for the
            remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          type: string
          description: Determines the cluster's runtime engine, either standard or
            Photon.  This field is not compatible with legacy `spark_version` values
            that contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`.  If left unspecified, the runtime engine
            defaults to standard unless the spark_version contains -photon-, in which
            case Photon will be used.
        single_user_name:
          type: string
          description: 'Single user name if data_security_mode is `SINGLE_USER`  spark_conf:
            Optional[Dict[str, str]] = None """An object containing a set of optional,
            user-specified Spark configuration key-value pairs. Users can also pass
            in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.'
        spark_conf:
          type: string
          description: An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of extra
            JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.
        spark_env_vars:
          type: string
          description: 'An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the form
            (X,Y) will be exported as is (i.e., `export X=''Y''`) while launching
            the driver and workers.  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`,
            we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the
            example below. This ensures that all default databricks managed environmental
            variables are included as well.  Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS": "/local_disk0"}`
            or `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`'
        spark_version:
          type: string
          description: The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A
            list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions
            API call.
        ssh_public_keys:
          type: string
          description: SSH public key contents that will be added to each Spark node
            in this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
        total_initial_remote_disk_size:
          type: string
          description: If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks.
        use_ml_runtime:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  `effective_spark_version`
            is determined by `spark_version` (DBR release), this field `use_ml_runtime`,
            and whether `node_type_id` is gpu node or not.
        workload_type:
          type: string
          description: ''
    CommandStatus:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - Cancelled
      - Cancelling
      - Error
      - Finished
      - Queued
      - Running
    CommandStatusResponse:
      type: object
      description: 'CommandStatusResponse(id: ''Optional[str]'' = None, results: ''Optional[Results]''
        = None, status: ''Optional[CommandStatus]'' = None)'
      properties:
        id:
          type: string
          description: ''
        results:
          type: string
          description: ''
        status:
          type: string
          description: ''
    ContextStatus:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - Error
      - Pending
      - Running
    ContextStatusResponse:
      type: object
      description: 'ContextStatusResponse(id: ''Optional[str]'' = None, status: ''Optional[ContextStatus]''
        = None)'
      properties:
        id:
          type: string
          description: ''
        status:
          type: string
          description: ''
    CreateClusterResponse:
      type: object
      description: 'CreateClusterResponse(cluster_id: ''Optional[str]'' = None)'
      properties:
        cluster_id:
          type: string
          description: ''
    CreateInstancePoolResponse:
      type: object
      description: 'CreateInstancePoolResponse(instance_pool_id: ''Optional[str]''
        = None)'
      properties:
        instance_pool_id:
          type: string
          description: 'The ID of the created instance pool.  def as_dict(self) ->
            dict: Serializes the CreateInstancePoolResponse into a dictionary suitable
            for use as a JSON request body.'
    CreatePolicyResponse:
      type: object
      description: 'CreatePolicyResponse(policy_id: ''Optional[str]'' = None)'
      properties:
        policy_id:
          type: string
          description: 'Canonical unique identifier for the cluster policy.  def as_dict(self)
            -> dict: Serializes the CreatePolicyResponse into a dictionary suitable
            for use as a JSON request body.'
    CreateResponse:
      type: object
      description: 'CreateResponse(script_id: ''Optional[str]'' = None)'
      properties:
        script_id:
          type: string
          description: 'The global init script ID.  def as_dict(self) -> dict: Serializes
            the CreateResponse into a dictionary suitable for use as a JSON request
            body.'
    Created:
      type: object
      description: 'Created(id: ''Optional[str]'' = None)'
      properties:
        id:
          type: string
          description: ''
    CustomPolicyTag:
      type: object
      description: 'CustomPolicyTag(key: ''str'', value: ''Optional[str]'' = None)'
      properties:
        key:
          type: string
          description: The key of the tag. - Must be unique among all custom tags
            of the same policy - Cannot be budget-policy-name, budget-policy-id
            or "budget-policy-resolution-result" - these tags are preserved.
        value:
          type: string
          description: 'The value of the tag.  def as_dict(self) -> dict: Serializes
            the CustomPolicyTag into a dictionary suitable for use as a JSON request
            body.'
    DataPlaneEventDetails:
      type: object
      description: 'DataPlaneEventDetails(event_type: ''Optional[DataPlaneEventDetailsEventType]''
        = None, executor_failures: ''Optional[int]'' = None, host_id: ''Optional[str]''
        = None, timestamp: ''Optional[int]'' = None)'
      properties:
        event_type:
          type: string
          description: ''
        executor_failures:
          type: string
          description: ''
        host_id:
          type: string
          description: ''
        timestamp:
          type: string
          description: ''
    DataPlaneEventDetailsEventType:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - NODE_BLACKLISTED
      - NODE_EXCLUDED_DECOMMISSIONED
    DataSecurityMode:
      type: string
      description: 'Data security mode decides what data governance model to use when
        accessing data from a cluster.


        The following modes can only be used when `kind = CLASSIC_PREVIEW`. * `DATA_SECURITY_MODE_AUTO`:

        Databricks will choose the most appropriate access mode depending on your
        compute configuration.

        * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`. * `DATA_SECURITY_MODE_DEDICATED`:

        Alias for `SINGLE_USER`.


        The following modes can be used regardless of `kind`. * `NONE`: No security
        isolation for

        multiple users sharing the cluster. Data governance features are not available
        in this mode. *

        `SINGLE_USER`: A secure cluster that can only be exclusively used by a single
        user specified in

        `single_user_name`. Most programming languages, cluster features and data
        governance features

        are available in this mode. * `USER_ISOLATION`: A secure cluster that can
        be shared by multiple

        users. Cluster users are fully isolated so that they cannot see each other''s
        data and

        credentials. Most data governance features are supported in this mode. But
        programming languages

        and cluster features might be limited.


        The following modes are deprecated starting with Databricks Runtime 15.0 and
        will be removed for

        future Databricks Runtime versions:


        * `LEGACY_TABLE_ACL`: This mode is for users migrating from legacy Table ACL
        clusters. *

        `LEGACY_PASSTHROUGH`: This mode is for users migrating from legacy Passthrough
        on high

        concurrency clusters. * `LEGACY_SINGLE_USER`: This mode is for users migrating
        from legacy

        Passthrough on standard clusters. * `LEGACY_SINGLE_USER_STANDARD`: This mode
        provides a way that

        doesnt have UC nor passthrough enabled.'
      enum:
      - DATA_SECURITY_MODE_AUTO
      - DATA_SECURITY_MODE_DEDICATED
      - DATA_SECURITY_MODE_STANDARD
      - LEGACY_PASSTHROUGH
      - LEGACY_SINGLE_USER
      - LEGACY_SINGLE_USER_STANDARD
      - LEGACY_TABLE_ACL
      - NONE
      - SINGLE_USER
      - USER_ISOLATION
    DbfsStorageInfo:
      type: object
      description: A storage location in DBFS
      properties:
        destination:
          type: string
          description: 'dbfs destination, e.g. `dbfs:/my/path`  def as_dict(self)
            -> dict: Serializes the DbfsStorageInfo into a dictionary suitable for
            use as a JSON request body.'
    DeleteClusterResponse:
      type: object
      description: DeleteClusterResponse()
      properties: {}
    DeleteInstancePoolResponse:
      type: object
      description: DeleteInstancePoolResponse()
      properties: {}
    DeletePolicyResponse:
      type: object
      description: DeletePolicyResponse()
      properties: {}
    DeleteResponse:
      type: object
      description: DeleteResponse()
      properties: {}
    DestroyResponse:
      type: object
      description: DestroyResponse()
      properties: {}
    DiskSpec:
      type: object
      description: 'Describes the disks that are launched for each instance in the
        spark cluster. For example, if

        the cluster has 3 instances, each instance is configured to launch 2 disks,
        100 GiB each, then

        Databricks will launch a total of 6 disks, 100 GiB each, for this cluster.'
      properties:
        disk_count:
          type: string
          description: 'The number of disks launched for each instance: - This feature
            is only enabled for supported node types. - Users can choose up to the
            limit of the disks supported by the node type. - For node types with no
            OS disk, at least one disk must be specified; otherwise, cluster creation
            will fail.  If disks are attached, Databricks will configure Spark to
            use only the disks for scratch storage, because heterogenously sized scratch
            devices can lead to inefficient disk utilization. If no disks are attached,
            Databricks will configure Spark to use instance store disks.  Note: If
            disks are specified, then the Spark configuration `spark.local.dir` will
            be overridden.  Disks will be mounted at: - For AWS: `/ebs0`, `/ebs1`,
            and etc. - For Azure: `/remote_volume0`, `/remote_volume1`, and etc.'
        disk_iops:
          type: string
          description: ''
        disk_size:
          type: string
          description: 'The size of each disk (in GiB) launched for each instance.
            Values must fall into the supported range for a particular instance type.  For
            AWS: - General Purpose SSD: 100 - 4096 GiB - Throughput Optimized HDD:
            500 - 4096 GiB  For Azure: - Premium LRS (SSD): 1 - 1023 GiB - Standard
            LRS (HDD): 1- 1023 GiB'
        disk_throughput:
          type: string
          description: ''
        disk_type:
          type: string
          description: 'The type of disks that will be launched with this cluster.  def
            as_dict(self) -> dict: Serializes the DiskSpec into a dictionary suitable
            for use as a JSON request body.'
    DiskType:
      type: object
      description: Describes the disk type.
      properties:
        azure_disk_volume_type:
          type: string
          description: ''
        ebs_volume_type:
          type: string
          description: ''
    DiskTypeAzureDiskVolumeType:
      type: string
      description: 'All Azure Disk types that Databricks supports. See

        https://docs.microsoft.com/en-us/azure/storage/storage-about-disks-and-vhds-linux#types-of-disks'
      enum:
      - PREMIUM_LRS
      - STANDARD_LRS
    DiskTypeEbsVolumeType:
      type: string
      description: 'All EBS volume types that Databricks supports. See https://aws.amazon.com/ebs/details/
        for

        details.'
      enum:
      - GENERAL_PURPOSE_SSD
      - THROUGHPUT_OPTIMIZED_HDD
    DockerBasicAuth:
      type: object
      description: 'DockerBasicAuth(password: ''Optional[str]'' = None, username:
        ''Optional[str]'' = None)'
      properties:
        password:
          type: string
          description: 'Password of the user  username: Optional[str] = None Name
            of the user'
        username:
          type: string
          description: 'Name of the user  def as_dict(self) -> dict: Serializes the
            DockerBasicAuth into a dictionary suitable for use as a JSON request body.'
    DockerImage:
      type: object
      description: 'DockerImage(basic_auth: ''Optional[DockerBasicAuth]'' = None,
        url: ''Optional[str]'' = None)'
      properties:
        basic_auth:
          type: string
          description: 'Basic auth with username and password  url: Optional[str]
            = None URL of the docker image.'
        url:
          type: string
          description: 'URL of the docker image.  def as_dict(self) -> dict: Serializes
            the DockerImage into a dictionary suitable for use as a JSON request body.'
    EbsVolumeType:
      type: string
      description: 'All EBS volume types that Databricks supports. See https://aws.amazon.com/ebs/details/
        for

        details.'
      enum:
      - GENERAL_PURPOSE_SSD
      - THROUGHPUT_OPTIMIZED_HDD
    EditClusterResponse:
      type: object
      description: EditClusterResponse()
      properties: {}
    EditInstancePoolResponse:
      type: object
      description: EditInstancePoolResponse()
      properties: {}
    EditPolicyResponse:
      type: object
      description: EditPolicyResponse()
      properties: {}
    EditResponse:
      type: object
      description: EditResponse()
      properties: {}
    EnforceClusterComplianceResponse:
      type: object
      description: 'EnforceClusterComplianceResponse(changes: ''Optional[List[ClusterSettingsChange]]''
        = None, has_changes: ''Optional[bool]'' = None)'
      properties:
        changes:
          type: string
          description: A list of changes that have been made to the cluster settings
            for the cluster to become compliant with its policy.
        has_changes:
          type: string
          description: Whether any changes have been made to the cluster settings
            for the cluster to become compliant with its policy.
    Environment:
      type: object
      description: 'The environment entity used to preserve serverless environment
        side panel, jobs'' environment for

        non-notebook task, and DLT''s environment for classic and serverless pipelines.
        In this minimal

        environment spec, only pip dependencies are supported.'
      properties:
        client:
          type: string
          description: 'Use `environment_version` instead.  dependencies: Optional[List[str]]
            = None """List of pip dependencies, as supported by the version of pip
            in this environment. Each dependency is a valid pip requirements file
            line per https://pip.pypa.io/en/stable/reference/requirements-file-format/.
            Allowed dependencies include a requirement specifier, an archive URL,
            a local project path (such as WSFS or UC Volumes in Databricks), or a
            VCS project URL.'
        dependencies:
          type: string
          description: List of pip dependencies, as supported by the version of pip
            in this environment. Each dependency is a valid pip requirements file
            line per https://pip.pypa.io/en/stable/reference/requirements-file-format/.
            Allowed dependencies include a requirement specifier, an archive URL,
            a local project path (such as WSFS or UC Volumes in Databricks), or a
            VCS project URL.
        environment_version:
          type: string
          description: Required. Environment version used by the environment. Each
            version comes with a specific Python version and a set of Python packages.
            The version is a string, consisting of an integer.
        java_dependencies:
          type: string
          description: 'List of java dependencies. Each dependency is a string representing
            a java library path. For example: `/Volumes/path/to/test.jar`.'
    EventDetails:
      type: object
      description: 'EventDetails(attributes: ''Optional[ClusterAttributes]'' = None,
        cause: ''Optional[EventDetailsCause]'' = None, cluster_size: ''Optional[ClusterSize]''
        = None, current_num_vcpus: ''Optional[int]'' = None, current_num_workers:
        ''Optional[int]'' = None, did_not_expand_reason: ''Optional[str]'' = None,
        disk_size: ''Optional[int]'' = None, driver_state_message: ''Optional[str]''
        = None, enable_termination_for_node_blocklisted: ''Optional[bool]'' = None,
        free_space: ''Optional[int]'' = None, init_scripts: ''Optional[InitScriptEventDetails]''
        = None, instance_id: ''Optional[str]'' = None, job_run_name: ''Optional[str]''
        = None, previous_attributes: ''Optional[ClusterAttributes]'' = None, previous_cluster_size:
        ''Optional[ClusterSize]'' = None, previous_disk_size: ''Optional[int]'' =
        None, reason: ''Optional[TerminationReason]'' = None, target_num_vcpus: ''Optional[int]''
        = None, target_num_workers: ''Optional[int]'' = None, user: ''Optional[str]''
        = None)'
      properties:
        attributes:
          type: string
          description: '* For created clusters, the attributes of the cluster. * For
            edited clusters, the new attributes of the cluster.'
        cause:
          type: string
          description: 'The cause of a change in target size.  cluster_size: Optional[ClusterSize]
            = None The actual cluster size that was set in the cluster creation or
            edit.'
        cluster_size:
          type: string
          description: 'The actual cluster size that was set in the cluster creation
            or edit.  current_num_vcpus: Optional[int] = None The current number of
            vCPUs in the cluster.'
        current_num_vcpus:
          type: string
          description: 'The current number of vCPUs in the cluster.  current_num_workers:
            Optional[int] = None The current number of nodes in the cluster.'
        current_num_workers:
          type: string
          description: 'The current number of nodes in the cluster.  did_not_expand_reason:
            Optional[str] = None  disk_size: Optional[int] = None Current disk size
            in bytes'
        did_not_expand_reason:
          type: string
          description: ''
        disk_size:
          type: string
          description: 'Current disk size in bytes  driver_state_message: Optional[str]
            = None More details about the change in driver''s state'
        driver_state_message:
          type: string
          description: 'More details about the change in driver''s state  enable_termination_for_node_blocklisted:
            Optional[bool] = None Whether or not a blocklisted node should be terminated.
            For ClusterEventType NODE_BLACKLISTED.'
        enable_termination_for_node_blocklisted:
          type: string
          description: 'Whether or not a blocklisted node should be terminated. For
            ClusterEventType NODE_BLACKLISTED.  free_space: Optional[int] = None  init_scripts:
            Optional[InitScriptEventDetails] = None List of global and cluster init
            scripts associated with this cluster event.'
        free_space:
          type: string
          description: ''
        init_scripts:
          type: string
          description: 'List of global and cluster init scripts associated with this
            cluster event.  instance_id: Optional[str] = None Instance Id where the
            event originated from'
        instance_id:
          type: string
          description: 'Instance Id where the event originated from  job_run_name:
            Optional[str] = None """Unique identifier of the specific job run associated
            with this cluster event * For clusters created for jobs, this will be
            the same as the cluster name'
        job_run_name:
          type: string
          description: Unique identifier of the specific job run associated with this
            cluster event * For clusters created for jobs, this will be the same as
            the cluster name
        previous_attributes:
          type: string
          description: 'The cluster attributes before a cluster was edited.  previous_cluster_size:
            Optional[ClusterSize] = None The size of the cluster before an edit or
            resize.'
        previous_cluster_size:
          type: string
          description: 'The size of the cluster before an edit or resize.  previous_disk_size:
            Optional[int] = None Previous disk size in bytes'
        previous_disk_size:
          type: string
          description: 'Previous disk size in bytes  reason: Optional[TerminationReason]
            = None """A termination reason: * On a TERMINATED event, this is the reason
            of the termination. * On a RESIZE_COMPLETE event, this indicates the reason
            that we failed to acquire some nodes.'
        reason:
          type: string
          description: 'A termination reason: * On a TERMINATED event, this is the
            reason of the termination. * On a RESIZE_COMPLETE event, this indicates
            the reason that we failed to acquire some nodes.'
        target_num_vcpus:
          type: string
          description: 'The targeted number of vCPUs in the cluster.  target_num_workers:
            Optional[int] = None The targeted number of nodes in the cluster.'
        target_num_workers:
          type: string
          description: 'The targeted number of nodes in the cluster.  user: Optional[str]
            = None The user that caused the event to occur. (Empty if it was done
            by the control plane.)'
        user:
          type: string
          description: 'The user that caused the event to occur. (Empty if it was
            done by the control plane.)  def as_dict(self) -> dict: Serializes the
            EventDetails into a dictionary suitable for use as a JSON request body.'
    EventDetailsCause:
      type: string
      description: The cause of a change in target size.
      enum:
      - AUTORECOVERY
      - AUTOSCALE
      - REPLACE_BAD_NODES
      - USER_REQUEST
    EventType:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - ADD_NODES_FAILED
      - AUTOMATIC_CLUSTER_UPDATE
      - AUTOSCALING_BACKOFF
      - AUTOSCALING_FAILED
      - AUTOSCALING_STATS_REPORT
      - CLUSTER_MIGRATED
      - CREATING
      - DBFS_DOWN
      - DID_NOT_EXPAND_DISK
      - DRIVER_HEALTHY
      - DRIVER_NOT_RESPONDING
      - DRIVER_UNAVAILABLE
      - EDITED
      - EXPANDED_DISK
      - FAILED_TO_EXPAND_DISK
      - INIT_SCRIPTS_FINISHED
      - INIT_SCRIPTS_STARTED
      - METASTORE_DOWN
      - NODES_LOST
      - NODE_BLACKLISTED
      - NODE_EXCLUDED_DECOMMISSIONED
      - PINNED
      - RESIZING
      - RESTARTING
      - RUNNING
      - SPARK_EXCEPTION
      - STARTING
      - TERMINATING
      - UNPINNED
      - UPSIZE_COMPLETED
    GcpAttributes:
      type: object
      description: Attributes set during cluster creation which are related to GCP.
      properties:
        availability:
          type: string
          description: This field determines whether the spark executors will be scheduled
            to run on preemptible VMs, on-demand VMs, or preemptible VMs with a fallback
            to on-demand VMs if the former is unavailable.
        boot_disk_size:
          type: string
          description: 'Boot disk size in GB  first_on_demand: Optional[int] = None
            """The first `first_on_demand` nodes of the cluster will be placed on
            on-demand instances. This value should be greater than 0, to make sure
            the cluster driver node is placed on an on-demand instance. If this value
            is greater than or equal to the current cluster size, all nodes will be
            placed on on-demand instances. If this value is less than the current
            cluster size, `first_on_demand` nodes will be placed on on-demand instances
            and the remainder will be placed on `availability` instances. Note that
            this value does not affect cluster size and cannot currently be mutated
            over the lifetime of a cluster.'
        first_on_demand:
          type: string
          description: The first `first_on_demand` nodes of the cluster will be placed
            on on-demand instances. This value should be greater than 0, to make sure
            the cluster driver node is placed on an on-demand instance. If this value
            is greater than or equal to the current cluster size, all nodes will be
            placed on on-demand instances. If this value is less than the current
            cluster size, `first_on_demand` nodes will be placed on on-demand instances
            and the remainder will be placed on `availability` instances. Note that
            this value does not affect cluster size and cannot currently be mutated
            over the lifetime of a cluster.
        google_service_account:
          type: string
          description: If provided, the cluster will impersonate the google service
            account when accessing gcloud services (like GCS). The google service
            account must have previously been added to the Databricks environment
            by an account administrator.
        local_ssd_count:
          type: string
          description: 'If provided, each node (workers and driver) in the cluster
            will have this number of local SSDs attached. Each local SSD is 375GB
            in size. Refer to [GCP documentation] for the supported number of local
            SSDs for each instance type.  [GCP documentation]: https://cloud.google.com/compute/docs/disks/local-ssd#choose_number_local_ssds'
        use_preemptible_executors:
          type: string
          description: 'This field determines whether the spark executors will be
            scheduled to run on preemptible VMs (when set to true) versus standard
            compute engine VMs (when set to false; default). Note: Soon to be deprecated,
            use the ''availability'' field instead.'
        zone_id:
          type: string
          description: 'Identifier for the availability zone in which the cluster
            resides. This can be one of the following: - "HA" => High availability,
            spread nodes across availability zones for a Databricks deployment region
            [default]. - "AUTO" => Databricks picks an availability zone to schedule
            the cluster on. - A GCP availability zone => Pick One of the available
            zones for (machine type + region) from https://cloud.google.com/compute/docs/regions-zones.'
    GcpAvailability:
      type: string
      description: 'This field determines whether the instance pool will contain preemptible
        VMs, on-demand VMs, or

        preemptible VMs with a fallback to on-demand VMs if the former is unavailable.'
      enum:
      - ON_DEMAND_GCP
      - PREEMPTIBLE_GCP
      - PREEMPTIBLE_WITH_FALLBACK_GCP
    GcsStorageInfo:
      type: object
      description: A storage location in Google Cloud Platform's GCS
      properties:
        destination:
          type: string
          description: 'GCS destination/URI, e.g. `gs://my-bucket/some-prefix`  def
            as_dict(self) -> dict: Serializes the GcsStorageInfo into a dictionary
            suitable for use as a JSON request body.'
    GetClusterComplianceResponse:
      type: object
      description: 'GetClusterComplianceResponse(is_compliant: ''Optional[bool]''
        = None, violations: ''Optional[Dict[str, str]]'' = None)'
      properties:
        is_compliant:
          type: string
          description: Whether the cluster is compliant with its policy or not. Clusters
            could be out of compliance if the policy was updated after the cluster
            was last edited.
        violations:
          type: string
          description: An object containing key-value mappings representing the first
            200 policy validation errors. The keys indicate the path where the policy
            validation error is occurring. The values indicate an error message describing
            the policy validation error.
    GetClusterPermissionLevelsResponse:
      type: object
      description: 'GetClusterPermissionLevelsResponse(permission_levels: ''Optional[List[ClusterPermissionsDescription]]''
        = None)'
      properties:
        permission_levels:
          type: string
          description: 'Specific permission levels  def as_dict(self) -> dict: Serializes
            the GetClusterPermissionLevelsResponse into a dictionary suitable for
            use as a JSON request body.'
    GetClusterPolicyPermissionLevelsResponse:
      type: object
      description: 'GetClusterPolicyPermissionLevelsResponse(permission_levels: ''Optional[List[ClusterPolicyPermissionsDescription]]''
        = None)'
      properties:
        permission_levels:
          type: string
          description: 'Specific permission levels  def as_dict(self) -> dict: Serializes
            the GetClusterPolicyPermissionLevelsResponse into a dictionary suitable
            for use as a JSON request body.'
    GetEvents:
      type: object
      description: 'GetEvents(cluster_id: ''str'', end_time: ''Optional[int]'' = None,
        event_types: ''Optional[List[EventType]]'' = None, limit: ''Optional[int]''
        = None, offset: ''Optional[int]'' = None, order: ''Optional[GetEventsOrder]''
        = None, page_size: ''Optional[int]'' = None, page_token: ''Optional[str]''
        = None, start_time: ''Optional[int]'' = None)'
      properties:
        cluster_id:
          type: string
          description: 'The ID of the cluster to retrieve events about.  end_time:
            Optional[int] = None The end time in epoch milliseconds. If empty, returns
            events up to the current time.'
        end_time:
          type: string
          description: 'The end time in epoch milliseconds. If empty, returns events
            up to the current time.  event_types: Optional[List[EventType]] = None
            An optional set of event types to filter on. If empty, all event types
            are returned.'
        event_types:
          type: string
          description: 'An optional set of event types to filter on. If empty, all
            event types are returned.  limit: Optional[int] = None """Deprecated:
            use page_token in combination with page_size instead.  The maximum number
            of events to include in a page of events. Defaults to 50, and maximum
            allowed value is 500.'
        limit:
          type: string
          description: 'Deprecated: use page_token in combination with page_size instead.  The
            maximum number of events to include in a page of events. Defaults to 50,
            and maximum allowed value is 500.'
        offset:
          type: string
          description: 'Deprecated: use page_token in combination with page_size instead.  The
            offset in the result set. Defaults to 0 (no offset). When an offset is
            specified and the results are requested in descending order, the end_time
            field is required.'
        order:
          type: string
          description: 'The order to list events in; either "ASC" or "DESC". Defaults
            to "DESC".  page_size: Optional[int] = None """The maximum number of events
            to include in a page of events. The server may further constrain the maximum
            number of results returned in a single page. If the page_size is empty
            or 0, the server will decide the number of results to be returned. The
            field has to be in the range [0,500]. If the value is outside the range,
            the server enforces 0 or 500.'
        page_size:
          type: string
          description: The maximum number of events to include in a page of events.
            The server may further constrain the maximum number of results returned
            in a single page. If the page_size is empty or 0, the server will decide
            the number of results to be returned. The field has to be in the range
            [0,500]. If the value is outside the range, the server enforces 0 or 500.
        page_token:
          type: string
          description: Use next_page_token or prev_page_token returned from the previous
            request to list the next or previous page of events respectively. If page_token
            is empty, the first page is returned.
        start_time:
          type: string
          description: The start time in epoch milliseconds. If empty, returns events
            starting from the beginning of time.
    GetEventsOrder:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - ASC
      - DESC
    GetEventsResponse:
      type: object
      description: 'GetEventsResponse(events: ''Optional[List[ClusterEvent]]'' = None,
        next_page: ''Optional[GetEvents]'' = None, next_page_token: ''Optional[str]''
        = None, prev_page_token: ''Optional[str]'' = None, total_count: ''Optional[int]''
        = None)'
      properties:
        events:
          type: string
          description: ''
        next_page:
          type: string
          description: 'Deprecated: use next_page_token or prev_page_token instead.  The
            parameters required to retrieve the next page of events. Omitted if there
            are no more events to read.'
        next_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            next page of results. If the value is , it means no further results for
            the request.
        prev_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            previous page of results. If the value is "", it means no further results
            for the request.
        total_count:
          type: string
          description: 'Deprecated: Returns 0 when request uses page_token. Will start
            returning zero when request uses offset/limit soon.  The total number
            of events filtered by the start_time, end_time, and event_types.'
    GetInstancePool:
      type: object
      description: 'GetInstancePool(instance_pool_id: ''str'', aws_attributes: ''Optional[InstancePoolAwsAttributes]''
        = None, azure_attributes: ''Optional[InstancePoolAzureAttributes]'' = None,
        custom_tags: ''Optional[Dict[str, str]]'' = None, default_tags: ''Optional[Dict[str,
        str]]'' = None, disk_spec: ''Optional[DiskSpec]'' = None, enable_elastic_disk:
        ''Optional[bool]'' = None, gcp_attributes: ''Optional[InstancePoolGcpAttributes]''
        = None, idle_instance_autotermination_minutes: ''Optional[int]'' = None, instance_pool_name:
        ''Optional[str]'' = None, max_capacity: ''Optional[int]'' = None, min_idle_instances:
        ''Optional[int]'' = None, node_type_id: ''Optional[str]'' = None, preloaded_docker_images:
        ''Optional[List[DockerImage]]'' = None, preloaded_spark_versions: ''Optional[List[str]]''
        = None, remote_disk_throughput: ''Optional[int]'' = None, state: ''Optional[InstancePoolState]''
        = None, stats: ''Optional[InstancePoolStats]'' = None, status: ''Optional[InstancePoolStatus]''
        = None, total_initial_remote_disk_size: ''Optional[int]'' = None)'
      properties:
        instance_pool_id:
          type: string
          description: 'Canonical unique identifier for the pool.  aws_attributes:
            Optional[InstancePoolAwsAttributes] = None """Attributes related to instance
            pools running on Amazon Web Services. If not specified at pool creation,
            a set of default values will be used.'
        aws_attributes:
          type: string
          description: Attributes related to instance pools running on Amazon Web
            Services. If not specified at pool creation, a set of default values will
            be used.
        azure_attributes:
          type: string
          description: Attributes related to instance pools running on Azure. If not
            specified at pool creation, a set of default values will be used.
        custom_tags:
          type: string
          description: 'Additional tags for pool resources. Databricks will tag all
            pool resources (e.g., AWS instances and EBS volumes) with these tags in
            addition to `default_tags`. Notes:  - Currently, Databricks allows at
            most 45 custom tags'
        default_tags:
          type: string
          description: 'Tags that are added by Databricks regardless of any ``custom_tags``,
            including:  - Vendor: Databricks  - InstancePoolCreator: <user_id_of_creator>  -
            InstancePoolName: <name_of_pool>  - InstancePoolId: <id_of_pool>'
        disk_spec:
          type: string
          description: 'Defines the specification of the disks that will be attached
            to all spark containers.  enable_elastic_disk: Optional[bool] = None """Autoscaling
            Local Storage: when enabled, this instances in this pool will dynamically
            acquire additional disk space when its Spark workers are running low on
            disk space. In AWS, this feature requires specific AWS permissions to
            function correctly - refer to the User Guide for more details.'
        enable_elastic_disk:
          type: string
          description: 'Autoscaling Local Storage: when enabled, this instances in
            this pool will dynamically acquire additional disk space when its Spark
            workers are running low on disk space. In AWS, this feature requires specific
            AWS permissions to function correctly - refer to the User Guide for more
            details.'
        gcp_attributes:
          type: string
          description: Attributes related to instance pools running on Google Cloud
            Platform. If not specified at pool creation, a set of default values will
            be used.
        idle_instance_autotermination_minutes:
          type: string
          description: Automatically terminates the extra instances in the pool cache
            after they are inactive for this time in minutes if min_idle_instances
            requirement is already met. If not set, the extra pool instances will
            be automatically terminated after a default timeout. If specified, the
            threshold must be between 0 and 10000 minutes. Users can also set this
            value to 0 to instantly remove idle instances from the cache if min cache
            size could still hold.
        instance_pool_name:
          type: string
          description: Pool name requested by the user. Pool name must be unique.
            Length must be between 1 and 100 characters.
        max_capacity:
          type: string
          description: Maximum number of outstanding instances to keep in the pool,
            including both instances used by clusters and idle instances. Clusters
            that require further instance provisioning will fail during upsize requests.
        min_idle_instances:
          type: string
          description: 'Minimum number of idle instances to keep in the instance pool  node_type_id:
            Optional[str] = None """This field encodes, through a single value, the
            resources available to each of the Spark nodes in this cluster. For example,
            the Spark nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved by
            using the :method:clusters/listNodeTypes API call.'
        node_type_id:
          type: string
          description: This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark nodes
            can be provisioned and optimized for memory or compute intensive workloads.
            A list of available node types can be retrieved by using the :method:clusters/listNodeTypes
            API call.
        preloaded_docker_images:
          type: string
          description: 'Custom Docker Image BYOC  preloaded_spark_versions: Optional[List[str]]
            = None """A list containing at most one preloaded Spark image version
            for the pool. Pool-backed clusters started with the preloaded Spark version
            will start faster. A list of available Spark versions can be retrieved
            by using the :method:clusters/sparkVersions API call.'
        preloaded_spark_versions:
          type: string
          description: A list containing at most one preloaded Spark image version
            for the pool. Pool-backed clusters started with the preloaded Spark version
            will start faster. A list of available Spark versions can be retrieved
            by using the :method:clusters/sparkVersions API call.
        remote_disk_throughput:
          type: string
          description: If set, what the configurable throughput (in Mb/s) for the
            remote disk is. Currently only supported for GCP HYPERDISK_BALANCED types.
        state:
          type: string
          description: 'Current state of the instance pool.  stats: Optional[InstancePoolStats]
            = None Usage statistics about the instance pool.'
        stats:
          type: string
          description: 'Usage statistics about the instance pool.  status: Optional[InstancePoolStatus]
            = None Status of failed pending instances in the pool.'
        status:
          type: string
          description: 'Status of failed pending instances in the pool.  total_initial_remote_disk_size:
            Optional[int] = None """If set, what the total initial volume size (in
            GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            types.'
        total_initial_remote_disk_size:
          type: string
          description: If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED types.
    GetInstancePoolPermissionLevelsResponse:
      type: object
      description: 'GetInstancePoolPermissionLevelsResponse(permission_levels: ''Optional[List[InstancePoolPermissionsDescription]]''
        = None)'
      properties:
        permission_levels:
          type: string
          description: 'Specific permission levels  def as_dict(self) -> dict: Serializes
            the GetInstancePoolPermissionLevelsResponse into a dictionary suitable
            for use as a JSON request body.'
    GetSparkVersionsResponse:
      type: object
      description: 'GetSparkVersionsResponse(versions: ''Optional[List[SparkVersion]]''
        = None)'
      properties:
        versions:
          type: string
          description: 'All the available Spark versions.  def as_dict(self) -> dict:
            Serializes the GetSparkVersionsResponse into a dictionary suitable for
            use as a JSON request body.'
    GlobalInitScriptDetails:
      type: object
      description: 'GlobalInitScriptDetails(created_at: ''Optional[int]'' = None,
        created_by: ''Optional[str]'' = None, enabled: ''Optional[bool]'' = None,
        name: ''Optional[str]'' = None, position: ''Optional[int]'' = None, script_id:
        ''Optional[str]'' = None, updated_at: ''Optional[int]'' = None, updated_by:
        ''Optional[str]'' = None)'
      properties:
        created_at:
          type: string
          description: 'Time when the script was created, represented as a Unix timestamp
            in milliseconds.  created_by: Optional[str] = None The username of the
            user who created the script.'
        created_by:
          type: string
          description: 'The username of the user who created the script.  enabled:
            Optional[bool] = None Specifies whether the script is enabled. The script
            runs only if enabled.'
        enabled:
          type: string
          description: 'Specifies whether the script is enabled. The script runs only
            if enabled.  name: Optional[str] = None The name of the script'
        name:
          type: string
          description: 'The name of the script  position: Optional[int] = None """The
            position of a script, where 0 represents the first script to run, 1 is
            the second script to run, in ascending order.'
        position:
          type: string
          description: The position of a script, where 0 represents the first script
            to run, 1 is the second script to run, in ascending order.
        script_id:
          type: string
          description: 'The global init script ID.  updated_at: Optional[int] = None
            Time when the script was updated, represented as a Unix timestamp in milliseconds.'
        updated_at:
          type: string
          description: 'Time when the script was updated, represented as a Unix timestamp
            in milliseconds.  updated_by: Optional[str] = None The username of the
            user who last updated the script'
        updated_by:
          type: string
          description: 'The username of the user who last updated the script  def
            as_dict(self) -> dict: Serializes the GlobalInitScriptDetails into a dictionary
            suitable for use as a JSON request body.'
    GlobalInitScriptDetailsWithContent:
      type: object
      description: 'GlobalInitScriptDetailsWithContent(created_at: ''Optional[int]''
        = None, created_by: ''Optional[str]'' = None, enabled: ''Optional[bool]''
        = None, name: ''Optional[str]'' = None, position: ''Optional[int]'' = None,
        script: ''Optional[str]'' = None, script_id: ''Optional[str]'' = None, updated_at:
        ''Optional[int]'' = None, updated_by: ''Optional[str]'' = None)'
      properties:
        created_at:
          type: string
          description: 'Time when the script was created, represented as a Unix timestamp
            in milliseconds.  created_by: Optional[str] = None The username of the
            user who created the script.'
        created_by:
          type: string
          description: 'The username of the user who created the script.  enabled:
            Optional[bool] = None Specifies whether the script is enabled. The script
            runs only if enabled.'
        enabled:
          type: string
          description: 'Specifies whether the script is enabled. The script runs only
            if enabled.  name: Optional[str] = None The name of the script'
        name:
          type: string
          description: 'The name of the script  position: Optional[int] = None """The
            position of a script, where 0 represents the first script to run, 1 is
            the second script to run, in ascending order.'
        position:
          type: string
          description: The position of a script, where 0 represents the first script
            to run, 1 is the second script to run, in ascending order.
        script:
          type: string
          description: 'The Base64-encoded content of the script.  script_id: Optional[str]
            = None The global init script ID.'
        script_id:
          type: string
          description: 'The global init script ID.  updated_at: Optional[int] = None
            Time when the script was updated, represented as a Unix timestamp in milliseconds.'
        updated_at:
          type: string
          description: 'Time when the script was updated, represented as a Unix timestamp
            in milliseconds.  updated_by: Optional[str] = None The username of the
            user who last updated the script'
        updated_by:
          type: string
          description: 'The username of the user who last updated the script  def
            as_dict(self) -> dict: Serializes the GlobalInitScriptDetailsWithContent
            into a dictionary suitable for use as a JSON request body.'
    InitScriptEventDetails:
      type: object
      description: 'InitScriptEventDetails(cluster: ''Optional[List[InitScriptInfoAndExecutionDetails]]''
        = None, global_: ''Optional[List[InitScriptInfoAndExecutionDetails]]'' = None,
        reported_for_node: ''Optional[str]'' = None)'
      properties:
        cluster:
          type: string
          description: 'The cluster scoped init scripts associated with this cluster
            event.  global_: Optional[List[InitScriptInfoAndExecutionDetails]] = None
            The global init scripts associated with this cluster event.'
        global_:
          type: string
          description: 'The global init scripts associated with this cluster event.  reported_for_node:
            Optional[str] = None """The private ip of the node we are reporting init
            script execution details for (we will select the execution details from
            only one node rather than reporting the execution details from every node
            to keep these event details small)  This should only be defined for the
            INIT_SCRIPTS_FINISHED event'
        reported_for_node:
          type: string
          description: The private ip of the node we are reporting init script execution
            details for (we will select the execution details from only one node rather
            than reporting the execution details from every node to keep these event
            details small)  This should only be defined for the INIT_SCRIPTS_FINISHED
            event
    InitScriptExecutionDetailsInitScriptExecutionStatus:
      type: string
      description: Result of attempted script execution
      enum:
      - FAILED_EXECUTION
      - FAILED_FETCH
      - FUSE_MOUNT_FAILED
      - NOT_EXECUTED
      - SKIPPED
      - SUCCEEDED
      - UNKNOWN
    InitScriptInfo:
      type: object
      description: 'Config for an individual init script Next ID: 11'
      properties:
        abfss:
          type: string
          description: destination needs to be provided, e.g. `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`
        dbfs:
          type: string
          description: 'destination needs to be provided. e.g. `{ "dbfs": { "destination"
            : "dbfs:/home/cluster_log" } }`'
        file:
          type: string
          description: 'destination needs to be provided, e.g. `{ "file": { "destination":
            "file:/my/local/file.sh" } }`  gcs: Optional[GcsStorageInfo] = None destination
            needs to be provided, e.g. `{ "gcs": { "destination": "gs://my-bucket/file.sh"
            } }`'
        gcs:
          type: string
          description: 'destination needs to be provided, e.g. `{ "gcs": { "destination":
            "gs://my-bucket/file.sh" } }`  s3: Optional[S3StorageInfo] = None """destination
            and either the region or endpoint need to be provided. e.g. `{ \"s3\":
            { \"destination\": \"s3://cluster_log_bucket/prefix\", \"region\": \"us-west-2\"
            } }` Cluster iam role is used to access s3, please make sure the cluster
            iam role in `instance_profile_arn` has permission to write data to the
            s3 destination.'
        s3:
          type: string
          description: 'destination and either the region or endpoint need to be provided.
            e.g. `{ \"s3\": { \"destination\": \"s3://cluster_log_bucket/prefix\",
            \"region\": \"us-west-2\" } }` Cluster iam role is used to access s3,
            please make sure the cluster iam role in `instance_profile_arn` has permission
            to write data to the s3 destination.'
        volumes:
          type: string
          description: 'destination needs to be provided. e.g. `{ \"volumes\" : {
            \"destination\" : \"/Volumes/my-init.sh\" } }`'
        workspace:
          type: string
          description: 'destination needs to be provided, e.g. `{ "workspace": { "destination":
            /cluster-init-scripts/setup-datadog.sh" } }`'
    InitScriptInfoAndExecutionDetails:
      type: object
      description: 'InitScriptInfoAndExecutionDetails(abfss: ''Optional[Adlsgen2Info]''
        = None, dbfs: ''Optional[DbfsStorageInfo]'' = None, error_message: ''Optional[str]''
        = None, execution_duration_seconds: ''Optional[int]'' = None, file: ''Optional[LocalFileInfo]''
        = None, gcs: ''Optional[GcsStorageInfo]'' = None, s3: ''Optional[S3StorageInfo]''
        = None, status: ''Optional[InitScriptExecutionDetailsInitScriptExecutionStatus]''
        = None, volumes: ''Optional[VolumesStorageInfo]'' = None, workspace: ''Optional[WorkspaceStorageInfo]''
        = None)'
      properties:
        abfss:
          type: string
          description: destination needs to be provided, e.g. `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`
        dbfs:
          type: string
          description: 'destination needs to be provided. e.g. `{ "dbfs": { "destination"
            : "dbfs:/home/cluster_log" } }`'
        error_message:
          type: string
          description: Additional details regarding errors (such as a file not found
            message if the status is FAILED_FETCH). This field should only be used
            to provide *additional* information to the status field, not duplicate
            it.
        execution_duration_seconds:
          type: string
          description: 'The number duration of the script execution in seconds  file:
            Optional[LocalFileInfo] = None destination needs to be provided, e.g.
            `{ "file": { "destination": "file:/my/local/file.sh" } }`'
        file:
          type: string
          description: 'destination needs to be provided, e.g. `{ "file": { "destination":
            "file:/my/local/file.sh" } }`  gcs: Optional[GcsStorageInfo] = None destination
            needs to be provided, e.g. `{ "gcs": { "destination": "gs://my-bucket/file.sh"
            } }`'
        gcs:
          type: string
          description: 'destination needs to be provided, e.g. `{ "gcs": { "destination":
            "gs://my-bucket/file.sh" } }`  s3: Optional[S3StorageInfo] = None """destination
            and either the region or endpoint need to be provided. e.g. `{ \"s3\":
            { \"destination\": \"s3://cluster_log_bucket/prefix\", \"region\": \"us-west-2\"
            } }` Cluster iam role is used to access s3, please make sure the cluster
            iam role in `instance_profile_arn` has permission to write data to the
            s3 destination.'
        s3:
          type: string
          description: 'destination and either the region or endpoint need to be provided.
            e.g. `{ \"s3\": { \"destination\": \"s3://cluster_log_bucket/prefix\",
            \"region\": \"us-west-2\" } }` Cluster iam role is used to access s3,
            please make sure the cluster iam role in `instance_profile_arn` has permission
            to write data to the s3 destination.'
        status:
          type: string
          description: 'The current status of the script  volumes: Optional[VolumesStorageInfo]
            = None """destination needs to be provided. e.g. `{ \"volumes\" : { \"destination\"
            : \"/Volumes/my-init.sh\" } }`'
        volumes:
          type: string
          description: 'destination needs to be provided. e.g. `{ \"volumes\" : {
            \"destination\" : \"/Volumes/my-init.sh\" } }`'
        workspace:
          type: string
          description: 'destination needs to be provided, e.g. `{ "workspace": { "destination":
            /cluster-init-scripts/setup-datadog.sh" } }`'
    InstallLibrariesResponse:
      type: object
      description: InstallLibrariesResponse()
      properties: {}
    InstancePoolAccessControlRequest:
      type: object
      description: 'InstancePoolAccessControlRequest(group_name: ''Optional[str]''
        = None, permission_level: ''Optional[InstancePoolPermissionLevel]'' = None,
        service_principal_name: ''Optional[str]'' = None, user_name: ''Optional[str]''
        = None)'
      properties:
        group_name:
          type: string
          description: 'name of the group  permission_level: Optional[InstancePoolPermissionLevel]
            = None  service_principal_name: Optional[str] = None application ID of
            a service principal'
        permission_level:
          type: string
          description: ''
        service_principal_name:
          type: string
          description: 'application ID of a service principal  user_name: Optional[str]
            = None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            InstancePoolAccessControlRequest into a dictionary suitable for use as
            a JSON request body.'
    InstancePoolAccessControlResponse:
      type: object
      description: 'InstancePoolAccessControlResponse(all_permissions: ''Optional[List[InstancePoolPermission]]''
        = None, display_name: ''Optional[str]'' = None, group_name: ''Optional[str]''
        = None, service_principal_name: ''Optional[str]'' = None, user_name: ''Optional[str]''
        = None)'
      properties:
        all_permissions:
          type: string
          description: 'All permissions.  display_name: Optional[str] = None Display
            name of the user or service principal.'
        display_name:
          type: string
          description: 'Display name of the user or service principal.  group_name:
            Optional[str] = None name of the group'
        group_name:
          type: string
          description: 'name of the group  service_principal_name: Optional[str] =
            None Name of the service principal.'
        service_principal_name:
          type: string
          description: 'Name of the service principal.  user_name: Optional[str] =
            None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            InstancePoolAccessControlResponse into a dictionary suitable for use as
            a JSON request body.'
    InstancePoolAndStats:
      type: object
      description: 'InstancePoolAndStats(aws_attributes: ''Optional[InstancePoolAwsAttributes]''
        = None, azure_attributes: ''Optional[InstancePoolAzureAttributes]'' = None,
        custom_tags: ''Optional[Dict[str, str]]'' = None, default_tags: ''Optional[Dict[str,
        str]]'' = None, disk_spec: ''Optional[DiskSpec]'' = None, enable_elastic_disk:
        ''Optional[bool]'' = None, gcp_attributes: ''Optional[InstancePoolGcpAttributes]''
        = None, idle_instance_autotermination_minutes: ''Optional[int]'' = None, instance_pool_id:
        ''Optional[str]'' = None, instance_pool_name: ''Optional[str]'' = None, max_capacity:
        ''Optional[int]'' = None, min_idle_instances: ''Optional[int]'' = None, node_type_id:
        ''Optional[str]'' = None, preloaded_docker_images: ''Optional[List[DockerImage]]''
        = None, preloaded_spark_versions: ''Optional[List[str]]'' = None, remote_disk_throughput:
        ''Optional[int]'' = None, state: ''Optional[InstancePoolState]'' = None, stats:
        ''Optional[InstancePoolStats]'' = None, status: ''Optional[InstancePoolStatus]''
        = None, total_initial_remote_disk_size: ''Optional[int]'' = None)'
      properties:
        aws_attributes:
          type: string
          description: Attributes related to instance pools running on Amazon Web
            Services. If not specified at pool creation, a set of default values will
            be used.
        azure_attributes:
          type: string
          description: Attributes related to instance pools running on Azure. If not
            specified at pool creation, a set of default values will be used.
        custom_tags:
          type: string
          description: 'Additional tags for pool resources. Databricks will tag all
            pool resources (e.g., AWS instances and EBS volumes) with these tags in
            addition to `default_tags`. Notes:  - Currently, Databricks allows at
            most 45 custom tags'
        default_tags:
          type: string
          description: 'Tags that are added by Databricks regardless of any ``custom_tags``,
            including:  - Vendor: Databricks  - InstancePoolCreator: <user_id_of_creator>  -
            InstancePoolName: <name_of_pool>  - InstancePoolId: <id_of_pool>'
        disk_spec:
          type: string
          description: 'Defines the specification of the disks that will be attached
            to all spark containers.  enable_elastic_disk: Optional[bool] = None """Autoscaling
            Local Storage: when enabled, this instances in this pool will dynamically
            acquire additional disk space when its Spark workers are running low on
            disk space. In AWS, this feature requires specific AWS permissions to
            function correctly - refer to the User Guide for more details.'
        enable_elastic_disk:
          type: string
          description: 'Autoscaling Local Storage: when enabled, this instances in
            this pool will dynamically acquire additional disk space when its Spark
            workers are running low on disk space. In AWS, this feature requires specific
            AWS permissions to function correctly - refer to the User Guide for more
            details.'
        gcp_attributes:
          type: string
          description: Attributes related to instance pools running on Google Cloud
            Platform. If not specified at pool creation, a set of default values will
            be used.
        idle_instance_autotermination_minutes:
          type: string
          description: Automatically terminates the extra instances in the pool cache
            after they are inactive for this time in minutes if min_idle_instances
            requirement is already met. If not set, the extra pool instances will
            be automatically terminated after a default timeout. If specified, the
            threshold must be between 0 and 10000 minutes. Users can also set this
            value to 0 to instantly remove idle instances from the cache if min cache
            size could still hold.
        instance_pool_id:
          type: string
          description: 'Canonical unique identifier for the pool.  instance_pool_name:
            Optional[str] = None """Pool name requested by the user. Pool name must
            be unique. Length must be between 1 and 100 characters.'
        instance_pool_name:
          type: string
          description: Pool name requested by the user. Pool name must be unique.
            Length must be between 1 and 100 characters.
        max_capacity:
          type: string
          description: Maximum number of outstanding instances to keep in the pool,
            including both instances used by clusters and idle instances. Clusters
            that require further instance provisioning will fail during upsize requests.
        min_idle_instances:
          type: string
          description: 'Minimum number of idle instances to keep in the instance pool  node_type_id:
            Optional[str] = None """This field encodes, through a single value, the
            resources available to each of the Spark nodes in this cluster. For example,
            the Spark nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved by
            using the :method:clusters/listNodeTypes API call.'
        node_type_id:
          type: string
          description: This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark nodes
            can be provisioned and optimized for memory or compute intensive workloads.
            A list of available node types can be retrieved by using the :method:clusters/listNodeTypes
            API call.
        preloaded_docker_images:
          type: string
          description: 'Custom Docker Image BYOC  preloaded_spark_versions: Optional[List[str]]
            = None """A list containing at most one preloaded Spark image version
            for the pool. Pool-backed clusters started with the preloaded Spark version
            will start faster. A list of available Spark versions can be retrieved
            by using the :method:clusters/sparkVersions API call.'
        preloaded_spark_versions:
          type: string
          description: A list containing at most one preloaded Spark image version
            for the pool. Pool-backed clusters started with the preloaded Spark version
            will start faster. A list of available Spark versions can be retrieved
            by using the :method:clusters/sparkVersions API call.
        remote_disk_throughput:
          type: string
          description: If set, what the configurable throughput (in Mb/s) for the
            remote disk is. Currently only supported for GCP HYPERDISK_BALANCED types.
        state:
          type: string
          description: 'Current state of the instance pool.  stats: Optional[InstancePoolStats]
            = None Usage statistics about the instance pool.'
        stats:
          type: string
          description: 'Usage statistics about the instance pool.  status: Optional[InstancePoolStatus]
            = None Status of failed pending instances in the pool.'
        status:
          type: string
          description: 'Status of failed pending instances in the pool.  total_initial_remote_disk_size:
            Optional[int] = None """If set, what the total initial volume size (in
            GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            types.'
        total_initial_remote_disk_size:
          type: string
          description: If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED types.
    InstancePoolAwsAttributes:
      type: object
      description: Attributes set during instance pool creation which are related
        to Amazon Web Services.
      properties:
        availability:
          type: string
          description: 'Availability type used for the spot nodes.  spot_bid_price_percent:
            Optional[int] = None """Calculates the bid price for AWS spot instances,
            as a percentage of the corresponding instance type''s on-demand price.
            For example, if this field is set to 50, and the cluster needs a new `r3.xlarge`
            spot instance, then the bid price is half of the price of on-demand `r3.xlarge`
            instances. Similarly, if this field is set to 200, the bid price is twice
            the price of on-demand `r3.xlarge` instances. If not specified, the default
            value is 100. When spot instances are requested for this cluster, only
            spot instances whose bid price percentage matches this field will be considered.
            Note that, for safety, we enforce this field to be no more than 10000.'
        spot_bid_price_percent:
          type: string
          description: Calculates the bid price for AWS spot instances, as a percentage
            of the corresponding instance type's on-demand price. For example, if
            this field is set to 50, and the cluster needs a new `r3.xlarge` spot
            instance, then the bid price is half of the price of on-demand `r3.xlarge`
            instances. Similarly, if this field is set to 200, the bid price is twice
            the price of on-demand `r3.xlarge` instances. If not specified, the default
            value is 100. When spot instances are requested for this cluster, only
            spot instances whose bid price percentage matches this field will be considered.
            Note that, for safety, we enforce this field to be no more than 10000.
        zone_id:
          type: string
          description: Identifier for the availability zone/datacenter in which the
            cluster resides. This string will be of a form like "us-west-2a". The
            provided availability zone must be in the same region as the Databricks
            deployment. For example, "us-west-2a" is not a valid zone id if the Databricks
            deployment resides in the "us-east-1" region. This is an optional field
            at cluster creation, and if not specified, a default zone will be used.
            The list of available zones as well as the default value can be found
            by using the `List Zones` method.
    InstancePoolAwsAttributesAvailability:
      type: string
      description: The set of AWS availability types supported when setting up nodes
        for a cluster.
      enum:
      - ON_DEMAND
      - SPOT
    InstancePoolAzureAttributes:
      type: object
      description: Attributes set during instance pool creation which are related
        to Azure.
      properties:
        availability:
          type: string
          description: 'Availability type used for the spot nodes.  spot_bid_max_price:
            Optional[float] = None """With variable pricing, you have option to set
            a max price, in US dollars (USD) For example, the value 2 would be a max
            price of $2.00 USD per hour. If you set the max price to be -1, the VM
            won''t be evicted based on price. The price for the VM will be the current
            price for spot or the price for a standard VM, which ever is less, as
            long as there is capacity and quota available.'
        spot_bid_max_price:
          type: string
          description: With variable pricing, you have option to set a max price,
            in US dollars (USD) For example, the value 2 would be a max price of $2.00
            USD per hour. If you set the max price to be -1, the VM won't be evicted
            based on price. The price for the VM will be the current price for spot
            or the price for a standard VM, which ever is less, as long as there is
            capacity and quota available.
    InstancePoolAzureAttributesAvailability:
      type: string
      description: The set of Azure availability types supported when setting up nodes
        for a cluster.
      enum:
      - ON_DEMAND_AZURE
      - SPOT_AZURE
    InstancePoolGcpAttributes:
      type: object
      description: Attributes set during instance pool creation which are related
        to GCP.
      properties:
        gcp_availability:
          type: string
          description: ''
        local_ssd_count:
          type: string
          description: 'If provided, each node in the instance pool will have this
            number of local SSDs attached. Each local SSD is 375GB in size. Refer
            to [GCP documentation] for the supported number of local SSDs for each
            instance type.  [GCP documentation]: https://cloud.google.com/compute/docs/disks/local-ssd#choose_number_local_ssds'
        zone_id:
          type: string
          description: 'Identifier for the availability zone/datacenter in which the
            cluster resides. This string will be of a form like "us-west1-a". The
            provided availability zone must be in the same region as the Databricks
            workspace. For example, "us-west1-a" is not a valid zone id if the Databricks
            workspace resides in the "us-east1" region. This is an optional field
            at instance pool creation, and if not specified, a default zone will be
            used.  This field can be one of the following: - "HA" => High availability,
            spread nodes across availability zones for a Databricks deployment region
            - A GCP availability zone => Pick One of the available zones for (machine
            type + region) from https://cloud.google.com/compute/docs/regions-zones
            (e.g. "us-west1-a").  If empty, Databricks picks an availability zone
            to schedule the cluster on.'
    InstancePoolPermission:
      type: object
      description: 'InstancePoolPermission(inherited: ''Optional[bool]'' = None, inherited_from_object:
        ''Optional[List[str]]'' = None, permission_level: ''Optional[InstancePoolPermissionLevel]''
        = None)'
      properties:
        inherited:
          type: string
          description: ''
        inherited_from_object:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    InstancePoolPermissionLevel:
      type: string
      description: Permission level
      enum:
      - CAN_ATTACH_TO
      - CAN_MANAGE
    InstancePoolPermissions:
      type: object
      description: 'InstancePoolPermissions(access_control_list: ''Optional[List[InstancePoolAccessControlResponse]]''
        = None, object_id: ''Optional[str]'' = None, object_type: ''Optional[str]''
        = None)'
      properties:
        access_control_list:
          type: string
          description: ''
        object_id:
          type: string
          description: ''
        object_type:
          type: string
          description: ''
    InstancePoolPermissionsDescription:
      type: object
      description: 'InstancePoolPermissionsDescription(description: ''Optional[str]''
        = None, permission_level: ''Optional[InstancePoolPermissionLevel]'' = None)'
      properties:
        description:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    InstancePoolState:
      type: string
      description: 'The state of a Cluster. The current allowable state transitions
        are as follows:


        - ``ACTIVE`` -> ``STOPPED`` - ``ACTIVE`` -> ``DELETED`` - ``STOPPED`` -> ``ACTIVE``
        -

        ``STOPPED`` -> ``DELETED``'
      enum:
      - ACTIVE
      - DELETED
      - STOPPED
    InstancePoolStats:
      type: object
      description: 'InstancePoolStats(idle_count: ''Optional[int]'' = None, pending_idle_count:
        ''Optional[int]'' = None, pending_used_count: ''Optional[int]'' = None, used_count:
        ''Optional[int]'' = None)'
      properties:
        idle_count:
          type: string
          description: 'Number of active instances in the pool that are NOT part of
            a cluster.  pending_idle_count: Optional[int] = None Number of pending
            instances in the pool that are NOT part of a cluster.'
        pending_idle_count:
          type: string
          description: 'Number of pending instances in the pool that are NOT part
            of a cluster.  pending_used_count: Optional[int] = None Number of pending
            instances in the pool that are part of a cluster.'
        pending_used_count:
          type: string
          description: 'Number of pending instances in the pool that are part of a
            cluster.  used_count: Optional[int] = None Number of active instances
            in the pool that are part of a cluster.'
        used_count:
          type: string
          description: 'Number of pending instances in the pool that are part of a
            cluster.  used_count: Optional[int] = None Number of active instances
            in the pool that are part of a cluster.'
    InstancePoolStatus:
      type: object
      description: 'InstancePoolStatus(pending_instance_errors: ''Optional[List[PendingInstanceError]]''
        = None)'
      properties:
        pending_instance_errors:
          type: string
          description: List of error messages for the failed pending instances. The
            pending_instance_errors follows FIFO with maximum length of the min_idle
            of the pool. The pending_instance_errors is emptied once the number of
            exiting available instances reaches the min_idle of the pool.
    InstanceProfile:
      type: object
      description: 'InstanceProfile(instance_profile_arn: ''str'', iam_role_arn: ''Optional[str]''
        = None, is_meta_instance_profile: ''Optional[bool]'' = None)'
      properties:
        instance_profile_arn:
          type: string
          description: 'The AWS ARN of the instance profile to register with Databricks.
            This field is required.  iam_role_arn: Optional[str] = None """The AWS
            IAM role ARN of the role associated with the instance profile. This field
            is required if your role name and instance profile name do not match and
            you want to use the instance profile with [Databricks SQL Serverless].  Otherwise,
            this field is optional.  [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html'
        iam_role_arn:
          type: string
          description: 'The AWS IAM role ARN of the role associated with the instance
            profile. This field is required if your role name and instance profile
            name do not match and you want to use the instance profile with [Databricks
            SQL Serverless].  Otherwise, this field is optional.  [Databricks SQL
            Serverless]: https://docs.databricks.com/sql/admin/serverless.html'
        is_meta_instance_profile:
          type: string
          description: Boolean flag indicating whether the instance profile should
            only be used in credential passthrough scenarios. If true, it means the
            instance profile contains an meta IAM role which could assume a wide range
            of roles. Therefore it should always be used with authorization. This
            field is optional, the default value is `false`.
    Kind:
      type: string
      description: 'The kind of compute described by this compute specification.


        Depending on `kind`, different validations and default values will be applied.


        Clusters with `kind = CLASSIC_PREVIEW` support the following fields, whereas
        clusters with no

        specified `kind` do not. * [is_single_node](/api/workspace/clusters/create#is_single_node)
        *

        [use_ml_runtime](/api/workspace/clusters/create#use_ml_runtime) *

        [data_security_mode](/api/workspace/clusters/create#data_security_mode) set
        to

        `DATA_SECURITY_MODE_AUTO`, `DATA_SECURITY_MODE_DEDICATED`, or `DATA_SECURITY_MODE_STANDARD`


        By using the [simple form], your clusters are automatically using `kind =
        CLASSIC_PREVIEW`.


        [simple form]: https://docs.databricks.com/compute/simple-form.html'
      enum:
      - CLASSIC_PREVIEW
    Language:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - python
      - r
      - scala
      - sql
    Library:
      type: object
      description: 'Library(cran: ''Optional[RCranLibrary]'' = None, egg: ''Optional[str]''
        = None, jar: ''Optional[str]'' = None, maven: ''Optional[MavenLibrary]'' =
        None, pypi: ''Optional[PythonPyPiLibrary]'' = None, requirements: ''Optional[str]''
        = None, whl: ''Optional[str]'' = None)'
      properties:
        cran:
          type: string
          description: 'Specification of a CRAN library to be installed as part of
            the library  egg: Optional[str] = None """Deprecated. URI of the egg library
            to install. Installing Python egg files is deprecated and is not supported
            in Databricks Runtime 14.0 and above.'
        egg:
          type: string
          description: Deprecated. URI of the egg library to install. Installing Python
            egg files is deprecated and is not supported in Databricks Runtime 14.0
            and above.
        jar:
          type: string
          description: 'URI of the JAR library to install. Supported URIs include
            Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example:
            `{ "jar": "/Workspace/path/to/library.jar" }`, `{ "jar" : "/Volumes/path/to/library.jar"
            }` or `{ "jar": "s3://my-bucket/library.jar" }`. If S3 is used, please
            make sure the cluster has read access on the library. You may need to
            launch the cluster with an IAM role to access the S3 URI.'
        maven:
          type: string
          description: 'Specification of a maven library to be installed. For example:
            `{ "coordinates": org.jsoup:jsoup:1.7.2" }`'
        pypi:
          type: string
          description: 'Specification of a PyPi library to be installed. For example:
            `{ "package": "simplejson" }`  requirements: Optional[str] = None """URI
            of the requirements.txt file to install. Only Workspace paths and Unity
            Catalog Volumes paths are supported. For example: `{ "requirements": "/Workspace/path/to/requirements.txt"
            }` or `{ "requirements" : "/Volumes/path/to/requirements.txt" }`'
        requirements:
          type: string
          description: 'URI of the requirements.txt file to install. Only Workspace
            paths and Unity Catalog Volumes paths are supported. For example: `{ "requirements":
            "/Workspace/path/to/requirements.txt" }` or `{ "requirements" : "/Volumes/path/to/requirements.txt"
            }`'
        whl:
          type: string
          description: 'URI of the wheel library to install. Supported URIs include
            Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example:
            `{ "whl": "/Workspace/path/to/library.whl" }`, `{ "whl" : "/Volumes/path/to/library.whl"
            }` or `{ "whl": "s3://my-bucket/library.whl" }`. If S3 is used, please
            make sure the cluster has read access on the library. You may need to
            launch the cluster with an IAM role to access the S3 URI.'
    LibraryFullStatus:
      type: object
      description: The status of the library on a specific cluster.
      properties:
        is_library_for_all_clusters:
          type: string
          description: 'Whether the library was set to be installed on all clusters
            via the libraries UI.  library: Optional[Library] = None Unique identifier
            for the library.'
        library:
          type: string
          description: 'Unique identifier for the library.  messages: Optional[List[str]]
            = None All the info and warning messages that have occurred so far for
            this library.'
        messages:
          type: string
          description: 'All the info and warning messages that have occurred so far
            for this library.  status: Optional[LibraryInstallStatus] = None Status
            of installing the library on the cluster.'
        status:
          type: string
          description: 'Status of installing the library on the cluster.  def as_dict(self)
            -> dict: Serializes the LibraryFullStatus into a dictionary suitable for
            use as a JSON request body.'
    LibraryInstallStatus:
      type: string
      description: The status of a library on a specific cluster.
      enum:
      - FAILED
      - INSTALLED
      - INSTALLING
      - PENDING
      - RESOLVING
      - RESTORED
      - SKIPPED
      - UNINSTALL_ON_RESTART
    ListAllClusterLibraryStatusesResponse:
      type: object
      description: 'ListAllClusterLibraryStatusesResponse(statuses: ''Optional[List[ClusterLibraryStatuses]]''
        = None)'
      properties:
        statuses:
          type: string
          description: 'A list of cluster statuses.  def as_dict(self) -> dict: Serializes
            the ListAllClusterLibraryStatusesResponse into a dictionary suitable for
            use as a JSON request body.'
    ListAvailableZonesResponse:
      type: object
      description: 'ListAvailableZonesResponse(default_zone: ''Optional[str]'' = None,
        zones: ''Optional[List[str]]'' = None)'
      properties:
        default_zone:
          type: string
          description: 'The availability zone if no ``zone_id`` is provided in the
            cluster creation request.  zones: Optional[List[str]] = None The list
            of available zones (e.g., [''us-west-2c'', ''us-east-2'']).'
        zones:
          type: string
          description: 'The list of available zones (e.g., [''us-west-2c'', ''us-east-2'']).  def
            as_dict(self) -> dict: Serializes the ListAvailableZonesResponse into
            a dictionary suitable for use as a JSON request body.'
    ListClusterCompliancesResponse:
      type: object
      description: 'ListClusterCompliancesResponse(clusters: ''Optional[List[ClusterCompliance]]''
        = None, next_page_token: ''Optional[str]'' = None, prev_page_token: ''Optional[str]''
        = None)'
      properties:
        clusters:
          type: string
          description: 'A list of clusters and their policy compliance statuses.  next_page_token:
            Optional[str] = None """This field represents the pagination token to
            retrieve the next page of results. If the value is , it means no further
            results for the request.'
        next_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            next page of results. If the value is , it means no further results for
            the request.
        prev_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            previous page of results. If the value is "", it means no further results
            for the request.
    ListClustersFilterBy:
      type: object
      description: 'ListClustersFilterBy(cluster_sources: ''Optional[List[ClusterSource]]''
        = None, cluster_states: ''Optional[List[State]]'' = None, is_pinned: ''Optional[bool]''
        = None, policy_id: ''Optional[str]'' = None)'
      properties:
        cluster_sources:
          type: string
          description: 'The source of cluster creation.  cluster_states: Optional[List[State]]
            = None The current state of the clusters.'
        cluster_states:
          type: string
          description: 'The current state of the clusters.  is_pinned: Optional[bool]
            = None Whether the clusters are pinned or not.'
        is_pinned:
          type: string
          description: 'Whether the clusters are pinned or not.  policy_id: Optional[str]
            = None The ID of the cluster policy used to create the cluster if applicable.'
        policy_id:
          type: string
          description: 'The ID of the cluster policy used to create the cluster if
            applicable.  def as_dict(self) -> dict: Serializes the ListClustersFilterBy
            into a dictionary suitable for use as a JSON request body.'
    ListClustersResponse:
      type: object
      description: 'ListClustersResponse(clusters: ''Optional[List[ClusterDetails]]''
        = None, next_page_token: ''Optional[str]'' = None, prev_page_token: ''Optional[str]''
        = None)'
      properties:
        clusters:
          type: string
          description: ''
        next_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            next page of results. If the value is , it means no further results for
            the request.
        prev_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            previous page of results. If the value is "", it means no further results
            for the request.
    ListClustersSortBy:
      type: object
      description: 'ListClustersSortBy(direction: ''Optional[ListClustersSortByDirection]''
        = None, field: ''Optional[ListClustersSortByField]'' = None)'
      properties:
        direction:
          type: string
          description: 'The direction to sort by.  field: Optional[ListClustersSortByField]
            = None """The sorting criteria. By default, clusters are sorted by 3 columns
            from highest to lowest precedence: cluster state, pinned or unpinned,
            then cluster name.'
        field:
          type: string
          description: 'The sorting criteria. By default, clusters are sorted by 3
            columns from highest to lowest precedence: cluster state, pinned or unpinned,
            then cluster name.'
    ListClustersSortByDirection:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - ASC
      - DESC
    ListClustersSortByField:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - CLUSTER_NAME
      - DEFAULT
    ListGlobalInitScriptsResponse:
      type: object
      description: 'ListGlobalInitScriptsResponse(scripts: ''Optional[List[GlobalInitScriptDetails]]''
        = None)'
      properties:
        scripts:
          type: string
          description: ''
    ListInstancePools:
      type: object
      description: 'ListInstancePools(instance_pools: ''Optional[List[InstancePoolAndStats]]''
        = None)'
      properties:
        instance_pools:
          type: string
          description: ''
    ListInstanceProfilesResponse:
      type: object
      description: 'ListInstanceProfilesResponse(instance_profiles: ''Optional[List[InstanceProfile]]''
        = None)'
      properties:
        instance_profiles:
          type: string
          description: 'A list of instance profiles that the user can access.  def
            as_dict(self) -> dict: Serializes the ListInstanceProfilesResponse into
            a dictionary suitable for use as a JSON request body.'
    ListNodeTypesResponse:
      type: object
      description: 'ListNodeTypesResponse(node_types: ''Optional[List[NodeType]]''
        = None)'
      properties:
        node_types:
          type: string
          description: 'The list of available Spark node types.  def as_dict(self)
            -> dict: Serializes the ListNodeTypesResponse into a dictionary suitable
            for use as a JSON request body.'
    ListPoliciesResponse:
      type: object
      description: 'ListPoliciesResponse(policies: ''Optional[List[Policy]]'' = None)'
      properties:
        policies:
          type: string
          description: 'List of policies.  def as_dict(self) -> dict: Serializes the
            ListPoliciesResponse into a dictionary suitable for use as a JSON request
            body.'
    ListPolicyFamiliesResponse:
      type: object
      description: 'ListPolicyFamiliesResponse(next_page_token: ''Optional[str]''
        = None, policy_families: ''Optional[List[PolicyFamily]]'' = None)'
      properties:
        next_page_token:
          type: string
          description: A token that can be used to get the next page of results. If
            not present, there are no more results to show.
        policy_families:
          type: string
          description: 'List of policy families.  def as_dict(self) -> dict: Serializes
            the ListPolicyFamiliesResponse into a dictionary suitable for use as a
            JSON request body.'
    ListSortColumn:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - POLICY_CREATION_TIME
      - POLICY_NAME
    ListSortOrder:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - ASC
      - DESC
    LocalFileInfo:
      type: object
      description: 'LocalFileInfo(destination: ''str'')'
      properties:
        destination:
          type: string
          description: 'local file destination, e.g. `file:/my/local/file.sh`  def
            as_dict(self) -> dict: Serializes the LocalFileInfo into a dictionary
            suitable for use as a JSON request body.'
    LogAnalyticsInfo:
      type: object
      description: 'LogAnalyticsInfo(log_analytics_primary_key: ''Optional[str]''
        = None, log_analytics_workspace_id: ''Optional[str]'' = None)'
      properties:
        log_analytics_primary_key:
          type: string
          description: ''
        log_analytics_workspace_id:
          type: string
          description: ''
    LogSyncStatus:
      type: object
      description: The log delivery status
      properties:
        last_attempted:
          type: string
          description: The timestamp of last attempt. If the last attempt fails, `last_exception`
            will contain the exception in the last attempt.
        last_exception:
          type: string
          description: The exception thrown in the last attempt, it would be null
            (omitted in the response) if there is no exception in last attempted.
    MavenLibrary:
      type: object
      description: 'MavenLibrary(coordinates: ''str'', exclusions: ''Optional[List[str]]''
        = None, repo: ''Optional[str]'' = None)'
      properties:
        coordinates:
          type: string
          description: 'Gradle-style maven coordinates. For example: "org.jsoup:jsoup:1.7.2".  exclusions:
            Optional[List[str]] = None """List of dependences to exclude. For example:
            `["slf4j:slf4j", "*:hadoop-client"]`.  Maven dependency exclusions: https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html.'
        exclusions:
          type: string
          description: 'List of dependences to exclude. For example: `["slf4j:slf4j",
            "*:hadoop-client"]`.  Maven dependency exclusions: https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html.'
        repo:
          type: string
          description: Maven repo to install the Maven package from. If omitted, both
            Maven Central Repository and Spark Packages are searched.
    NodeInstanceType:
      type: object
      description: 'This structure embodies the machine type that hosts spark containers
        Note: this should be an

        internal data structure for now It is defined in proto in case we want to
        send it over the wire

        in the future (which is likely)'
      properties:
        instance_type_id:
          type: string
          description: 'Unique identifier across instance types  local_disk_size_gb:
            Optional[int] = None Size of the individual local disks attached to this
            instance (i.e. per local disk).'
        local_disk_size_gb:
          type: string
          description: 'Size of the individual local disks attached to this instance
            (i.e. per local disk).  local_disks: Optional[int] = None Number of local
            disks that are present on this instance.'
        local_disks:
          type: string
          description: 'Number of local disks that are present on this instance.  local_nvme_disk_size_gb:
            Optional[int] = None Size of the individual local nvme disks attached
            to this instance (i.e. per local disk).'
        local_nvme_disk_size_gb:
          type: string
          description: 'Size of the individual local nvme disks attached to this instance
            (i.e. per local disk).  local_nvme_disks: Optional[int] = None Number
            of local nvme disks that are present on this instance.'
        local_nvme_disks:
          type: string
          description: 'Number of local nvme disks that are present on this instance.  def
            as_dict(self) -> dict: Serializes the NodeInstanceType into a dictionary
            suitable for use as a JSON request body.'
    NodeType:
      type: object
      description: 'A description of a Spark node type including both the dimensions
        of the node and the instance

        type on which it will be hosted.'
      properties:
        node_type_id:
          type: string
          description: 'Unique identifier for this node type.  memory_mb: int Memory
            (in MB) available for this node type.'
        memory_mb:
          type: string
          description: 'Memory (in MB) available for this node type.  num_cores: float
            """Number of CPU cores available for this node type. Note that this can
            be fractional, e.g., 2.5 cores, if the the number of cores on a machine
            instance is not divisible by the number of Spark nodes on that machine.'
        num_cores:
          type: string
          description: Number of CPU cores available for this node type. Note that
            this can be fractional, e.g., 2.5 cores, if the the number of cores on
            a machine instance is not divisible by the number of Spark nodes on that
            machine.
        description:
          type: string
          description: 'A string description associated with this node type, e.g.,
            "r3.xlarge".  instance_type_id: str An identifier for the type of hardware
            that this node runs on, e.g., "r3.2xlarge" in AWS.'
        instance_type_id:
          type: string
          description: 'An identifier for the type of hardware that this node runs
            on, e.g., "r3.2xlarge" in AWS.  category: str """A descriptive category
            for this node type. Examples include "Memory Optimized" and "Compute Optimized".'
        category:
          type: string
          description: A descriptive category for this node type. Examples include
            "Memory Optimized" and "Compute Optimized".
        display_order:
          type: string
          description: An optional hint at the display order of node types in the
            UI. Within a node type category, lowest numbers come first.
        is_deprecated:
          type: string
          description: 'Whether the node type is deprecated. Non-deprecated node types
            offer greater performance.  is_encrypted_in_transit: Optional[bool] =
            None """AWS specific, whether this instance supports encryption in transit,
            used for hipaa and pci workloads.'
        is_encrypted_in_transit:
          type: string
          description: AWS specific, whether this instance supports encryption in
            transit, used for hipaa and pci workloads.
        is_graviton:
          type: string
          description: 'Whether this is an Arm-based instance.  is_hidden: Optional[bool]
            = None Whether this node is hidden from presentation in the UI.'
        is_hidden:
          type: string
          description: 'Whether this node is hidden from presentation in the UI.  is_io_cache_enabled:
            Optional[bool] = None Whether this node comes with IO cache enabled by
            default.'
        is_io_cache_enabled:
          type: string
          description: 'Whether this node comes with IO cache enabled by default.  node_info:
            Optional[CloudProviderNodeInfo] = None A collection of node type info
            reported by the cloud provider'
        node_info:
          type: string
          description: 'A collection of node type info reported by the cloud provider  node_instance_type:
            Optional[NodeInstanceType] = None The NodeInstanceType object corresponding
            to instance_type_id'
        node_instance_type:
          type: string
          description: 'The NodeInstanceType object corresponding to instance_type_id  num_gpus:
            Optional[int] = None Number of GPUs available for this node type.'
        num_gpus:
          type: string
          description: 'Number of GPUs available for this node type.  photon_driver_capable:
            Optional[bool] = None  photon_worker_capable: Optional[bool] = None  support_cluster_tags:
            Optional[bool] = None Whether this node type support cluster tags.'
        photon_driver_capable:
          type: string
          description: ''
        photon_worker_capable:
          type: string
          description: ''
        support_cluster_tags:
          type: string
          description: 'Whether this node type support cluster tags.  support_ebs_volumes:
            Optional[bool] = None """Whether this node type support EBS volumes. EBS
            volumes is disabled for node types that we could place multiple corresponding
            containers on the same hosting instance.'
        support_ebs_volumes:
          type: string
          description: Whether this node type support EBS volumes. EBS volumes is
            disabled for node types that we could place multiple corresponding containers
            on the same hosting instance.
        support_port_forwarding:
          type: string
          description: 'Whether this node type supports port forwarding.  def as_dict(self)
            -> dict: Serializes the NodeType into a dictionary suitable for use as
            a JSON request body.'
    PendingInstanceError:
      type: object
      description: Error message of a failed pending instances
      properties:
        instance_id:
          type: string
          description: ''
        message:
          type: string
          description: ''
    PermanentDeleteClusterResponse:
      type: object
      description: PermanentDeleteClusterResponse()
      properties: {}
    PinClusterResponse:
      type: object
      description: PinClusterResponse()
      properties: {}
    Policy:
      type: object
      description: Describes a Cluster Policy entity.
      properties:
        created_at_timestamp:
          type: string
          description: 'Creation time. The timestamp (in millisecond) when this Cluster
            Policy was created.  creator_user_name: Optional[str] = None """Creator
            user name. The field won''t be included in the response if the user has
            already been deleted.'
        creator_user_name:
          type: string
          description: Creator user name. The field won't be included in the response
            if the user has already been deleted.
        definition:
          type: string
          description: 'Policy definition document expressed in [Databricks Cluster
            Policy Definition Language].  [Databricks Cluster Policy Definition Language]:
            https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
        description:
          type: string
          description: 'Additional human-readable description of the cluster policy.  is_default:
            Optional[bool] = None """If true, policy is a default policy created and
            managed by Databricks. Default policies cannot be deleted, and their policy
            families cannot be changed.'
        is_default:
          type: string
          description: If true, policy is a default policy created and managed by
            Databricks. Default policies cannot be deleted, and their policy families
            cannot be changed.
        libraries:
          type: string
          description: A list of libraries to be installed on the next cluster restart
            that uses this policy. The maximum number of libraries is 500.
        max_clusters_per_user:
          type: string
          description: Max number of clusters per user that can be active using this
            policy. If not present, there is no max limit.
        name:
          type: string
          description: Creator user name. The field won't be included in the response
            if the user has already been deleted.
        policy_family_definition_overrides:
          type: string
          description: 'Policy definition JSON document expressed in [Databricks Policy
            Definition Language]. The JSON document must be passed as a string and
            cannot be embedded in the requests.  You can use this to customize the
            policy definition inherited from the policy family. Policy rules specified
            here are merged into the inherited policy definition.  [Databricks Policy
            Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
        policy_family_id:
          type: string
          description: ID of the policy family. The cluster policy's policy definition
            inherits the policy family's policy definition.  Cannot be used with `definition`.
            Use `policy_family_definition_overrides` instead to customize the policy
            definition.
        policy_id:
          type: string
          description: 'Canonical unique identifier for the Cluster Policy.  def as_dict(self)
            -> dict: Serializes the Policy into a dictionary suitable for use as a
            JSON request body.'
    PolicyFamily:
      type: object
      description: 'PolicyFamily(definition: ''Optional[str]'' = None, description:
        ''Optional[str]'' = None, name: ''Optional[str]'' = None, policy_family_id:
        ''Optional[str]'' = None)'
      properties:
        definition:
          type: string
          description: 'Policy definition document expressed in [Databricks Cluster
            Policy Definition Language].  [Databricks Cluster Policy Definition Language]:
            https://docs.databricks.com/administration-guide/clusters/policy-definition.html'
        description:
          type: string
          description: 'Human-readable description of the purpose of the policy family.  name:
            Optional[str] = None Name of the policy family.'
        name:
          type: string
          description: 'Name of the policy family.  policy_family_id: Optional[str]
            = None Unique identifier for the policy family.'
        policy_family_id:
          type: string
          description: 'Unique identifier for the policy family.  def as_dict(self)
            -> dict: Serializes the PolicyFamily into a dictionary suitable for use
            as a JSON request body.'
    PythonPyPiLibrary:
      type: object
      description: 'PythonPyPiLibrary(package: ''str'', repo: ''Optional[str]'' =
        None)'
      properties:
        package:
          type: string
          description: 'The name of the pypi package to install. An optional exact
            version specification is also supported. Examples: "simplejson" and "simplejson==3.8.0".'
        repo:
          type: string
          description: 'The repository where the package can be found. If not specified,
            the default pip index is used.  def as_dict(self) -> dict: Serializes
            the PythonPyPiLibrary into a dictionary suitable for use as a JSON request
            body.'
    RCranLibrary:
      type: object
      description: 'RCranLibrary(package: ''str'', repo: ''Optional[str]'' = None)'
      properties:
        package:
          type: string
          description: 'The name of the CRAN package to install.  repo: Optional[str]
            = None The repository where the package can be found. If not specified,
            the default CRAN repo is used.'
        repo:
          type: string
          description: 'The repository where the package can be found. If not specified,
            the default CRAN repo is used.  def as_dict(self) -> dict: Serializes
            the RCranLibrary into a dictionary suitable for use as a JSON request
            body.'
    RemoveResponse:
      type: object
      description: RemoveResponse()
      properties: {}
    ResizeClusterResponse:
      type: object
      description: ResizeClusterResponse()
      properties: {}
    RestartClusterResponse:
      type: object
      description: RestartClusterResponse()
      properties: {}
    ResultType:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - error
      - image
      - images
      - table
      - text
    Results:
      type: object
      description: 'Results(cause: ''Optional[str]'' = None, data: ''Optional[Any]''
        = None, file_name: ''Optional[str]'' = None, file_names: ''Optional[List[str]]''
        = None, is_json_schema: ''Optional[bool]'' = None, pos: ''Optional[int]''
        = None, result_type: ''Optional[ResultType]'' = None, schema: ''Optional[List[Dict[str,
        Any]]]'' = None, summary: ''Optional[str]'' = None, truncated: ''Optional[bool]''
        = None)'
      properties:
        cause:
          type: string
          description: 'The cause of the error  data: Optional[Any] = None  file_name:
            Optional[str] = None """The image data in one of the following formats:  1.
            A Data URL with base64-encoded image data: `data:image/{type};base64,{base64-data}`.
            Example: `data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUA...`  2. A
            FileStore file path for large images: `/plots/{filename}.png`. Example:
            `/plots/b6a7ad70-fb2c-4353-8aed-3f1e015174a4.png`'
        data:
          type: string
          description: ''
        file_name:
          type: string
          description: 'The image data in one of the following formats:  1. A Data
            URL with base64-encoded image data: `data:image/{type};base64,{base64-data}`.
            Example: `data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUA...`  2. A
            FileStore file path for large images: `/plots/{filename}.png`. Example:
            `/plots/b6a7ad70-fb2c-4353-8aed-3f1e015174a4.png`'
        file_names:
          type: string
          description: 'List of image data for multiple images. Each element follows
            the same format as file_name.  is_json_schema: Optional[bool] = None true
            if a JSON schema is returned instead of a string representation of the
            Hive type.'
        is_json_schema:
          type: string
          description: 'true if a JSON schema is returned instead of a string representation
            of the Hive type.  pos: Optional[int] = None internal field used by SDK'
        pos:
          type: string
          description: 'internal field used by SDK  result_type: Optional[ResultType]
            = None  schema: Optional[List[Dict[str, Any]]] = None The table schema'
        result_type:
          type: string
          description: ''
        schema:
          type: string
          description: 'true if a JSON schema is returned instead of a string representation
            of the Hive type.  pos: Optional[int] = None internal field used by SDK'
        summary:
          type: string
          description: 'The summary of the error  truncated: Optional[bool] = None
            true if partial results are returned.'
        truncated:
          type: string
          description: 'true if partial results are returned.  def as_dict(self) ->
            dict: Serializes the Results into a dictionary suitable for use as a JSON
            request body.'
    RuntimeEngine:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - 'NULL'
      - PHOTON
      - STANDARD
    S3StorageInfo:
      type: object
      description: A storage location in Amazon S3
      properties:
        destination:
          type: string
          description: S3 destination, e.g. `s3://my-bucket/some-prefix` Note that
            logs will be delivered using cluster iam role, please make sure you set
            cluster iam role and the role has write access to the destination. Please
            also note that you cannot use AWS keys to deliver logs.
        canned_acl:
          type: string
          description: (Optional) Set canned access control list for the logs, e.g.
            `bucket-owner-full-control`. If `canned_cal` is set, please make sure
            the cluster iam role has `s3:PutObjectAcl` permission on the destination
            bucket and prefix. The full list of possible canned acl can be found at
            http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl.
            Please also note that by default only the object owner gets full controls.
            If you are using cross account role for writing data, you may want to
            set `bucket-owner-full-control` to make bucket owner able to read the
            logs.
        enable_encryption:
          type: string
          description: '(Optional) Flag to enable server side encryption, `false`
            by default.  encryption_type: Optional[str] = None """(Optional) The encryption
            type, it could be `sse-s3` or `sse-kms`. It will be used only when encryption
            is enabled and the default type is `sse-s3`.'
        encryption_type:
          type: string
          description: (Optional) The encryption type, it could be `sse-s3` or `sse-kms`.
            It will be used only when encryption is enabled and the default type is
            `sse-s3`.
        endpoint:
          type: string
          description: S3 endpoint, e.g. `https://s3-us-west-2.amazonaws.com`. Either
            region or endpoint needs to be set. If both are set, endpoint will be
            used.
        kms_key:
          type: string
          description: (Optional) Kms key which will be used if encryption is enabled
            and encryption type is set to `sse-kms`.
        region:
          type: string
          description: S3 region, e.g. `us-west-2`. Either region or endpoint needs
            to be set. If both are set, endpoint will be used.
    SparkNode:
      type: object
      description: Describes a specific Spark driver or executor.
      properties:
        host_private_ip:
          type: string
          description: 'The private IP address of the host instance.  instance_id:
            Optional[str] = None Globally unique identifier for the host instance
            from the cloud provider.'
        instance_id:
          type: string
          description: 'Globally unique identifier for the host instance from the
            cloud provider.  node_aws_attributes: Optional[SparkNodeAwsAttributes]
            = None Attributes specific to AWS for a Spark node.'
        node_aws_attributes:
          type: string
          description: 'Attributes specific to AWS for a Spark node.  node_id: Optional[str]
            = None Globally unique identifier for this node.'
        node_id:
          type: string
          description: 'Globally unique identifier for this node.  private_ip: Optional[str]
            = None """Private IP address (typically a 10.x.x.x address) of the Spark
            node. Note that this is different from the private IP address of the host
            instance.'
        private_ip:
          type: string
          description: 'The private IP address of the host instance.  instance_id:
            Optional[str] = None Globally unique identifier for the host instance
            from the cloud provider.'
        public_dns:
          type: string
          description: Public DNS address of this node. This address can be used to
            access the Spark JDBC server on the driver node. To communicate with the
            JDBC server, traffic must be manually authorized by adding security group
            rules to the "worker-unmanaged" security group via the AWS console.
        start_timestamp:
          type: string
          description: 'The timestamp (in millisecond) when the Spark node is launched.  def
            as_dict(self) -> dict: Serializes the SparkNode into a dictionary suitable
            for use as a JSON request body.'
    SparkNodeAwsAttributes:
      type: object
      description: Attributes specific to AWS for a Spark node.
      properties:
        is_spot:
          type: string
          description: 'Whether this node is on an Amazon spot instance.  def as_dict(self)
            -> dict: Serializes the SparkNodeAwsAttributes into a dictionary suitable
            for use as a JSON request body.'
    SparkVersion:
      type: object
      description: 'SparkVersion(key: ''Optional[str]'' = None, name: ''Optional[str]''
        = None)'
      properties:
        key:
          type: string
          description: Spark version key, for example "2.1.x-scala2.11". This is the
            value which should be provided as the "spark_version" when creating a
            new cluster. Note that the exact Spark version may change over time for
            a "wildcard" version (i.e., "2.1.x-scala2.11" is a "wildcard" version)
            with minor bug fixes.
        name:
          type: string
          description: 'A descriptive name for this Spark version, for example "Spark
            2.1".  def as_dict(self) -> dict: Serializes the SparkVersion into a dictionary
            suitable for use as a JSON request body.'
    StartClusterResponse:
      type: object
      description: StartClusterResponse()
      properties: {}
    State:
      type: string
      description: 'The state of a Cluster. The current allowable state transitions
        are as follows:


        - `PENDING` -> `RUNNING` - `PENDING` -> `TERMINATING` - `RUNNING` -> `RESIZING`
        - `RUNNING` ->

        `RESTARTING` - `RUNNING` -> `TERMINATING` - `RESTARTING` -> `RUNNING` - `RESTARTING`
        ->

        `TERMINATING` - `RESIZING` -> `RUNNING` - `RESIZING` -> `TERMINATING` - `TERMINATING`
        ->

        `TERMINATED`'
      enum:
      - ERROR
      - PENDING
      - RESIZING
      - RESTARTING
      - RUNNING
      - TERMINATED
      - TERMINATING
      - UNKNOWN
    TerminationReason:
      type: object
      description: 'TerminationReason(code: ''Optional[TerminationReasonCode]'' =
        None, parameters: ''Optional[Dict[str, str]]'' = None, type: ''Optional[TerminationReasonType]''
        = None)'
      properties:
        code:
          type: string
          description: 'status code indicating why the cluster was terminated  parameters:
            Optional[Dict[str, str]] = None list of parameters that provide additional
            information about why the cluster was terminated'
        parameters:
          type: string
          description: 'list of parameters that provide additional information about
            why the cluster was terminated  type: Optional[TerminationReasonType]
            = None type of the termination'
        type:
          type: string
          description: 'type of the termination  def as_dict(self) -> dict: Serializes
            the TerminationReason into a dictionary suitable for use as a JSON request
            body.'
    TerminationReasonCode:
      type: string
      description: The status code indicating why the cluster was terminated
      enum:
      - ABUSE_DETECTED
      - ACCESS_TOKEN_FAILURE
      - ALLOCATION_TIMEOUT
      - ALLOCATION_TIMEOUT_NODE_DAEMON_NOT_READY
      - ALLOCATION_TIMEOUT_NO_HEALTHY_AND_WARMED_UP_CLUSTERS
      - ALLOCATION_TIMEOUT_NO_HEALTHY_CLUSTERS
      - ALLOCATION_TIMEOUT_NO_MATCHED_CLUSTERS
      - ALLOCATION_TIMEOUT_NO_READY_CLUSTERS
      - ALLOCATION_TIMEOUT_NO_UNALLOCATED_CLUSTERS
      - ALLOCATION_TIMEOUT_NO_WARMED_UP_CLUSTERS
      - ATTACH_PROJECT_FAILURE
      - AWS_AUTHORIZATION_FAILURE
      - AWS_INACCESSIBLE_KMS_KEY_FAILURE
      - AWS_INSTANCE_PROFILE_UPDATE_FAILURE
      - AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE
      - AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE
      - AWS_INVALID_KEY_PAIR
      - AWS_INVALID_KMS_KEY_STATE
      - AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE
      - AWS_REQUEST_LIMIT_EXCEEDED
      - AWS_RESOURCE_QUOTA_EXCEEDED
      - AWS_UNSUPPORTED_FAILURE
      - AZURE_BYOK_KEY_PERMISSION_FAILURE
      - AZURE_EPHEMERAL_DISK_FAILURE
      - AZURE_INVALID_DEPLOYMENT_TEMPLATE
      - AZURE_OPERATION_NOT_ALLOWED_EXCEPTION
      - AZURE_PACKED_DEPLOYMENT_PARTIAL_FAILURE
      - AZURE_QUOTA_EXCEEDED_EXCEPTION
      - AZURE_RESOURCE_MANAGER_THROTTLING
      - AZURE_RESOURCE_PROVIDER_THROTTLING
      - AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE
      - AZURE_VM_EXTENSION_FAILURE
      - AZURE_VNET_CONFIGURATION_FAILURE
      - BOOTSTRAP_TIMEOUT
      - BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION
      - BOOTSTRAP_TIMEOUT_DUE_TO_MISCONFIG
      - BUDGET_POLICY_LIMIT_ENFORCEMENT_ACTIVATED
      - BUDGET_POLICY_RESOLUTION_FAILURE
      - CLOUD_ACCOUNT_POD_QUOTA_EXCEEDED
      - CLOUD_ACCOUNT_SETUP_FAILURE
      - CLOUD_OPERATION_CANCELLED
      - CLOUD_PROVIDER_DISK_SETUP_FAILURE
      - CLOUD_PROVIDER_INSTANCE_NOT_LAUNCHED
      - CLOUD_PROVIDER_LAUNCH_FAILURE
      - CLOUD_PROVIDER_LAUNCH_FAILURE_DUE_TO_MISCONFIG
      - CLOUD_PROVIDER_RESOURCE_STOCKOUT
      - CLOUD_PROVIDER_RESOURCE_STOCKOUT_DUE_TO_MISCONFIG
      - CLOUD_PROVIDER_SHUTDOWN
      - CLUSTER_OPERATION_THROTTLED
      - CLUSTER_OPERATION_TIMEOUT
      - COMMUNICATION_LOST
      - CONTAINER_LAUNCH_FAILURE
      - CONTROL_PLANE_REQUEST_FAILURE
      - CONTROL_PLANE_REQUEST_FAILURE_DUE_TO_MISCONFIG
      - DATABASE_CONNECTION_FAILURE
      - DATA_ACCESS_CONFIG_CHANGED
      - DBFS_COMPONENT_UNHEALTHY
      - DISASTER_RECOVERY_REPLICATION
      - DNS_RESOLUTION_ERROR
      - DOCKER_CONTAINER_CREATION_EXCEPTION
      - DOCKER_IMAGE_PULL_FAILURE
      - DOCKER_IMAGE_TOO_LARGE_FOR_INSTANCE_EXCEPTION
      - DOCKER_INVALID_OS_EXCEPTION
      - DRIVER_DNS_RESOLUTION_FAILURE
      - DRIVER_EVICTION
      - DRIVER_LAUNCH_TIMEOUT
      - DRIVER_NODE_UNREACHABLE
      - DRIVER_OUT_OF_DISK
      - DRIVER_OUT_OF_MEMORY
      - DRIVER_POD_CREATION_FAILURE
      - DRIVER_UNEXPECTED_FAILURE
      - DRIVER_UNHEALTHY
      - DRIVER_UNREACHABLE
      - DRIVER_UNRESPONSIVE
      - DYNAMIC_SPARK_CONF_SIZE_EXCEEDED
      - EOS_SPARK_IMAGE
      - EXECUTION_COMPONENT_UNHEALTHY
      - EXECUTOR_POD_UNSCHEDULED
      - GCP_API_RATE_QUOTA_EXCEEDED
      - GCP_DENIED_BY_ORG_POLICY
      - GCP_FORBIDDEN
      - GCP_IAM_TIMEOUT
      - GCP_INACCESSIBLE_KMS_KEY_FAILURE
      - GCP_INSUFFICIENT_CAPACITY
      - GCP_IP_SPACE_EXHAUSTED
      - GCP_KMS_KEY_PERMISSION_DENIED
      - GCP_NOT_FOUND
      - GCP_QUOTA_EXCEEDED
      - GCP_RESOURCE_QUOTA_EXCEEDED
      - GCP_SERVICE_ACCOUNT_ACCESS_DENIED
      - GCP_SERVICE_ACCOUNT_DELETED
      - GCP_SERVICE_ACCOUNT_NOT_FOUND
      - GCP_SUBNET_NOT_READY
      - GCP_TRUSTED_IMAGE_PROJECTS_VIOLATED
      - GKE_BASED_CLUSTER_TERMINATION
      - GLOBAL_INIT_SCRIPT_FAILURE
      - HIVE_METASTORE_PROVISIONING_FAILURE
      - IMAGE_PULL_PERMISSION_DENIED
      - INACTIVITY
      - INIT_CONTAINER_NOT_FINISHED
      - INIT_SCRIPT_FAILURE
      - INSTANCE_POOL_CLUSTER_FAILURE
      - INSTANCE_POOL_MAX_CAPACITY_REACHED
      - INSTANCE_POOL_NOT_FOUND
      - INSTANCE_UNREACHABLE
      - INSTANCE_UNREACHABLE_DUE_TO_MISCONFIG
      - INTERNAL_CAPACITY_FAILURE
      - INTERNAL_ERROR
      - INVALID_ARGUMENT
      - INVALID_AWS_PARAMETER
      - INVALID_INSTANCE_PLACEMENT_PROTOCOL
      - INVALID_SPARK_IMAGE
      - INVALID_WORKER_IMAGE_FAILURE
      - IN_PENALTY_BOX
      - IP_EXHAUSTION_FAILURE
      - JOB_FINISHED
      - K8S_ACTIVE_POD_QUOTA_EXCEEDED
      - K8S_AUTOSCALING_FAILURE
      - K8S_DBR_CLUSTER_LAUNCH_TIMEOUT
      - LAZY_ALLOCATION_TIMEOUT
      - MAINTENANCE_MODE
      - METASTORE_COMPONENT_UNHEALTHY
      - NEPHOS_RESOURCE_MANAGEMENT
      - NETVISOR_SETUP_TIMEOUT
      - NETWORK_CHECK_CONTROL_PLANE_FAILURE
      - NETWORK_CHECK_CONTROL_PLANE_FAILURE_DUE_TO_MISCONFIG
      - NETWORK_CHECK_DNS_SERVER_FAILURE
      - NETWORK_CHECK_DNS_SERVER_FAILURE_DUE_TO_MISCONFIG
      - NETWORK_CHECK_METADATA_ENDPOINT_FAILURE
      - NETWORK_CHECK_METADATA_ENDPOINT_FAILURE_DUE_TO_MISCONFIG
      - NETWORK_CHECK_MULTIPLE_COMPONENTS_FAILURE
      - NETWORK_CHECK_MULTIPLE_COMPONENTS_FAILURE_DUE_TO_MISCONFIG
      - NETWORK_CHECK_NIC_FAILURE
      - NETWORK_CHECK_NIC_FAILURE_DUE_TO_MISCONFIG
      - NETWORK_CHECK_STORAGE_FAILURE
      - NETWORK_CHECK_STORAGE_FAILURE_DUE_TO_MISCONFIG
      - NETWORK_CONFIGURATION_FAILURE
      - NFS_MOUNT_FAILURE
      - NO_ACTIVATED_K8S
      - NO_ACTIVATED_K8S_TESTING_TAG
      - NO_MATCHED_K8S
      - NO_MATCHED_K8S_TESTING_TAG
      - NPIP_TUNNEL_SETUP_FAILURE
      - NPIP_TUNNEL_TOKEN_FAILURE
      - POD_ASSIGNMENT_FAILURE
      - POD_SCHEDULING_FAILURE
      - REQUEST_REJECTED
      - REQUEST_THROTTLED
      - RESOURCE_USAGE_BLOCKED
      - SECRET_CREATION_FAILURE
      - SECRET_PERMISSION_DENIED
      - SECRET_RESOLUTION_ERROR
      - SECURITY_AGENTS_FAILED_INITIAL_VERIFICATION
      - SECURITY_DAEMON_REGISTRATION_EXCEPTION
      - SELF_BOOTSTRAP_FAILURE
      - SERVERLESS_LONG_RUNNING_TERMINATED
      - SKIPPED_SLOW_NODES
      - SLOW_IMAGE_DOWNLOAD
      - SPARK_ERROR
      - SPARK_IMAGE_DOWNLOAD_FAILURE
      - SPARK_IMAGE_DOWNLOAD_THROTTLED
      - SPARK_IMAGE_NOT_FOUND
      - SPARK_STARTUP_FAILURE
      - SPOT_INSTANCE_TERMINATION
      - SSH_BOOTSTRAP_FAILURE
      - STORAGE_DOWNLOAD_FAILURE
      - STORAGE_DOWNLOAD_FAILURE_DUE_TO_MISCONFIG
      - STORAGE_DOWNLOAD_FAILURE_SLOW
      - STORAGE_DOWNLOAD_FAILURE_THROTTLED
      - STS_CLIENT_SETUP_FAILURE
      - SUBNET_EXHAUSTED_FAILURE
      - TEMPORARILY_UNAVAILABLE
      - TRIAL_EXPIRED
      - UNEXPECTED_LAUNCH_FAILURE
      - UNEXPECTED_POD_RECREATION
      - UNKNOWN
      - UNSUPPORTED_INSTANCE_TYPE
      - UPDATE_INSTANCE_PROFILE_FAILURE
      - USAGE_POLICY_ENTITLEMENT_DENIED
      - USER_INITIATED_VM_TERMINATION
      - USER_REQUEST
      - WORKER_SETUP_FAILURE
      - WORKSPACE_CANCELLED_ERROR
      - WORKSPACE_CONFIGURATION_ERROR
      - WORKSPACE_UPDATE
    TerminationReasonType:
      type: string
      description: type of the termination
      enum:
      - CLIENT_ERROR
      - CLOUD_FAILURE
      - SERVICE_FAULT
      - SUCCESS
    UninstallLibrariesResponse:
      type: object
      description: UninstallLibrariesResponse()
      properties: {}
    UnpinClusterResponse:
      type: object
      description: UnpinClusterResponse()
      properties: {}
    UpdateClusterResource:
      type: object
      description: 'UpdateClusterResource(autoscale: ''Optional[AutoScale]'' = None,
        autotermination_minutes: ''Optional[int]'' = None, aws_attributes: ''Optional[AwsAttributes]''
        = None, azure_attributes: ''Optional[AzureAttributes]'' = None, cluster_log_conf:
        ''Optional[ClusterLogConf]'' = None, cluster_name: ''Optional[str]'' = None,
        custom_tags: ''Optional[Dict[str, str]]'' = None, data_security_mode: ''Optional[DataSecurityMode]''
        = None, docker_image: ''Optional[DockerImage]'' = None, driver_instance_pool_id:
        ''Optional[str]'' = None, driver_node_type_id: ''Optional[str]'' = None, enable_elastic_disk:
        ''Optional[bool]'' = None, enable_local_disk_encryption: ''Optional[bool]''
        = None, gcp_attributes: ''Optional[GcpAttributes]'' = None, init_scripts:
        ''Optional[List[InitScriptInfo]]'' = None, instance_pool_id: ''Optional[str]''
        = None, is_single_node: ''Optional[bool]'' = None, kind: ''Optional[Kind]''
        = None, node_type_id: ''Optional[str]'' = None, num_workers: ''Optional[int]''
        = None, policy_id: ''Optional[str]'' = None, remote_disk_throughput: ''Optional[int]''
        = None, runtime_engine: ''Optional[RuntimeEngine]'' = None, single_user_name:
        ''Optional[str]'' = None, spark_conf: ''Optional[Dict[str, str]]'' = None,
        spark_env_vars: ''Optional[Dict[str, str]]'' = None, spark_version: ''Optional[str]''
        = None, ssh_public_keys: ''Optional[List[str]]'' = None, total_initial_remote_disk_size:
        ''Optional[int]'' = None, use_ml_runtime: ''Optional[bool]'' = None, workload_type:
        ''Optional[WorkloadType]'' = None)'
      properties:
        autoscale:
          type: string
          description: 'Parameters needed in order to automatically scale clusters
            up and down based on load. Note: autoscaling works best with DB runtime
            versions 3.0 or later.'
        autotermination_minutes:
          type: string
          description: Automatically terminates the cluster after it is inactive for
            this time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000 minutes.
            Users can also set this value to 0 to explicitly disable automatic termination.
        aws_attributes:
          type: string
          description: Attributes related to clusters running on Amazon Web Services.
            If not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          type: string
          description: Attributes related to clusters running on Microsoft Azure.
            If not specified at cluster creation, a set of default values will be
            used.
        cluster_log_conf:
          type: string
          description: The configuration for delivering spark logs to a long-term
            storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for one
            cluster. If the conf is given, the logs will be delivered to the destination
            every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`,
            while the destination of executor logs is `$destination/$clusterId/executor`.
        cluster_name:
          type: string
          description: Cluster name requested by the user. This doesn't have to be
            unique. If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set based
            on the job and job run IDs.
        custom_tags:
          type: string
          description: 'Additional tags for cluster resources. Databricks will tag
            all cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes:  - Currently, Databricks allows
            at most 45 custom tags  - Clusters can only reuse cloud resources if the
            resources'' tags are a subset of the cluster tags'
        data_security_mode:
          type: string
          description: ''
        docker_image:
          type: string
          description: 'Custom docker image BYOC  driver_instance_pool_id: Optional[str]
            = None """The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.'
        driver_instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        driver_node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        enable_elastic_disk:
          type: string
          description: 'Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are running
            low on disk space. This feature requires specific AWS permissions to function
            correctly - refer to the User Guide for more details.'
        enable_local_disk_encryption:
          type: string
          description: 'Whether to enable LUKS on cluster VMs'' local disks  gcp_attributes:
            Optional[GcpAttributes] = None """Attributes related to clusters running
            on Google Cloud Platform. If not specified at cluster creation, a set
            of default values will be used.'
        gcp_attributes:
          type: string
          description: Attributes related to clusters running on Google Cloud Platform.
            If not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: string
          description: The configuration for storing init scripts. Any number of destinations
            can be specified. The scripts are executed sequentially in the order provided.
            If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool for the driver of the
            cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id)
            if the driver pool is not assigned.
        is_single_node:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  When
            set to true, Databricks will automatically set single node related `custom_tags`,
            `spark_conf`, and `num_workers`
        kind:
          type: string
          description: ''
        node_type_id:
          type: string
          description: The node type of the Spark driver. Note that this field is
            optional; if unset, the driver node type will be set as the same value
            as `node_type_id` defined above.  This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both driver_node_type_id,
            node_type_id, and virtual_cluster_size are specified, driver_node_type_id
            and node_type_id take precedence.
        num_workers:
          type: string
          description: 'Number of worker nodes that this cluster should have. A cluster
            has one Spark Driver and `num_workers` Executors for a total of `num_workers`
            + 1 Spark nodes.  Note: When reading the properties of a cluster, this
            field reflects the desired number of workers rather than the actual current
            number of workers. For instance, if a cluster is resized from 5 to 10
            workers, this field will immediately be updated to reflect the target
            size of 10 workers, whereas the workers listed in `spark_info` will gradually
            increase from 5 to 10 as the new nodes are provisioned.'
        policy_id:
          type: string
          description: 'The ID of the cluster policy used to create the cluster if
            applicable.  remote_disk_throughput: Optional[int] = None """If set, what
            the configurable throughput (in Mb/s) for the remote disk is. Currently
            only supported for GCP HYPERDISK_BALANCED disks.'
        remote_disk_throughput:
          type: string
          description: If set, what the configurable throughput (in Mb/s) for the
            remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          type: string
          description: Determines the cluster's runtime engine, either standard or
            Photon.  This field is not compatible with legacy `spark_version` values
            that contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`.  If left unspecified, the runtime engine
            defaults to standard unless the spark_version contains -photon-, in which
            case Photon will be used.
        single_user_name:
          type: string
          description: 'Single user name if data_security_mode is `SINGLE_USER`  spark_conf:
            Optional[Dict[str, str]] = None """An object containing a set of optional,
            user-specified Spark configuration key-value pairs. Users can also pass
            in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.'
        spark_conf:
          type: string
          description: An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of extra
            JVM options to the driver and the executors via `spark.driver.extraJavaOptions`
            and `spark.executor.extraJavaOptions` respectively.
        spark_env_vars:
          type: string
          description: 'An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the form
            (X,Y) will be exported as is (i.e., `export X=''Y''`) while launching
            the driver and workers.  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`,
            we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the
            example below. This ensures that all default databricks managed environmental
            variables are included as well.  Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS": "/local_disk0"}`
            or `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`'
        spark_version:
          type: string
          description: The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A
            list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions
            API call.
        ssh_public_keys:
          type: string
          description: SSH public key contents that will be added to each Spark node
            in this cluster. The corresponding private keys can be used to login with
            the user name `ubuntu` on port `2200`. Up to 10 keys can be specified.
        total_initial_remote_disk_size:
          type: string
          description: If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks.
        use_ml_runtime:
          type: string
          description: This field can only be used when `kind = CLASSIC_PREVIEW`.  `effective_spark_version`
            is determined by `spark_version` (DBR release), this field `use_ml_runtime`,
            and whether `node_type_id` is gpu node or not.
        workload_type:
          type: string
          description: ''
    UpdateClusterResponse:
      type: object
      description: UpdateClusterResponse()
      properties: {}
    UpdateResponse:
      type: object
      description: UpdateResponse()
      properties: {}
    VolumesStorageInfo:
      type: object
      description: A storage location back by UC Volumes.
      properties:
        destination:
          type: string
          description: UC Volumes destination, e.g. `/Volumes/catalog/schema/vol1/init-scripts/setup-datadog.sh`
            or `dbfs:/Volumes/catalog/schema/vol1/init-scripts/setup-datadog.sh`
    WorkloadType:
      type: object
      description: Cluster Attributes showing for clusters workload types.
      properties:
        clients:
          type: string
          description: 'defined what type of clients can use the cluster. E.g. Notebooks,
            Jobs  def as_dict(self) -> dict: Serializes the WorkloadType into a dictionary
            suitable for use as a JSON request body.'
    WorkspaceStorageInfo:
      type: object
      description: A storage location in Workspace Filesystem (WSFS)
      properties:
        destination:
          type: string
          description: 'wsfs destination, e.g. `workspace:/cluster-init-scripts/setup-datadog.sh`  def
            as_dict(self) -> dict: Serializes the WorkspaceStorageInfo into a dictionary
            suitable for use as a JSON request body.'
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
      description: Databricks personal access token
security:
- bearerAuth: []
