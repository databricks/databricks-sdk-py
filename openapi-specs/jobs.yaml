openapi: 3.0.0
info:
  title: Databricks Jobs API
  description: API for Databricks jobs service
  version: 1.0.0
  contact:
    name: Databricks
    url: https://databricks.com
servers:
- url: https://{deployment_name}.cloud.databricks.com
  description: Databricks Workspace API
  variables:
    deployment_name:
      default: your-deployment
      description: Databricks workspace deployment name
paths:
  /api/2.2/jobs/runs/cancel-all:
    post:
      operationId: cancel_all_runs
      summary: 'Cancels all active runs of a job. The runs are canceled asynchronously,
        so it doesn''t prevent new runs

        from being started.'
      description: "Cancels all active runs of a job. The runs are canceled asynchronously,\
        \ so it doesn't prevent new runs\nfrom being started.\n\n:param all_queued_runs:\
        \ bool (optional)\n  Optional boolean parameter to cancel all queued runs.\
        \ If no job_id is provided, all queued runs in\n  the workspace are canceled.\n\
        :param job_id: int (optional)\n  The canonical identifier of the job to cancel\
        \ all runs of."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                all_queued_runs:
                  type: boolean
                  description: bool (optional) Optional boolean parameter to cancel
                    all queued runs. If no job_id is provided, all queued runs in
                    the workspace are canceled.
                job_id:
                  type: integer
                  description: int (optional) The canonical identifier of the job
                    to cancel all runs of.
      responses:
        '200':
          description: Success
  /api/2.2/jobs/runs/cancel:
    post:
      operationId: cancel_run
      summary: 'Cancels a job run or a task run. The run is canceled asynchronously,
        so it may still be running when

        this request completes.'
      description: "Cancels a job run or a task run. The run is canceled asynchronously,\
        \ so it may still be running when\nthis request completes.\n\n:param run_id:\
        \ int\n  This field is required.\n\n:returns:\n  Long-running operation waiter\
        \ for :class:`Run`.\n  See :method:wait_get_run_job_terminated_or_skipped\
        \ for more details."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                run_id:
                  type: integer
                  description: int This field is required.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Run'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/create:
    post:
      operationId: create
      summary: Create a new job.
      description: "Create a new job.\n\n:param access_control_list: List[:class:`JobAccessControlRequest`]\
        \ (optional)\n  List of permissions to set on the job.\n:param budget_policy_id:\
        \ str (optional)\n  The id of the user specified budget policy to use for\
        \ this job. If not specified, a default budget\n  policy may be applied when\
        \ creating or modifying the job. See `effective_budget_policy_id` for the\n\
        \  budget policy used by this workload.\n:param continuous: :class:`Continuous`\
        \ (optional)\n  An optional continuous property for this job. The continuous\
        \ property will ensure that there is\n  always one run executing. Only one\
        \ of `schedule` and `continuous` can be used.\n:param deployment: :class:`JobDeployment`\
        \ (optional)\n  Deployment information for jobs managed by external sources.\n\
        :param description: str (optional)\n  An optional description for the job.\
        \ The maximum length is 27700 characters in UTF-8 encoding.\n:param edit_mode:\
        \ :class:`JobEditMode` (optional)\n  Edit mode of the job.\n\n  * `UI_LOCKED`:\
        \ The job is in a locked UI state and cannot be modified. * `EDITABLE`: The\
        \ job is in\n  an editable state and can be modified.\n:param email_notifications:\
        \ :class:`JobEmailNotifications` (optional)\n  An optional set of email addresses\
        \ that is notified when runs of this job begin or complete as well\n  as when\
        \ this job is deleted.\n:param environments: List[:class:`JobEnvironment`]\
        \ (optional)\n  A list of task execution environment specifications that can\
        \ be referenced by serverless tasks of\n  this job. An environment is required\
        \ to be present for serverless tasks. For serverless notebook\n  tasks, the\
        \ environment is accessible in the notebook environment panel. For other serverless\
        \ tasks,\n  the task environment is required to be specified using environment_key\
        \ in the task settings.\n:param format: :class:`Format` (optional)\n  Used\
        \ to tell what is the format of the job. This field is ignored in Create/Update/Reset\
        \ calls. When\n  using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"\
        `.\n:param git_source: :class:`GitSource` (optional)\n  An optional specification\
        \ for a remote Git repository containing the source code used by tasks.\n\
        \  Version-controlled source code is supported by notebook, dbt, Python script,\
        \ and SQL File tasks.\n\n  If `git_source` is set, these tasks retrieve the\
        \ file from the remote repository by default.\n  However, this behavior can\
        \ be overridden by setting `source` to `WORKSPACE` on the task.\n\n  Note:\
        \ dbt and SQL File tasks support only version-controlled sources. If dbt or\
        \ SQL File tasks are\n  used, `git_source` must be defined on the job.\n:param\
        \ health: :class:`JobsHealthRules` (optional)\n:param job_clusters: List[:class:`JobCluster`]\
        \ (optional)\n  A list of job cluster specifications that can be shared and\
        \ reused by tasks of this job. Libraries\n  cannot be declared in a shared\
        \ job cluster. You must declare dependent libraries in task settings.\n:param\
        \ max_concurrent_runs: int (optional)\n  An optional maximum allowed number\
        \ of concurrent runs of the job. Set this value if you want to be\n  able\
        \ to execute multiple runs of the same job concurrently. This is useful for\
        \ example if you\n  trigger your job on a frequent schedule and want to allow\
        \ consecutive runs to overlap with each\n  other, or if you want to trigger\
        \ multiple runs which differ by their input parameters. This setting\n  affects\
        \ only new runs. For example, suppose the job’s concurrency is 4 and there\
        \ are 4 concurrent\n  active runs. Then setting the concurrency to 3 won’t\
        \ kill any of the active runs. However, from\n  then on, new runs are skipped\
        \ unless there are fewer than 3 active runs. This value cannot exceed\n  1000.\
        \ Setting this value to `0` causes all new runs to be skipped.\n:param name:\
        \ str (optional)\n  An optional name for the job. The maximum length is 4096\
        \ bytes in UTF-8 encoding.\n:param notification_settings: :class:`JobNotificationSettings`\
        \ (optional)\n  Optional notification settings that are used when sending\
        \ notifications to each of the\n  `email_notifications` and `webhook_notifications`\
        \ for this job.\n:param parameters: List[:class:`JobParameterDefinition`]\
        \ (optional)\n  Job-level parameter definitions\n:param performance_target:\
        \ :class:`PerformanceTarget` (optional)\n  The performance mode on a serverless\
        \ job. This field determines the level of compute performance or\n  cost-efficiency\
        \ for the run.\n\n  * `STANDARD`: Enables cost-efficient execution of serverless\
        \ workloads. * `PERFORMANCE_OPTIMIZED`:\n  Prioritizes fast startup and execution\
        \ times through rapid scaling and optimized cluster\n  performance.\n:param\
        \ queue: :class:`QueueSettings` (optional)\n  The queue settings of the job.\n\
        :param run_as: :class:`JobRunAs` (optional)\n  The user or service principal\
        \ that the job runs as, if specified in the request. This field\n  indicates\
        \ the explicit configuration of `run_as` for the job. To find the value in\
        \ all cases,\n  explicit or implicit, use `run_as_user_name`.\n:param schedule:\
        \ :class:`CronSchedule` (optional)\n  An optional periodic schedule for this\
        \ job. The default behavior is that the job only runs when\n  triggered by\
        \ clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.\n\
        :param tags: Dict[str,str] (optional)\n  A map of tags associated with the\
        \ job. These are forwarded to the cluster as cluster tags for jobs\n  clusters,\
        \ and are subject to the same limitations as cluster tags. A maximum of 25\
        \ tags can be added\n  to the job.\n:param tasks: List[:class:`Task`] (optional)\n\
        \  A list of task specifications to be executed by this job. It supports up\
        \ to 1000 elements in write\n  endpoints (:method:jobs/create, :method:jobs/reset,\
        \ :method:jobs/update, :method:jobs/submit). Read\n  endpoints return only\
        \ 100 tasks. If more than 100 tasks are available, you can paginate through\
        \ them\n  using :method:jobs/get. Use the `next_page_token` field at the object\
        \ root to determine if more\n  results are available.\n:param timeout_seconds:\
        \ int (optional)\n  An optional timeout applied to each run of this job. A\
        \ value of `0` means no timeout.\n:param trigger: :class:`TriggerSettings`\
        \ (optional)\n  A configuration to trigger a run when certain conditions are\
        \ met. The default behavior is that the\n  job runs only when triggered by\
        \ clicking “Run Now” in the Jobs UI or sending an API request to\n  `runNow`.\n\
        :param usage_policy_id: str (optional)\n  The id of the user specified usage\
        \ policy to use for this job. If not specified, a default usage\n  policy\
        \ may be applied when creating or modifying the job. See `effective_usage_policy_id`\
        \ for the\n  usage policy used by this workload.\n:param webhook_notifications:\
        \ :class:`WebhookNotifications` (optional)\n  A collection of system notification\
        \ IDs to notify when runs of this job begin or complete.\n\n:returns: :class:`CreateResponse`"
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobAccessControlRequest'
                  description: List[:class:`JobAccessControlRequest`] (optional) List
                    of permissions to set on the job.
                budget_policy_id:
                  type: string
                  description: str (optional) The id of the user specified budget
                    policy to use for this job. If not specified, a default budget
                    policy may be applied when creating or modifying the job. See
                    `effective_budget_policy_id` for the budget policy used by this
                    workload.
                continuous:
                  $ref: '#/components/schemas/Continuous'
                deployment:
                  $ref: '#/components/schemas/JobDeployment'
                description:
                  type: string
                  description: str (optional) An optional description for the job.
                    The maximum length is 27700 characters in UTF-8 encoding.
                edit_mode:
                  type: string
                  enum:
                  - EDITABLE
                  - UI_LOCKED
                  description: ':class:`JobEditMode` (optional) Edit mode of the job.
                    * `UI_LOCKED`: The job is in a locked UI state and cannot be modified.
                    * `EDITABLE`: The job is in an editable state and can be modified.'
                email_notifications:
                  $ref: '#/components/schemas/JobEmailNotifications'
                environments:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobEnvironment'
                  description: List[:class:`JobEnvironment`] (optional) A list of
                    task execution environment specifications that can be referenced
                    by serverless tasks of this job. An environment is required to
                    be present for serverless tasks. For serverless notebook tasks,
                    the environment is accessible in the notebook environment panel.
                    For other serverless tasks, the task environment is required to
                    be specified using environment_key in the task settings.
                format:
                  type: string
                  enum:
                  - MULTI_TASK
                  - SINGLE_TASK
                  description: :class:`Format` (optional) Used to tell what is the
                    format of the job. This field is ignored in Create/Update/Reset
                    calls. When using the Jobs API 2.1 this value is always set to
                    `"MULTI_TASK"`.
                git_source:
                  $ref: '#/components/schemas/GitSource'
                health:
                  $ref: '#/components/schemas/JobsHealthRules'
                job_clusters:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobCluster'
                  description: List[:class:`JobCluster`] (optional) A list of job
                    cluster specifications that can be shared and reused by tasks
                    of this job. Libraries cannot be declared in a shared job cluster.
                    You must declare dependent libraries in task settings.
                max_concurrent_runs:
                  type: integer
                  description: int (optional) An optional maximum allowed number of
                    concurrent runs of the job. Set this value if you want to be able
                    to execute multiple runs of the same job concurrently. This is
                    useful for example if you trigger your job on a frequent schedule
                    and want to allow consecutive runs to overlap with each other,
                    or if you want to trigger multiple runs which differ by their
                    input parameters. This setting affects only new runs. For example,
                    suppose the job’s concurrency is 4 and there are 4 concurrent
                    active runs. Then setting the concurrency to 3 won’t kill any
                    of the active runs. However, from then on, new runs are skipped
                    unless there are fewer than 3 active runs. This value cannot exceed
                    1000. Setting this value to `0` causes all new runs to be skipped.
                name:
                  type: string
                  description: str (optional) An optional name for the job. The maximum
                    length is 4096 bytes in UTF-8 encoding.
                notification_settings:
                  $ref: '#/components/schemas/JobNotificationSettings'
                parameters:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobParameterDefinition'
                  description: List[:class:`JobParameterDefinition`] (optional) Job-level
                    parameter definitions
                performance_target:
                  type: string
                  enum:
                  - PERFORMANCE_OPTIMIZED
                  - STANDARD
                  description: ':class:`PerformanceTarget` (optional) The performance
                    mode on a serverless job. This field determines the level of compute
                    performance or cost-efficiency for the run. * `STANDARD`: Enables
                    cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:
                    Prioritizes fast startup and execution times through rapid scaling
                    and optimized cluster performance.'
                queue:
                  $ref: '#/components/schemas/QueueSettings'
                run_as:
                  $ref: '#/components/schemas/JobRunAs'
                schedule:
                  $ref: '#/components/schemas/CronSchedule'
                tags:
                  type: object
                  additionalProperties: true
                  description: Dict[str,str] (optional) A map of tags associated with
                    the job. These are forwarded to the cluster as cluster tags for
                    jobs clusters, and are subject to the same limitations as cluster
                    tags. A maximum of 25 tags can be added to the job.
                tasks:
                  type: array
                  items:
                    $ref: '#/components/schemas/Task'
                  description: List[:class:`Task`] (optional) A list of task specifications
                    to be executed by this job. It supports up to 1000 elements in
                    write endpoints (:method:jobs/create, :method:jobs/reset, :method:jobs/update,
                    :method:jobs/submit). Read endpoints return only 100 tasks. If
                    more than 100 tasks are available, you can paginate through them
                    using :method:jobs/get. Use the `next_page_token` field at the
                    object root to determine if more results are available.
                timeout_seconds:
                  type: integer
                  description: int (optional) An optional timeout applied to each
                    run of this job. A value of `0` means no timeout.
                trigger:
                  $ref: '#/components/schemas/TriggerSettings'
                usage_policy_id:
                  type: string
                  description: str (optional) The id of the user specified usage policy
                    to use for this job. If not specified, a default usage policy
                    may be applied when creating or modifying the job. See `effective_usage_policy_id`
                    for the usage policy used by this workload.
                webhook_notifications:
                  $ref: '#/components/schemas/WebhookNotifications'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/delete:
    post:
      operationId: delete
      summary: Deletes a job.
      description: "Deletes a job.\n\n:param job_id: int\n  The canonical identifier\
        \ of the job to delete. This field is required."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: int The canonical identifier of the job to delete.
                    This field is required.
      responses:
        '200':
          description: Success
  /api/2.2/jobs/runs/delete:
    post:
      operationId: delete_run
      summary: Deletes a non-active run. Returns an error if the run is active.
      description: "Deletes a non-active run. Returns an error if the run is active.\n\
        \n:param run_id: int\n  ID of the run to delete."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                run_id:
                  type: integer
                  description: int ID of the run to delete.
      responses:
        '200':
          description: Success
  /api/2.2/jobs/runs/export:
    get:
      operationId: export_run
      summary: Export and retrieve the job run task.
      description: "Export and retrieve the job run task.\n\n:param run_id: int\n\
        \  The canonical identifier for the run. This field is required.\n:param views_to_export:\
        \ :class:`ViewsToExport` (optional)\n  Which views to export (CODE, DASHBOARDS,\
        \ or ALL). Defaults to CODE.\n\n:returns: :class:`ExportRunOutput`"
      tags:
      - jobs
      parameters:
      - name: run_id
        description: int The canonical identifier for the run. This field is required.
        required: true
        schema:
          type: integer
        in: query
      - name: views_to_export
        description: :class:`ViewsToExport` (optional) Which views to export (CODE,
          DASHBOARDS, or ALL). Defaults to CODE.
        required: false
        schema:
          type: string
          enum:
          - ALL
          - CODE
          - DASHBOARDS
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExportRunOutput'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/get:
    get:
      operationId: get
      summary: Retrieves the details for a single job.
      description: "Retrieves the details for a single job.\n\nLarge arrays in the\
        \ results will be paginated when they exceed 100 elements. A request for a\
        \ single\njob will return all properties for that job, and the first 100 elements\
        \ of array properties (`tasks`,\n`job_clusters`, `environments` and `parameters`).\
        \ Use the `next_page_token` field to check for more\nresults and pass its\
        \ value as the `page_token` in subsequent requests. If any array properties\
        \ have\nmore than 100 elements, additional results will be returned on subsequent\
        \ requests. Arrays without\nadditional results will be empty on later pages.\n\
        \n:param job_id: int\n  The canonical identifier of the job to retrieve information\
        \ about. This field is required.\n:param page_token: str (optional)\n  Use\
        \ `next_page_token` returned from the previous GetJob response to request\
        \ the next page of the\n  job's array properties.\n\n:returns: :class:`Job`"
      tags:
      - jobs
      parameters:
      - name: job_id
        description: int The canonical identifier of the job to retrieve information
          about. This field is required.
        required: true
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) Use `next_page_token` returned from the previous
          GetJob response to request the next page of the job's array properties.
        required: false
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Job'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/permissions/jobs/{job_id}/permissionLevels:
    get:
      operationId: get_permission_levels
      summary: Gets the permission levels that a user can have on an object.
      description: "Gets the permission levels that a user can have on an object.\n\
        \n:param job_id: str\n  The job for which to get or manage permissions.\n\n\
        :returns: :class:`GetJobPermissionLevelsResponse`"
      tags:
      - jobs
      parameters:
      - name: job_id
        description: str The job for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetJobPermissionLevelsResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/permissions/jobs/{job_id}:
    get:
      operationId: get_permissions
      summary: Gets the permissions of a job. Jobs can inherit permissions from their
        root object.
      description: "Gets the permissions of a job. Jobs can inherit permissions from\
        \ their root object.\n\n:param job_id: str\n  The job for which to get or\
        \ manage permissions.\n\n:returns: :class:`JobPermissions`"
      tags:
      - jobs
      parameters:
      - name: job_id
        description: str The job for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    put:
      operationId: set_permissions
      summary: 'Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from their
        root object.'
      description: "Sets permissions on an object, replacing existing permissions\
        \ if they exist. Deletes all direct\npermissions if none are specified. Objects\
        \ can inherit permissions from their root object.\n\n:param job_id: str\n\
        \  The job for which to get or manage permissions.\n:param access_control_list:\
        \ List[:class:`JobAccessControlRequest`] (optional)\n\n:returns: :class:`JobPermissions`"
      tags:
      - jobs
      parameters:
      - name: job_id
        description: str The job for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobAccessControlRequest'
                  description: List[:class:`JobAccessControlRequest`] (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
    patch:
      operationId: update_permissions
      summary: Updates the permissions on a job. Jobs can inherit permissions from
        their root object.
      description: "Updates the permissions on a job. Jobs can inherit permissions\
        \ from their root object.\n\n:param job_id: str\n  The job for which to get\
        \ or manage permissions.\n:param access_control_list: List[:class:`JobAccessControlRequest`]\
        \ (optional)\n\n:returns: :class:`JobPermissions`"
      tags:
      - jobs
      parameters:
      - name: job_id
        description: str The job for which to get or manage permissions.
        required: true
        schema:
          type: string
        in: path
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobAccessControlRequest'
                  description: List[:class:`JobAccessControlRequest`] (optional)
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobPermissions'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/runs/get:
    get:
      operationId: get_run
      summary: Retrieves the metadata of a run.
      description: "Retrieves the metadata of a run.\n\nLarge arrays in the results\
        \ will be paginated when they exceed 100 elements. A request for a single\n\
        run will return all properties for that run, and the first 100 elements of\
        \ array properties (`tasks`,\n`job_clusters`, `job_parameters` and `repair_history`).\
        \ Use the next_page_token field to check for\nmore results and pass its value\
        \ as the page_token in subsequent requests. If any array properties have\n\
        more than 100 elements, additional results will be returned on subsequent\
        \ requests. Arrays without\nadditional results will be empty on later pages.\n\
        \n:param run_id: int\n  The canonical identifier of the run for which to retrieve\
        \ the metadata. This field is required.\n:param include_history: bool (optional)\n\
        \  Whether to include the repair history in the response.\n:param include_resolved_values:\
        \ bool (optional)\n  Whether to include resolved parameter values in the response.\n\
        :param page_token: str (optional)\n  Use `next_page_token` returned from the\
        \ previous GetRun response to request the next page of the\n  run's array\
        \ properties.\n\n:returns: :class:`Run`"
      tags:
      - jobs
      parameters:
      - name: run_id
        description: int The canonical identifier of the run for which to retrieve
          the metadata. This field is required.
        required: true
        schema:
          type: integer
        in: query
      - name: include_history
        description: bool (optional) Whether to include the repair history in the
          response.
        required: false
        schema:
          type: boolean
        in: query
      - name: include_resolved_values
        description: bool (optional) Whether to include resolved parameter values
          in the response.
        required: false
        schema:
          type: boolean
        in: query
      - name: page_token
        description: str (optional) Use `next_page_token` returned from the previous
          GetRun response to request the next page of the run's array properties.
        required: false
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Run'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/runs/get-output:
    get:
      operationId: get_run_output
      summary: 'Retrieve the output and metadata of a single task run. When a notebook
        task returns a value through

        the `dbutils.notebook.exit()` call, you can use this endpoint to retrieve
        that value. Databricks

        restricts this API to returning the first 5 MB of the output. To return a
        larger result, you can store

        job results in a cloud storage service.'
      description: "Retrieve the output and metadata of a single task run. When a\
        \ notebook task returns a value through\nthe `dbutils.notebook.exit()` call,\
        \ you can use this endpoint to retrieve that value. Databricks\nrestricts\
        \ this API to returning the first 5 MB of the output. To return a larger result,\
        \ you can store\njob results in a cloud storage service.\n\nThis endpoint\
        \ validates that the __run_id__ parameter is valid and returns an HTTP status\
        \ code 400 if\nthe __run_id__ parameter is invalid. Runs are automatically\
        \ removed after 60 days. If you to want to\nreference them beyond 60 days,\
        \ you must save old run results before they expire.\n\n:param run_id: int\n\
        \  The canonical identifier for the run.\n\n:returns: :class:`RunOutput`"
      tags:
      - jobs
      parameters:
      - name: run_id
        description: int The canonical identifier for the run.
        required: true
        schema:
          type: integer
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunOutput'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/list:
    get:
      operationId: list
      summary: Retrieves a list of jobs.
      description: "Retrieves a list of jobs.\n\n:param expand_tasks: bool (optional)\n\
        \  Whether to include task and cluster details in the response. Note that\
        \ only the first 100 elements\n  will be shown. Use :method:jobs/get to paginate\
        \ through all tasks and clusters.\n:param limit: int (optional)\n  The number\
        \ of jobs to return. This value must be greater than 0 and less or equal to\
        \ 100. The\n  default value is 20.\n:param name: str (optional)\n  A filter\
        \ on the list based on the exact (case insensitive) job name.\n:param offset:\
        \ int (optional)\n  The offset of the first job to return, relative to the\
        \ most recently created job. Deprecated since\n  June 2023. Use `page_token`\
        \ to iterate through the pages instead.\n:param page_token: str (optional)\n\
        \  Use `next_page_token` or `prev_page_token` returned from the previous request\
        \ to list the next or\n  previous page of jobs respectively.\n\n:returns:\
        \ Iterator over :class:`BaseJob`"
      tags:
      - jobs
      parameters:
      - name: expand_tasks
        description: bool (optional) Whether to include task and cluster details in
          the response. Note that only the first 100 elements will be shown. Use :method:jobs/get
          to paginate through all tasks and clusters.
        required: false
        schema:
          type: boolean
        in: query
      - name: limit
        description: int (optional) The number of jobs to return. This value must
          be greater than 0 and less or equal to 100. The default value is 20.
        required: false
        schema:
          type: integer
        in: query
      - name: name
        description: str (optional) A filter on the list based on the exact (case
          insensitive) job name.
        required: false
        schema:
          type: string
        in: query
      - name: offset
        description: int (optional) The offset of the first job to return, relative
          to the most recently created job. Deprecated since June 2023. Use `page_token`
          to iterate through the pages instead.
        required: false
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) Use `next_page_token` or `prev_page_token` returned
          from the previous request to list the next or previous page of jobs respectively.
        required: false
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BaseJob'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/runs/list:
    get:
      operationId: list_runs
      summary: List runs in descending order by start time.
      description: "List runs in descending order by start time.\n\n:param active_only:\
        \ bool (optional)\n  If active_only is `true`, only active runs are included\
        \ in the results; otherwise, lists both active\n  and completed runs. An active\
        \ run is a run in the `QUEUED`, `PENDING`, `RUNNING`, or `TERMINATING`.\n\
        \  This field cannot be `true` when completed_only is `true`.\n:param completed_only:\
        \ bool (optional)\n  If completed_only is `true`, only completed runs are\
        \ included in the results; otherwise, lists both\n  active and completed runs.\
        \ This field cannot be `true` when active_only is `true`.\n:param expand_tasks:\
        \ bool (optional)\n  Whether to include task and cluster details in the response.\
        \ Note that only the first 100 elements\n  will be shown. Use :method:jobs/getrun\
        \ to paginate through all tasks and clusters.\n:param job_id: int (optional)\n\
        \  The job for which to list runs. If omitted, the Jobs service lists runs\
        \ from all jobs.\n:param limit: int (optional)\n  The number of runs to return.\
        \ This value must be greater than 0 and less than 25. The default value\n\
        \  is 20. If a request specifies a limit of 0, the service instead uses the\
        \ maximum limit.\n:param offset: int (optional)\n  The offset of the first\
        \ run to return, relative to the most recent run. Deprecated since June 2023.\n\
        \  Use `page_token` to iterate through the pages instead.\n:param page_token:\
        \ str (optional)\n  Use `next_page_token` or `prev_page_token` returned from\
        \ the previous request to list the next or\n  previous page of runs respectively.\n\
        :param run_type: :class:`RunType` (optional)\n  The type of runs to return.\
        \ For a description of run types, see :method:jobs/getRun.\n:param start_time_from:\
        \ int (optional)\n  Show runs that started _at or after_ this value. The value\
        \ must be a UTC timestamp in milliseconds.\n  Can be combined with _start_time_to_\
        \ to filter by a time range.\n:param start_time_to: int (optional)\n  Show\
        \ runs that started _at or before_ this value. The value must be a UTC timestamp\
        \ in milliseconds.\n  Can be combined with _start_time_from_ to filter by\
        \ a time range.\n\n:returns: Iterator over :class:`BaseRun`"
      tags:
      - jobs
      parameters:
      - name: active_only
        description: bool (optional) If active_only is `true`, only active runs are
          included in the results; otherwise, lists both active and completed runs.
          An active run is a run in the `QUEUED`, `PENDING`, `RUNNING`, or `TERMINATING`.
          This field cannot be `true` when completed_only is `true`.
        required: false
        schema:
          type: boolean
        in: query
      - name: completed_only
        description: bool (optional) If completed_only is `true`, only completed runs
          are included in the results; otherwise, lists both active and completed
          runs. This field cannot be `true` when active_only is `true`.
        required: false
        schema:
          type: boolean
        in: query
      - name: expand_tasks
        description: bool (optional) Whether to include task and cluster details in
          the response. Note that only the first 100 elements will be shown. Use :method:jobs/getrun
          to paginate through all tasks and clusters.
        required: false
        schema:
          type: boolean
        in: query
      - name: job_id
        description: int (optional) The job for which to list runs. If omitted, the
          Jobs service lists runs from all jobs.
        required: false
        schema:
          type: integer
        in: query
      - name: limit
        description: int (optional) The number of runs to return. This value must
          be greater than 0 and less than 25. The default value is 20. If a request
          specifies a limit of 0, the service instead uses the maximum limit.
        required: false
        schema:
          type: integer
        in: query
      - name: offset
        description: int (optional) The offset of the first run to return, relative
          to the most recent run. Deprecated since June 2023. Use `page_token` to
          iterate through the pages instead.
        required: false
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) Use `next_page_token` or `prev_page_token` returned
          from the previous request to list the next or previous page of runs respectively.
        required: false
        schema:
          type: string
        in: query
      - name: run_type
        description: :class:`RunType` (optional) The type of runs to return. For a
          description of run types, see :method:jobs/getRun.
        required: false
        schema:
          type: string
          enum:
          - JOB_RUN
          - SUBMIT_RUN
          - WORKFLOW_RUN
        in: query
      - name: start_time_from
        description: int (optional) Show runs that started _at or after_ this value.
          The value must be a UTC timestamp in milliseconds. Can be combined with
          _start_time_to_ to filter by a time range.
        required: false
        schema:
          type: integer
        in: query
      - name: start_time_to
        description: int (optional) Show runs that started _at or before_ this value.
          The value must be a UTC timestamp in milliseconds. Can be combined with
          _start_time_from_ to filter by a time range.
        required: false
        schema:
          type: integer
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BaseRun'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/runs/repair:
    post:
      operationId: repair_run
      summary: 'Re-run one or more tasks. Tasks are re-run as part of the original
        job run. They use the current job

        and task settings, and can be viewed in the history for the original job run.'
      description: "Re-run one or more tasks. Tasks are re-run as part of the original\
        \ job run. They use the current job\nand task settings, and can be viewed\
        \ in the history for the original job run.\n\n:param run_id: int\n  The job\
        \ run ID of the run to repair. The run must not be in progress.\n:param dbt_commands:\
        \ List[str] (optional)\n  An array of commands to execute for jobs with the\
        \ dbt task, for example `\"dbt_commands\": [\"dbt\n  deps\", \"dbt seed\"\
        , \"dbt deps\", \"dbt seed\", \"dbt run\"]`\n\n  ⚠ **Deprecation note** Use\
        \ [job parameters] to pass information down to tasks.\n\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param jar_params: List[str] (optional)\n  A list of parameters for jobs with\
        \ Spark JAR tasks, for example `\"jar_params\": [\"john doe\", \"35\"]`.\n\
        \  The parameters are used to invoke the main function of the main class specified\
        \ in the Spark JAR\n  task. If not specified upon `run-now`, it defaults to\
        \ an empty list. jar_params cannot be specified\n  in conjunction with notebook_params.\
        \ The JSON representation of this field (for example\n  `{\"jar_params\":[\"\
        john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\n  ⚠ **Deprecation note**\
        \ Use [job parameters] to pass information down to tasks.\n\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param job_parameters: Dict[str,str] (optional)\n  Job-level parameters used\
        \ in the run. for example `\"param\": \"overriding_val\"`\n:param latest_repair_id:\
        \ int (optional)\n  The ID of the latest repair. This parameter is not required\
        \ when repairing a run for the first time,\n  but must be provided on subsequent\
        \ requests to repair the same run.\n:param notebook_params: Dict[str,str]\
        \ (optional)\n  A map from keys to values for jobs with notebook task, for\
        \ example `\"notebook_params\": {\"name\":\n  \"john doe\", \"age\": \"35\"\
        }`. The map is passed to the notebook and is accessible through the\n  [dbutils.widgets.get]\
        \ function.\n\n  If not specified upon `run-now`, the triggered run uses the\
        \ job’s base parameters.\n\n  notebook_params cannot be specified in conjunction\
        \ with jar_params.\n\n  ⚠ **Deprecation note** Use [job parameters] to pass\
        \ information down to tasks.\n\n  The JSON representation of this field (for\
        \ example `{\"notebook_params\":{\"name\":\"john\n  doe\",\"age\":\"35\"}}`)\
        \ cannot exceed 10,000 bytes.\n\n  [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html\n\
        \  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param performance_target: :class:`PerformanceTarget` (optional)\n  The performance\
        \ mode on a serverless job. The performance target determines the level of\
        \ compute\n  performance or cost-efficiency for the run. This field overrides\
        \ the performance target defined on\n  the job level.\n\n  * `STANDARD`: Enables\
        \ cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:\n\
        \  Prioritizes fast startup and execution times through rapid scaling and\
        \ optimized cluster\n  performance.\n:param pipeline_params: :class:`PipelineParams`\
        \ (optional)\n  Controls whether the pipeline should perform a full refresh\n\
        :param python_named_params: Dict[str,str] (optional)\n:param python_params:\
        \ List[str] (optional)\n  A list of parameters for jobs with Python tasks,\
        \ for example `\"python_params\": [\"john doe\", \"35\"]`.\n  The parameters\
        \ are passed to Python file as command-line parameters. If specified upon\
        \ `run-now`, it\n  would overwrite the parameters specified in job setting.\
        \ The JSON representation of this field (for\n  example `{\"python_params\"\
        :[\"john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\n  ⚠ **Deprecation\
        \ note** Use [job parameters] to pass information down to tasks.\n\n  Important\n\
        \n  These parameters accept only Latin characters (ASCII character set). Using\
        \ non-ASCII characters\n  returns an error. Examples of invalid, non-ASCII\
        \ characters are Chinese, Japanese kanjis, and\n  emojis.\n\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param rerun_all_failed_tasks: bool (optional)\n  If true, repair all failed\
        \ tasks. Only one of `rerun_tasks` or `rerun_all_failed_tasks` can be used.\n\
        :param rerun_dependent_tasks: bool (optional)\n  If true, repair all tasks\
        \ that depend on the tasks in `rerun_tasks`, even if they were previously\n\
        \  successful. Can be also used in combination with `rerun_all_failed_tasks`.\n\
        :param rerun_tasks: List[str] (optional)\n  The task keys of the task runs\
        \ to repair.\n:param spark_submit_params: List[str] (optional)\n  A list of\
        \ parameters for jobs with spark submit task, for example `\"spark_submit_params\"\
        :\n  [\"--class\", \"org.apache.spark.examples.SparkPi\"]`. The parameters\
        \ are passed to spark-submit script\n  as command-line parameters. If specified\
        \ upon `run-now`, it would overwrite the parameters specified\n  in job setting.\
        \ The JSON representation of this field (for example `{\"python_params\":[\"\
        john\n  doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\n  ⚠ **Deprecation\
        \ note** Use [job parameters] to pass information down to tasks.\n\n  Important\n\
        \n  These parameters accept only Latin characters (ASCII character set). Using\
        \ non-ASCII characters\n  returns an error. Examples of invalid, non-ASCII\
        \ characters are Chinese, Japanese kanjis, and\n  emojis.\n\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param sql_params: Dict[str,str] (optional)\n  A map from keys to values for\
        \ jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\"\
        ,\n  \"age\": \"35\"}`. The SQL alert task does not support custom parameters.\n\
        \n  ⚠ **Deprecation note** Use [job parameters] to pass information down to\
        \ tasks.\n\n  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        \n:returns:\n  Long-running operation waiter for :class:`Run`.\n  See :method:wait_get_run_job_terminated_or_skipped\
        \ for more details."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                run_id:
                  type: integer
                  description: int The job run ID of the run to repair. The run must
                    not be in progress.
                dbt_commands:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) An array of commands to execute
                    for jobs with the dbt task, for example `"dbt_commands": ["dbt
                    deps", "dbt seed", "dbt deps", "dbt seed", "dbt run"]` ⚠ **Deprecation
                    note** Use [job parameters] to pass information down to tasks.
                    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                jar_params:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) A list of parameters for jobs
                    with Spark JAR tasks, for example `"jar_params": ["john doe",
                    "35"]`. The parameters are used to invoke the main function of
                    the main class specified in the Spark JAR task. If not specified
                    upon `run-now`, it defaults to an empty list. jar_params cannot
                    be specified in conjunction with notebook_params. The JSON representation
                    of this field (for example `{"jar_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job parameters]
                    to pass information down to tasks. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                job_parameters:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) Job-level parameters used
                    in the run. for example `"param": "overriding_val"`'
                latest_repair_id:
                  type: integer
                  description: int (optional) The ID of the latest repair. This parameter
                    is not required when repairing a run for the first time, but must
                    be provided on subsequent requests to repair the same run.
                notebook_params:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) A map from keys to values
                    for jobs with notebook task, for example `"notebook_params": {"name":
                    "john doe", "age": "35"}`. The map is passed to the notebook and
                    is accessible through the [dbutils.widgets.get] function. If not
                    specified upon `run-now`, the triggered run uses the job’s base
                    parameters. notebook_params cannot be specified in conjunction
                    with jar_params. ⚠ **Deprecation note** Use [job parameters] to
                    pass information down to tasks. The JSON representation of this
                    field (for example `{"notebook_params":{"name":"john doe","age":"35"}}`)
                    cannot exceed 10,000 bytes. [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
                    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                performance_target:
                  type: string
                  enum:
                  - PERFORMANCE_OPTIMIZED
                  - STANDARD
                  description: ':class:`PerformanceTarget` (optional) The performance
                    mode on a serverless job. The performance target determines the
                    level of compute performance or cost-efficiency for the run. This
                    field overrides the performance target defined on the job level.
                    * `STANDARD`: Enables cost-efficient execution of serverless workloads.
                    * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
                    times through rapid scaling and optimized cluster performance.'
                pipeline_params:
                  $ref: '#/components/schemas/PipelineParams'
                python_named_params:
                  type: object
                  additionalProperties: true
                  description: Dict[str,str] (optional)
                python_params:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) A list of parameters for jobs
                    with Python tasks, for example `"python_params": ["john doe",
                    "35"]`. The parameters are passed to Python file as command-line
                    parameters. If specified upon `run-now`, it would overwrite the
                    parameters specified in job setting. The JSON representation of
                    this field (for example `{"python_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job parameters]
                    to pass information down to tasks. Important These parameters
                    accept only Latin characters (ASCII character set). Using non-ASCII
                    characters returns an error. Examples of invalid, non-ASCII characters
                    are Chinese, Japanese kanjis, and emojis. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                rerun_all_failed_tasks:
                  type: boolean
                  description: bool (optional) If true, repair all failed tasks. Only
                    one of `rerun_tasks` or `rerun_all_failed_tasks` can be used.
                rerun_dependent_tasks:
                  type: boolean
                  description: bool (optional) If true, repair all tasks that depend
                    on the tasks in `rerun_tasks`, even if they were previously successful.
                    Can be also used in combination with `rerun_all_failed_tasks`.
                rerun_tasks:
                  type: array
                  items:
                    type: string
                  description: List[str] (optional) The task keys of the task runs
                    to repair.
                spark_submit_params:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) A list of parameters for jobs
                    with spark submit task, for example `"spark_submit_params": ["--class",
                    "org.apache.spark.examples.SparkPi"]`. The parameters are passed
                    to spark-submit script as command-line parameters. If specified
                    upon `run-now`, it would overwrite the parameters specified in
                    job setting. The JSON representation of this field (for example
                    `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.
                    ⚠ **Deprecation note** Use [job parameters] to pass information
                    down to tasks. Important These parameters accept only Latin characters
                    (ASCII character set). Using non-ASCII characters returns an error.
                    Examples of invalid, non-ASCII characters are Chinese, Japanese
                    kanjis, and emojis. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                sql_params:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) A map from keys to values
                    for jobs with SQL task, for example `"sql_params": {"name": "john
                    doe", "age": "35"}`. The SQL alert task does not support custom
                    parameters. ⚠ **Deprecation note** Use [job parameters] to pass
                    information down to tasks. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Run'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/reset:
    post:
      operationId: reset
      summary: 'Overwrite all settings for the given job. Use the [_Update_ endpoint](:method:jobs/update)
        to update

        job settings partially.'
      description: "Overwrite all settings for the given job. Use the [_Update_ endpoint](:method:jobs/update)\
        \ to update\njob settings partially.\n\n:param job_id: int\n  The canonical\
        \ identifier of the job to reset. This field is required.\n:param new_settings:\
        \ :class:`JobSettings`\n  The new settings of the job. These settings completely\
        \ replace the old settings.\n\n  Changes to the field `JobBaseSettings.timeout_seconds`\
        \ are applied to active runs. Changes to other\n  fields are applied to future\
        \ runs only."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: int The canonical identifier of the job to reset. This
                    field is required.
                new_settings:
                  $ref: '#/components/schemas/JobSettings'
      responses:
        '200':
          description: Success
  /api/2.2/jobs/run-now:
    post:
      operationId: run_now
      summary: Run a job and return the `run_id` of the triggered run.
      description: "Run a job and return the `run_id` of the triggered run.\n\n:param\
        \ job_id: int\n  The ID of the job to be executed\n:param dbt_commands: List[str]\
        \ (optional)\n  An array of commands to execute for jobs with the dbt task,\
        \ for example `\"dbt_commands\": [\"dbt\n  deps\", \"dbt seed\", \"dbt deps\"\
        , \"dbt seed\", \"dbt run\"]`\n\n  ⚠ **Deprecation note** Use [job parameters]\
        \ to pass information down to tasks.\n\n  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param idempotency_token: str (optional)\n  An optional token to guarantee\
        \ the idempotency of job run requests. If a run with the provided token\n\
        \  already exists, the request does not create a new run but returns the ID\
        \ of the existing run\n  instead. If a run with the provided token is deleted,\
        \ an error is returned.\n\n  If you specify the idempotency token, upon failure\
        \ you can retry until the request succeeds.\n  Databricks guarantees that\
        \ exactly one run is launched with that idempotency token.\n\n  This token\
        \ must have at most 64 characters.\n\n  For more information, see [How to\
        \ ensure idempotency for jobs].\n\n  [How to ensure idempotency for jobs]:\
        \ https://kb.databricks.com/jobs/jobs-idempotency.html\n:param jar_params:\
        \ List[str] (optional)\n  A list of parameters for jobs with Spark JAR tasks,\
        \ for example `\"jar_params\": [\"john doe\", \"35\"]`.\n  The parameters\
        \ are used to invoke the main function of the main class specified in the\
        \ Spark JAR\n  task. If not specified upon `run-now`, it defaults to an empty\
        \ list. jar_params cannot be specified\n  in conjunction with notebook_params.\
        \ The JSON representation of this field (for example\n  `{\"jar_params\":[\"\
        john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\n  ⚠ **Deprecation note**\
        \ Use [job parameters] to pass information down to tasks.\n\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param job_parameters: Dict[str,str] (optional)\n  Job-level parameters used\
        \ in the run. for example `\"param\": \"overriding_val\"`\n:param notebook_params:\
        \ Dict[str,str] (optional)\n  A map from keys to values for jobs with notebook\
        \ task, for example `\"notebook_params\": {\"name\":\n  \"john doe\", \"age\"\
        : \"35\"}`. The map is passed to the notebook and is accessible through the\n\
        \  [dbutils.widgets.get] function.\n\n  If not specified upon `run-now`, the\
        \ triggered run uses the job’s base parameters.\n\n  notebook_params cannot\
        \ be specified in conjunction with jar_params.\n\n  ⚠ **Deprecation note**\
        \ Use [job parameters] to pass information down to tasks.\n\n  The JSON representation\
        \ of this field (for example `{\"notebook_params\":{\"name\":\"john\n  doe\"\
        ,\"age\":\"35\"}}`) cannot exceed 10,000 bytes.\n\n  [dbutils.widgets.get]:\
        \ https://docs.databricks.com/dev-tools/databricks-utils.html\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param only: List[str] (optional)\n  A list of task keys to run inside of\
        \ the job. If this field is not provided, all tasks in the job\n  will be\
        \ run.\n:param performance_target: :class:`PerformanceTarget` (optional)\n\
        \  The performance mode on a serverless job. The performance target determines\
        \ the level of compute\n  performance or cost-efficiency for the run. This\
        \ field overrides the performance target defined on\n  the job level.\n\n\
        \  * `STANDARD`: Enables cost-efficient execution of serverless workloads.\
        \ * `PERFORMANCE_OPTIMIZED`:\n  Prioritizes fast startup and execution times\
        \ through rapid scaling and optimized cluster\n  performance.\n:param pipeline_params:\
        \ :class:`PipelineParams` (optional)\n  Controls whether the pipeline should\
        \ perform a full refresh\n:param python_named_params: Dict[str,str] (optional)\n\
        :param python_params: List[str] (optional)\n  A list of parameters for jobs\
        \ with Python tasks, for example `\"python_params\": [\"john doe\", \"35\"\
        ]`.\n  The parameters are passed to Python file as command-line parameters.\
        \ If specified upon `run-now`, it\n  would overwrite the parameters specified\
        \ in job setting. The JSON representation of this field (for\n  example `{\"\
        python_params\":[\"john doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\n \
        \ ⚠ **Deprecation note** Use [job parameters] to pass information down to\
        \ tasks.\n\n  Important\n\n  These parameters accept only Latin characters\
        \ (ASCII character set). Using non-ASCII characters\n  returns an error. Examples\
        \ of invalid, non-ASCII characters are Chinese, Japanese kanjis, and\n  emojis.\n\
        \n  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param queue: :class:`QueueSettings` (optional)\n  The queue settings of the\
        \ run.\n:param spark_submit_params: List[str] (optional)\n  A list of parameters\
        \ for jobs with spark submit task, for example `\"spark_submit_params\":\n\
        \  [\"--class\", \"org.apache.spark.examples.SparkPi\"]`. The parameters are\
        \ passed to spark-submit script\n  as command-line parameters. If specified\
        \ upon `run-now`, it would overwrite the parameters specified\n  in job setting.\
        \ The JSON representation of this field (for example `{\"python_params\":[\"\
        john\n  doe\",\"35\"]}`) cannot exceed 10,000 bytes.\n\n  ⚠ **Deprecation\
        \ note** Use [job parameters] to pass information down to tasks.\n\n  Important\n\
        \n  These parameters accept only Latin characters (ASCII character set). Using\
        \ non-ASCII characters\n  returns an error. Examples of invalid, non-ASCII\
        \ characters are Chinese, Japanese kanjis, and\n  emojis.\n\n  [job parameters]:\
        \ https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        :param sql_params: Dict[str,str] (optional)\n  A map from keys to values for\
        \ jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\"\
        ,\n  \"age\": \"35\"}`. The SQL alert task does not support custom parameters.\n\
        \n  ⚠ **Deprecation note** Use [job parameters] to pass information down to\
        \ tasks.\n\n  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown\n\
        \n:returns:\n  Long-running operation waiter for :class:`Run`.\n  See :method:wait_get_run_job_terminated_or_skipped\
        \ for more details."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: int The ID of the job to be executed
                dbt_commands:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) An array of commands to execute
                    for jobs with the dbt task, for example `"dbt_commands": ["dbt
                    deps", "dbt seed", "dbt deps", "dbt seed", "dbt run"]` ⚠ **Deprecation
                    note** Use [job parameters] to pass information down to tasks.
                    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                idempotency_token:
                  type: string
                  description: 'str (optional) An optional token to guarantee the
                    idempotency of job run requests. If a run with the provided token
                    already exists, the request does not create a new run but returns
                    the ID of the existing run instead. If a run with the provided
                    token is deleted, an error is returned. If you specify the idempotency
                    token, upon failure you can retry until the request succeeds.
                    Databricks guarantees that exactly one run is launched with that
                    idempotency token. This token must have at most 64 characters.
                    For more information, see [How to ensure idempotency for jobs].
                    [How to ensure idempotency for jobs]: https://kb.databricks.com/jobs/jobs-idempotency.html'
                jar_params:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) A list of parameters for jobs
                    with Spark JAR tasks, for example `"jar_params": ["john doe",
                    "35"]`. The parameters are used to invoke the main function of
                    the main class specified in the Spark JAR task. If not specified
                    upon `run-now`, it defaults to an empty list. jar_params cannot
                    be specified in conjunction with notebook_params. The JSON representation
                    of this field (for example `{"jar_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job parameters]
                    to pass information down to tasks. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                job_parameters:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) Job-level parameters used
                    in the run. for example `"param": "overriding_val"`'
                notebook_params:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) A map from keys to values
                    for jobs with notebook task, for example `"notebook_params": {"name":
                    "john doe", "age": "35"}`. The map is passed to the notebook and
                    is accessible through the [dbutils.widgets.get] function. If not
                    specified upon `run-now`, the triggered run uses the job’s base
                    parameters. notebook_params cannot be specified in conjunction
                    with jar_params. ⚠ **Deprecation note** Use [job parameters] to
                    pass information down to tasks. The JSON representation of this
                    field (for example `{"notebook_params":{"name":"john doe","age":"35"}}`)
                    cannot exceed 10,000 bytes. [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
                    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                only:
                  type: array
                  items:
                    type: string
                  description: List[str] (optional) A list of task keys to run inside
                    of the job. If this field is not provided, all tasks in the job
                    will be run.
                performance_target:
                  type: string
                  enum:
                  - PERFORMANCE_OPTIMIZED
                  - STANDARD
                  description: ':class:`PerformanceTarget` (optional) The performance
                    mode on a serverless job. The performance target determines the
                    level of compute performance or cost-efficiency for the run. This
                    field overrides the performance target defined on the job level.
                    * `STANDARD`: Enables cost-efficient execution of serverless workloads.
                    * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
                    times through rapid scaling and optimized cluster performance.'
                pipeline_params:
                  $ref: '#/components/schemas/PipelineParams'
                python_named_params:
                  type: object
                  additionalProperties: true
                  description: Dict[str,str] (optional)
                python_params:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) A list of parameters for jobs
                    with Python tasks, for example `"python_params": ["john doe",
                    "35"]`. The parameters are passed to Python file as command-line
                    parameters. If specified upon `run-now`, it would overwrite the
                    parameters specified in job setting. The JSON representation of
                    this field (for example `{"python_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job parameters]
                    to pass information down to tasks. Important These parameters
                    accept only Latin characters (ASCII character set). Using non-ASCII
                    characters returns an error. Examples of invalid, non-ASCII characters
                    are Chinese, Japanese kanjis, and emojis. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                queue:
                  $ref: '#/components/schemas/QueueSettings'
                spark_submit_params:
                  type: array
                  items:
                    type: string
                  description: 'List[str] (optional) A list of parameters for jobs
                    with spark submit task, for example `"spark_submit_params": ["--class",
                    "org.apache.spark.examples.SparkPi"]`. The parameters are passed
                    to spark-submit script as command-line parameters. If specified
                    upon `run-now`, it would overwrite the parameters specified in
                    job setting. The JSON representation of this field (for example
                    `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.
                    ⚠ **Deprecation note** Use [job parameters] to pass information
                    down to tasks. Important These parameters accept only Latin characters
                    (ASCII character set). Using non-ASCII characters returns an error.
                    Examples of invalid, non-ASCII characters are Chinese, Japanese
                    kanjis, and emojis. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
                sql_params:
                  type: object
                  additionalProperties: true
                  description: 'Dict[str,str] (optional) A map from keys to values
                    for jobs with SQL task, for example `"sql_params": {"name": "john
                    doe", "age": "35"}`. The SQL alert task does not support custom
                    parameters. ⚠ **Deprecation note** Use [job parameters] to pass
                    information down to tasks. [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Run'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/runs/submit:
    post:
      operationId: submit
      summary: 'Submit a one-time run. This endpoint allows you to submit a workload
        directly without creating a job.

        Runs submitted using this endpoint don’t display in the UI. Use the `jobs/runs/get`
        API to check the

        run state after the job is submitted.'
      description: "Submit a one-time run. This endpoint allows you to submit a workload\
        \ directly without creating a job.\nRuns submitted using this endpoint don’t\
        \ display in the UI. Use the `jobs/runs/get` API to check the\nrun state after\
        \ the job is submitted.\n\n:param access_control_list: List[:class:`JobAccessControlRequest`]\
        \ (optional)\n  List of permissions to set on the job.\n:param budget_policy_id:\
        \ str (optional)\n  The user specified id of the budget policy to use for\
        \ this one-time run. If not specified, the run\n  will be not be attributed\
        \ to any budget policy.\n:param email_notifications: :class:`JobEmailNotifications`\
        \ (optional)\n  An optional set of email addresses notified when the run begins\
        \ or completes.\n:param environments: List[:class:`JobEnvironment`] (optional)\n\
        \  A list of task execution environment specifications that can be referenced\
        \ by tasks of this run.\n:param git_source: :class:`GitSource` (optional)\n\
        \  An optional specification for a remote Git repository containing the source\
        \ code used by tasks.\n  Version-controlled source code is supported by notebook,\
        \ dbt, Python script, and SQL File tasks.\n\n  If `git_source` is set, these\
        \ tasks retrieve the file from the remote repository by default.\n  However,\
        \ this behavior can be overridden by setting `source` to `WORKSPACE` on the\
        \ task.\n\n  Note: dbt and SQL File tasks support only version-controlled\
        \ sources. If dbt or SQL File tasks are\n  used, `git_source` must be defined\
        \ on the job.\n:param health: :class:`JobsHealthRules` (optional)\n:param\
        \ idempotency_token: str (optional)\n  An optional token that can be used\
        \ to guarantee the idempotency of job run requests. If a run with\n  the provided\
        \ token already exists, the request does not create a new run but returns\
        \ the ID of the\n  existing run instead. If a run with the provided token\
        \ is deleted, an error is returned.\n\n  If you specify the idempotency token,\
        \ upon failure you can retry until the request succeeds.\n  Databricks guarantees\
        \ that exactly one run is launched with that idempotency token.\n\n  This\
        \ token must have at most 64 characters.\n\n  For more information, see [How\
        \ to ensure idempotency for jobs].\n\n  [How to ensure idempotency for jobs]:\
        \ https://kb.databricks.com/jobs/jobs-idempotency.html\n:param notification_settings:\
        \ :class:`JobNotificationSettings` (optional)\n  Optional notification settings\
        \ that are used when sending notifications to each of the\n  `email_notifications`\
        \ and `webhook_notifications` for this run.\n:param queue: :class:`QueueSettings`\
        \ (optional)\n  The queue settings of the one-time run.\n:param run_as: :class:`JobRunAs`\
        \ (optional)\n  Specifies the user or service principal that the job runs\
        \ as. If not specified, the job runs as the\n  user who submits the request.\n\
        :param run_name: str (optional)\n  An optional name for the run. The default\
        \ value is `Untitled`.\n:param tasks: List[:class:`SubmitTask`] (optional)\n\
        :param timeout_seconds: int (optional)\n  An optional timeout applied to each\
        \ run of this job. A value of `0` means no timeout.\n:param usage_policy_id:\
        \ str (optional)\n  The user specified id of the usage policy to use for this\
        \ one-time run. If not specified, a default\n  usage policy may be applied\
        \ when creating or modifying the job.\n:param webhook_notifications: :class:`WebhookNotifications`\
        \ (optional)\n  A collection of system notification IDs to notify when the\
        \ run begins or completes.\n\n:returns:\n  Long-running operation waiter for\
        \ :class:`Run`.\n  See :method:wait_get_run_job_terminated_or_skipped for\
        \ more details."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobAccessControlRequest'
                  description: List[:class:`JobAccessControlRequest`] (optional) List
                    of permissions to set on the job.
                budget_policy_id:
                  type: string
                  description: str (optional) The user specified id of the budget
                    policy to use for this one-time run. If not specified, the run
                    will be not be attributed to any budget policy.
                email_notifications:
                  $ref: '#/components/schemas/JobEmailNotifications'
                environments:
                  type: array
                  items:
                    $ref: '#/components/schemas/JobEnvironment'
                  description: List[:class:`JobEnvironment`] (optional) A list of
                    task execution environment specifications that can be referenced
                    by tasks of this run.
                git_source:
                  $ref: '#/components/schemas/GitSource'
                health:
                  $ref: '#/components/schemas/JobsHealthRules'
                idempotency_token:
                  type: string
                  description: 'str (optional) An optional token that can be used
                    to guarantee the idempotency of job run requests. If a run with
                    the provided token already exists, the request does not create
                    a new run but returns the ID of the existing run instead. If a
                    run with the provided token is deleted, an error is returned.
                    If you specify the idempotency token, upon failure you can retry
                    until the request succeeds. Databricks guarantees that exactly
                    one run is launched with that idempotency token. This token must
                    have at most 64 characters. For more information, see [How to
                    ensure idempotency for jobs]. [How to ensure idempotency for jobs]:
                    https://kb.databricks.com/jobs/jobs-idempotency.html'
                notification_settings:
                  $ref: '#/components/schemas/JobNotificationSettings'
                queue:
                  $ref: '#/components/schemas/QueueSettings'
                run_as:
                  $ref: '#/components/schemas/JobRunAs'
                run_name:
                  type: string
                  description: str (optional) An optional name for the run. The default
                    value is `Untitled`.
                tasks:
                  type: array
                  items:
                    $ref: '#/components/schemas/SubmitTask'
                  description: List[:class:`SubmitTask`] (optional)
                timeout_seconds:
                  type: integer
                  description: int (optional) An optional timeout applied to each
                    run of this job. A value of `0` means no timeout.
                usage_policy_id:
                  type: string
                  description: str (optional) The user specified id of the usage policy
                    to use for this one-time run. If not specified, a default usage
                    policy may be applied when creating or modifying the job.
                webhook_notifications:
                  $ref: '#/components/schemas/WebhookNotifications'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Run'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.2/jobs/update:
    post:
      operationId: update
      summary: 'Add, update, or remove specific settings of an existing job. Use the
        [_Reset_

        endpoint](:method:jobs/reset) to overwrite all job settings.'
      description: "Add, update, or remove specific settings of an existing job. Use\
        \ the [_Reset_\nendpoint](:method:jobs/reset) to overwrite all job settings.\n\
        \n:param job_id: int\n  The canonical identifier of the job to update. This\
        \ field is required.\n:param fields_to_remove: List[str] (optional)\n  Remove\
        \ top-level fields in the job settings. Removing nested fields is not supported,\
        \ except for\n  tasks and job clusters (`tasks/task_1`). This field is optional.\n\
        :param new_settings: :class:`JobSettings` (optional)\n  The new settings for\
        \ the job.\n\n  Top-level fields specified in `new_settings` are completely\
        \ replaced, except for arrays which are\n  merged. That is, new and existing\
        \ entries are completely replaced based on the respective key\n  fields, i.e.\
        \ `task_key` or `job_cluster_key`, while previous entries are kept.\n\n  Partially\
        \ updating nested fields is not supported.\n\n  Changes to the field `JobSettings.timeout_seconds`\
        \ are applied to active runs. Changes to other\n  fields are applied to future\
        \ runs only."
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: int The canonical identifier of the job to update.
                    This field is required.
                fields_to_remove:
                  type: array
                  items:
                    type: string
                  description: List[str] (optional) Remove top-level fields in the
                    job settings. Removing nested fields is not supported, except
                    for tasks and job clusters (`tasks/task_1`). This field is optional.
                new_settings:
                  $ref: '#/components/schemas/JobSettings'
      responses:
        '200':
          description: Success
  /api/2.0/policies/jobs/enforce-compliance:
    post:
      operationId: enforce_compliance
      summary: 'Updates a job so the job clusters that are created when running the
        job (specified in `new_cluster`)

        are compliant with the current versions of their respective cluster policies.
        All-purpose clusters

        used in the job will not be updated.'
      description: "Updates a job so the job clusters that are created when running\
        \ the job (specified in `new_cluster`)\nare compliant with the current versions\
        \ of their respective cluster policies. All-purpose clusters\nused in the\
        \ job will not be updated.\n\n:param job_id: int\n  The ID of the job you\
        \ want to enforce policy compliance on.\n:param validate_only: bool (optional)\n\
        \  If set, previews changes made to the job to comply with its policy, but\
        \ does not update the job.\n\n:returns: :class:`EnforcePolicyComplianceResponse`"
      tags:
      - jobs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: int The ID of the job you want to enforce policy compliance
                    on.
                validate_only:
                  type: boolean
                  description: bool (optional) If set, previews changes made to the
                    job to comply with its policy, but does not update the job.
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/EnforcePolicyComplianceResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policies/jobs/get-compliance:
    get:
      operationId: get_compliance
      summary: 'Returns the policy compliance status of a job. Jobs could be out of
        compliance if a cluster policy

        they use was updated after the job was last edited and some of its job clusters
        no longer comply with

        their updated policies.'
      description: "Returns the policy compliance status of a job. Jobs could be out\
        \ of compliance if a cluster policy\nthey use was updated after the job was\
        \ last edited and some of its job clusters no longer comply with\ntheir updated\
        \ policies.\n\n:param job_id: int\n  The ID of the job whose compliance status\
        \ you are requesting.\n\n:returns: :class:`GetPolicyComplianceResponse`"
      tags:
      - jobs
      parameters:
      - name: job_id
        description: int The ID of the job whose compliance status you are requesting.
        required: true
        schema:
          type: integer
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetPolicyComplianceResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
  /api/2.0/policies/jobs/list-compliance:
    get:
      operationId: list_compliance
      summary: 'Returns the policy compliance status of all jobs that use a given
        policy. Jobs could be out of

        compliance if a cluster policy they use was updated after the job was last
        edited and its job clusters

        no longer comply with the updated policy.'
      description: "Returns the policy compliance status of all jobs that use a given\
        \ policy. Jobs could be out of\ncompliance if a cluster policy they use was\
        \ updated after the job was last edited and its job clusters\nno longer comply\
        \ with the updated policy.\n\n:param policy_id: str\n  Canonical unique identifier\
        \ for the cluster policy.\n:param page_size: int (optional)\n  Use this field\
        \ to specify the maximum number of results to be returned by the server. The\
        \ server may\n  further constrain the maximum number of results returned in\
        \ a single page.\n:param page_token: str (optional)\n  A page token that can\
        \ be used to navigate to the next page or previous page as returned by\n \
        \ `next_page_token` or `prev_page_token`.\n\n:returns: Iterator over :class:`JobCompliance`"
      tags:
      - jobs
      parameters:
      - name: policy_id
        description: str Canonical unique identifier for the cluster policy.
        required: true
        schema:
          type: string
        in: query
      - name: page_size
        description: int (optional) Use this field to specify the maximum number of
          results to be returned by the server. The server may further constrain the
          maximum number of results returned in a single page.
        required: false
        schema:
          type: integer
        in: query
      - name: page_token
        description: str (optional) A page token that can be used to navigate to the
          next page or previous page as returned by `next_page_token` or `prev_page_token`.
        required: false
        schema:
          type: string
        in: query
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobCompliance'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '404':
          description: Not found
        '500':
          description: Internal server error
components:
  schemas:
    AuthenticationMethod:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - OAUTH
      - PAT
    BaseJob:
      type: object
      description: 'BaseJob(created_time: ''Optional[int]'' = None, creator_user_name:
        ''Optional[str]'' = None, effective_budget_policy_id: ''Optional[str]'' =
        None, effective_usage_policy_id: ''Optional[str]'' = None, has_more: ''Optional[bool]''
        = None, job_id: ''Optional[int]'' = None, settings: ''Optional[JobSettings]''
        = None, trigger_state: ''Optional[TriggerStateProto]'' = None)'
      properties:
        created_time:
          type: string
          description: 'The time at which this job was created in epoch milliseconds
            (milliseconds since 1/1/1970 UTC).  creator_user_name: Optional[str] =
            None """The creator user name. This field won’t be included in the response
            if the user has already been deleted.'
        creator_user_name:
          type: string
          description: The creator user name. This field won’t be included in the
            response if the user has already been deleted.
        effective_budget_policy_id:
          type: string
          description: 'The id of the budget policy used by this job for cost attribution
            purposes. This may be set through (in order of precedence): 1. Budget
            admins through the account or workspace console 2. Jobs UI in the job
            details page and Jobs API using `budget_policy_id` 3. Inferred default
            based on accessible budget policies of the run_as identity on job creation
            or modification.'
        effective_usage_policy_id:
          type: string
          description: 'The id of the usage policy used by this job for cost attribution
            purposes.  has_more: Optional[bool] = None """Indicates if the job has
            more array properties (`tasks`, `job_clusters`) that are not shown. They
            can be accessed via :method:jobs/get endpoint. It is only relevant for
            API 2.2 :method:jobs/list requests with `expand_tasks=true`.'
        has_more:
          type: string
          description: Indicates if the job has more array properties (`tasks`, `job_clusters`)
            that are not shown. They can be accessed via :method:jobs/get endpoint.
            It is only relevant for API 2.2 :method:jobs/list requests with `expand_tasks=true`.
        job_id:
          type: string
          description: 'The canonical identifier for this job.  settings: Optional[JobSettings]
            = None """Settings for this job and all of its runs. These settings can
            be updated using the `resetJob` method.'
        settings:
          type: string
          description: Settings for this job and all of its runs. These settings can
            be updated using the `resetJob` method.
        trigger_state:
          type: string
          description: 'State of the trigger associated with the job.  def as_dict(self)
            -> dict: Serializes the BaseJob into a dictionary suitable for use as
            a JSON request body.'
    BaseRun:
      type: object
      description: 'BaseRun(attempt_number: ''Optional[int]'' = None, cleanup_duration:
        ''Optional[int]'' = None, cluster_instance: ''Optional[ClusterInstance]''
        = None, cluster_spec: ''Optional[ClusterSpec]'' = None, creator_user_name:
        ''Optional[str]'' = None, description: ''Optional[str]'' = None, effective_performance_target:
        ''Optional[PerformanceTarget]'' = None, effective_usage_policy_id: ''Optional[str]''
        = None, end_time: ''Optional[int]'' = None, execution_duration: ''Optional[int]''
        = None, git_source: ''Optional[GitSource]'' = None, has_more: ''Optional[bool]''
        = None, job_clusters: ''Optional[List[JobCluster]]'' = None, job_id: ''Optional[int]''
        = None, job_parameters: ''Optional[List[JobParameter]]'' = None, job_run_id:
        ''Optional[int]'' = None, number_in_job: ''Optional[int]'' = None, original_attempt_run_id:
        ''Optional[int]'' = None, overriding_parameters: ''Optional[RunParameters]''
        = None, queue_duration: ''Optional[int]'' = None, repair_history: ''Optional[List[RepairHistoryItem]]''
        = None, run_duration: ''Optional[int]'' = None, run_id: ''Optional[int]''
        = None, run_name: ''Optional[str]'' = None, run_page_url: ''Optional[str]''
        = None, run_type: ''Optional[RunType]'' = None, schedule: ''Optional[CronSchedule]''
        = None, setup_duration: ''Optional[int]'' = None, start_time: ''Optional[int]''
        = None, state: ''Optional[RunState]'' = None, status: ''Optional[RunStatus]''
        = None, tasks: ''Optional[List[RunTask]]'' = None, trigger: ''Optional[TriggerType]''
        = None, trigger_info: ''Optional[TriggerInfo]'' = None)'
      properties:
        attempt_number:
          type: string
          description: The sequence number of this run attempt for a triggered job
            run. The initial attempt of a run has an attempt_number of 0. If the initial
            run attempt fails, and the job has a retry policy (`max_retries` > 0),
            subsequent runs are created with an `original_attempt_run_id` of the original
            attempt’s ID and an incrementing `attempt_number`. Runs are retried only
            until they succeed, and the maximum `attempt_number` is the same as the
            `max_retries` value for the job.
        cleanup_duration:
          type: string
          description: The time in milliseconds it took to terminate the cluster and
            clean up any associated artifacts. The duration of a task run is the sum
            of the `setup_duration`, `execution_duration`, and the `cleanup_duration`.
            The `cleanup_duration` field is set to 0 for multitask job runs. The total
            duration of a multitask job run is the value of the `run_duration` field.
        cluster_instance:
          type: string
          description: The cluster used for this run. If the run is specified to use
            a new cluster, this field is set once the Jobs service has requested a
            cluster for the run.
        cluster_spec:
          type: string
          description: 'A snapshot of the job’s cluster specification when this run
            was created.  creator_user_name: Optional[str] = None """The creator user
            name. This field won’t be included in the response if the user has already
            been deleted.'
        creator_user_name:
          type: string
          description: The creator user name. This field won’t be included in the
            response if the user has already been deleted.
        description:
          type: string
          description: 'Description of the run  effective_performance_target: Optional[PerformanceTarget]
            = None """The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on the
            request depending on whether the performance mode is supported by the
            job type.  * `STANDARD`: Enables cost-efficient execution of serverless
            workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
            times through rapid scaling and optimized cluster performance.'
        effective_performance_target:
          type: string
          description: 'The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on the
            request depending on whether the performance mode is supported by the
            job type.  * `STANDARD`: Enables cost-efficient execution of serverless
            workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
            times through rapid scaling and optimized cluster performance.'
        effective_usage_policy_id:
          type: string
          description: 'The id of the usage policy used by this run for cost attribution
            purposes.  end_time: Optional[int] = None """The time at which this run
            ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field
            is set to 0 if the job is still running.'
        end_time:
          type: string
          description: The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still running.
        execution_duration:
          type: string
          description: The time in milliseconds it took to execute the commands in
            the JAR or notebook until they completed, failed, timed out, were cancelled,
            or encountered an unexpected error. The duration of a task run is the
            sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`.
            The `execution_duration` field is set to 0 for multitask job runs. The
            total duration of a multitask job run is the value of the `run_duration`
            field.
        git_source:
          type: string
          description: 'An optional specification for a remote Git repository containing
            the source code used by tasks. Version-controlled source code is supported
            by notebook, dbt, Python script, and SQL File tasks.  If `git_source`
            is set, these tasks retrieve the file from the remote repository by default.
            However, this behavior can be overridden by setting `source` to `WORKSPACE`
            on the task.  Note: dbt and SQL File tasks support only version-controlled
            sources. If dbt or SQL File tasks are used, `git_source` must be defined
            on the job.'
        has_more:
          type: string
          description: Indicates if the run has more array properties (`tasks`, `job_clusters`)
            that are not shown. They can be accessed via :method:jobs/getrun endpoint.
            It is only relevant for API 2.2 :method:jobs/listruns requests with `expand_tasks=true`.
        job_clusters:
          type: string
          description: A list of job cluster specifications that can be shared and
            reused by tasks of this job. Libraries cannot be declared in a shared
            job cluster. You must declare dependent libraries in task settings. If
            more than 100 job clusters are available, you can paginate through them
            using :method:jobs/getrun.
        job_id:
          type: string
          description: 'The canonical identifier of the job that contains this run.  job_parameters:
            Optional[List[JobParameter]] = None Job-level parameters used in the run'
        job_parameters:
          type: string
          description: 'Job-level parameters used in the run  job_run_id: Optional[int]
            = None """ID of the job run that this run belongs to. For legacy and single-task
            job runs the field is populated with the job run ID. For task runs, the
            field is populated with the ID of the job run that the task run belongs
            to.'
        job_run_id:
          type: string
          description: ID of the job run that this run belongs to. For legacy and
            single-task job runs the field is populated with the job run ID. For task
            runs, the field is populated with the ID of the job run that the task
            run belongs to.
        number_in_job:
          type: string
          description: 'A unique identifier for this job run. This is set to the same
            value as `run_id`.  original_attempt_run_id: Optional[int] = None """If
            this run is a retry of a prior run attempt, this field contains the run_id
            of the original attempt; otherwise, it is the same as the run_id.'
        original_attempt_run_id:
          type: string
          description: If this run is a retry of a prior run attempt, this field contains
            the run_id of the original attempt; otherwise, it is the same as the run_id.
        overriding_parameters:
          type: string
          description: 'The parameters used for this run.  queue_duration: Optional[int]
            = None The time in milliseconds that the run has spent in the queue.'
        queue_duration:
          type: string
          description: 'The time in milliseconds that the run has spent in the queue.  repair_history:
            Optional[List[RepairHistoryItem]] = None The repair history of the run.'
        repair_history:
          type: string
          description: 'The repair history of the run.  run_duration: Optional[int]
            = None The time in milliseconds it took the job run and all of its repairs
            to finish.'
        run_duration:
          type: string
          description: 'The time in milliseconds it took the job run and all of its
            repairs to finish.  run_id: Optional[int] = None The canonical identifier
            of the run. This ID is unique across all runs of all jobs.'
        run_id:
          type: string
          description: ID of the job run that this run belongs to. For legacy and
            single-task job runs the field is populated with the job run ID. For task
            runs, the field is populated with the ID of the job run that the task
            run belongs to.
        run_name:
          type: string
          description: 'An optional name for the run. The maximum length is 4096 bytes
            in UTF-8 encoding.  run_page_url: Optional[str] = None The URL to the
            detail page of the run.'
        run_page_url:
          type: string
          description: 'The URL to the detail page of the run.  run_type: Optional[RunType]
            = None  schedule: Optional[CronSchedule] = None The cron schedule that
            triggered this run if it was triggered by the periodic scheduler.'
        run_type:
          type: string
          description: ''
        schedule:
          type: string
          description: 'The cron schedule that triggered this run if it was triggered
            by the periodic scheduler.  setup_duration: Optional[int] = None """The
            time in milliseconds it took to set up the cluster. For runs that run
            on new clusters this is the cluster creation time, for runs that run on
            existing clusters this time should be very short. The duration of a task
            run is the sum of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `setup_duration` field is set to 0 for multitask
            job runs. The total duration of a multitask job run is the value of the
            `run_duration` field.'
        setup_duration:
          type: string
          description: The time in milliseconds it took to set up the cluster. For
            runs that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The duration
            of a task run is the sum of the `setup_duration`, `execution_duration`,
            and the `cleanup_duration`. The `setup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the value
            of the `run_duration` field.
        start_time:
          type: string
          description: The time at which this run was started in epoch milliseconds
            (milliseconds since 1/1/1970 UTC). This may not be the time when the job
            task starts executing, for example, if the job is scheduled to run on
            a new cluster, this is the time the cluster creation call is issued.
        state:
          type: string
          description: 'Deprecated. Please use the `status` field instead.  status:
            Optional[RunStatus] = None  tasks: Optional[List[RunTask]] = None """The
            list of tasks performed by the run. Each task has its own `run_id` which
            you can use to call `JobsGetOutput` to retrieve the run resutls. If more
            than 100 tasks are available, you can paginate through them using :method:jobs/getrun.
            Use the `next_page_token` field at the object root to determine if more
            results are available.'
        status:
          type: string
          description: ''
        tasks:
          type: string
          description: The list of tasks performed by the run. Each task has its own
            `run_id` which you can use to call `JobsGetOutput` to retrieve the run
            resutls. If more than 100 tasks are available, you can paginate through
            them using :method:jobs/getrun. Use the `next_page_token` field at the
            object root to determine if more results are available.
        trigger:
          type: string
          description: ''
        trigger_info:
          type: string
          description: ''
    CancelAllRunsResponse:
      type: object
      description: CancelAllRunsResponse()
      properties: {}
    CancelRunResponse:
      type: object
      description: CancelRunResponse()
      properties: {}
    CleanRoomTaskRunLifeCycleState:
      type: string
      description: 'Copied from elastic-spark-common/api/messages/runs.proto. Using
        the original definition to

        remove coupling with jobs API definition'
      enum:
      - BLOCKED
      - INTERNAL_ERROR
      - PENDING
      - QUEUED
      - RUNNING
      - RUN_LIFE_CYCLE_STATE_UNSPECIFIED
      - SKIPPED
      - TERMINATED
      - TERMINATING
      - WAITING_FOR_RETRY
    CleanRoomTaskRunResultState:
      type: string
      description: 'Copied from elastic-spark-common/api/messages/runs.proto. Using
        the original definition to avoid

        cyclic dependency.'
      enum:
      - CANCELED
      - DISABLED
      - EVICTED
      - EXCLUDED
      - FAILED
      - MAXIMUM_CONCURRENT_RUNS_REACHED
      - RUN_RESULT_STATE_UNSPECIFIED
      - SUCCESS
      - SUCCESS_WITH_FAILURES
      - TIMEDOUT
      - UPSTREAM_CANCELED
      - UPSTREAM_EVICTED
      - UPSTREAM_FAILED
    CleanRoomTaskRunState:
      type: object
      description: Stores the run state of the clean rooms notebook task.
      properties:
        life_cycle_state:
          type: string
          description: 'A value indicating the run''s current lifecycle state. This
            field is always available in the response. Note: Additional states might
            be introduced in future releases.'
        result_state:
          type: string
          description: 'A value indicating the run''s result. This field is only available
            for terminal lifecycle states. Note: Additional states might be introduced
            in future releases.'
    CleanRoomsNotebookTask:
      type: object
      description: 'Clean Rooms notebook task for V1 Clean Room service (GA). Replaces
        the deprecated

        CleanRoomNotebookTask (defined above) which was for V0 service.'
      properties:
        clean_room_name:
          type: string
          description: 'The clean room that the notebook belongs to.  notebook_name:
            str Name of the notebook being run.'
        notebook_name:
          type: string
          description: 'Name of the notebook being run.  etag: Optional[str] = None
            """Checksum to validate the freshness of the notebook resource (i.e. the
            notebook being run is the latest version). It can be fetched by calling
            the :method:cleanroomassets/get API.'
        etag:
          type: string
          description: Checksum to validate the freshness of the notebook resource
            (i.e. the notebook being run is the latest version). It can be fetched
            by calling the :method:cleanroomassets/get API.
        notebook_base_parameters:
          type: string
          description: 'Base parameters to be used for the clean room notebook job.  def
            as_dict(self) -> dict: Serializes the CleanRoomsNotebookTask into a dictionary
            suitable for use as a JSON request body.'
    CleanRoomsNotebookTaskCleanRoomsNotebookTaskOutput:
      type: object
      description: 'CleanRoomsNotebookTaskCleanRoomsNotebookTaskOutput(clean_room_job_run_state:
        ''Optional[CleanRoomTaskRunState]'' = None, notebook_output: ''Optional[NotebookOutput]''
        = None, output_schema_info: ''Optional[OutputSchemaInfo]'' = None)'
      properties:
        clean_room_job_run_state:
          type: string
          description: 'The run state of the clean rooms notebook task.  notebook_output:
            Optional[NotebookOutput] = None The notebook output for the clean room
            run'
        notebook_output:
          type: string
          description: 'The notebook output for the clean room run  output_schema_info:
            Optional[OutputSchemaInfo] = None Information on how to access the output
            schema for the clean room run'
        output_schema_info:
          type: string
          description: 'Information on how to access the output schema for the clean
            room run  def as_dict(self) -> dict: Serializes the CleanRoomsNotebookTaskCleanRoomsNotebookTaskOutput
            into a dictionary suitable for use as a JSON request body.'
    ClusterInstance:
      type: object
      description: 'ClusterInstance(cluster_id: ''Optional[str]'' = None, spark_context_id:
        ''Optional[str]'' = None)'
      properties:
        cluster_id:
          type: string
          description: The canonical identifier for the cluster used by a run. This
            field is always available for runs on existing clusters. For runs on new
            clusters, it becomes available once the cluster is created. This value
            can be used to view logs by browsing to `/#setting/sparkui/$cluster_id/driver-logs`.
            The logs continue to be available after the run completes.  The response
            won’t include this field if the identifier is not available yet.
        spark_context_id:
          type: string
          description: The canonical identifier for the Spark context used by a run.
            This field is filled in once the run begins execution. This value can
            be used to view the Spark UI by browsing to `/#setting/sparkui/$cluster_id/$spark_context_id`.
            The Spark UI continues to be available after the run has completed.  The
            response won’t include this field if the identifier is not available yet.
    ClusterSpec:
      type: object
      description: 'ClusterSpec(existing_cluster_id: ''Optional[str]'' = None, job_cluster_key:
        ''Optional[str]'' = None, libraries: ''Optional[List[compute.Library]]'' =
        None, new_cluster: ''Optional[compute.ClusterSpec]'' = None)'
      properties:
        existing_cluster_id:
          type: string
          description: If existing_cluster_id, the ID of an existing cluster that
            is used for all runs. When running jobs or tasks on an existing cluster,
            you may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater reliability
        job_cluster_key:
          type: string
          description: If job_cluster_key, this task is executed reusing the cluster
            specified in `job.settings.job_clusters`.
        libraries:
          type: string
          description: An optional list of libraries to be installed on the cluster.
            The default value is an empty list.
        new_cluster:
          type: string
          description: 'If new_cluster, a description of a new cluster that is created
            for each run.  def as_dict(self) -> dict: Serializes the ClusterSpec into
            a dictionary suitable for use as a JSON request body.'
    ComputeConfig:
      type: object
      description: 'ComputeConfig(num_gpus: ''int'', gpu_node_pool_id: ''Optional[str]''
        = None, gpu_type: ''Optional[str]'' = None)'
      properties:
        num_gpus:
          type: string
          description: 'Number of GPUs.  gpu_node_pool_id: Optional[str] = None IDof
            the GPU pool to use.'
        gpu_node_pool_id:
          type: string
          description: 'IDof the GPU pool to use.  gpu_type: Optional[str] = None
            GPU type.'
        gpu_type:
          type: string
          description: 'GPU type.  def as_dict(self) -> dict: Serializes the ComputeConfig
            into a dictionary suitable for use as a JSON request body.'
    Condition:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - ALL_UPDATED
      - ANY_UPDATED
    ConditionTask:
      type: object
      description: 'ConditionTask(op: ''ConditionTaskOp'', left: ''str'', right: ''str'')'
      properties:
        op:
          type: string
          description: '* `EQUAL_TO`, `NOT_EQUAL` operators perform string comparison
            of their operands. This means that `“12.0” == “12”` will evaluate to `false`.
            * `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`
            operators perform numeric comparison of their operands. `“12.0” >= “12”`
            will evaluate to `true`, `“10.0” >= “12”` will evaluate to `false`.  The
            boolean comparison to task values can be implemented with operators `EQUAL_TO`,
            `NOT_EQUAL`. If a task value was set to a boolean value, it will be serialized
            to `“true”` or `“false”` for the comparison.'
        left:
          type: string
          description: The left operand of the condition task. Can be either a string
            value or a job state or parameter reference.
        right:
          type: string
          description: The right operand of the condition task. Can be either a string
            value or a job state or parameter reference.
    ConditionTaskOp:
      type: string
      description: '* `EQUAL_TO`, `NOT_EQUAL` operators perform string comparison
        of their operands. This means that

        `“12.0” == “12”` will evaluate to `false`. * `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`,

        `LESS_THAN`, `LESS_THAN_OR_EQUAL` operators perform numeric comparison of
        their operands.

        `“12.0” >= “12”` will evaluate to `true`, `“10.0” >= “12”` will evaluate to

        `false`.


        The boolean comparison to task values can be implemented with operators `EQUAL_TO`,
        `NOT_EQUAL`.

        If a task value was set to a boolean value, it will be serialized to `“true”`
        or

        `“false”` for the comparison.'
      enum:
      - EQUAL_TO
      - GREATER_THAN
      - GREATER_THAN_OR_EQUAL
      - LESS_THAN
      - LESS_THAN_OR_EQUAL
      - NOT_EQUAL
    Continuous:
      type: object
      description: 'Continuous(pause_status: ''Optional[PauseStatus]'' = None, task_retry_mode:
        ''Optional[TaskRetryMode]'' = None)'
      properties:
        pause_status:
          type: string
          description: 'Indicate whether the continuous execution of the job is paused
            or not. Defaults to UNPAUSED.  task_retry_mode: Optional[TaskRetryMode]
            = None Indicate whether the continuous job is applying task level retries
            or not. Defaults to NEVER.'
        task_retry_mode:
          type: string
          description: 'Indicate whether the continuous job is applying task level
            retries or not. Defaults to NEVER.  def as_dict(self) -> dict: Serializes
            the Continuous into a dictionary suitable for use as a JSON request body.'
    CreateResponse:
      type: object
      description: Job was created successfully
      properties:
        job_id:
          type: string
          description: 'The canonical identifier for the newly created job.  def as_dict(self)
            -> dict: Serializes the CreateResponse into a dictionary suitable for
            use as a JSON request body.'
    CronSchedule:
      type: object
      description: 'CronSchedule(quartz_cron_expression: ''str'', timezone_id: ''str'',
        pause_status: ''Optional[PauseStatus]'' = None)'
      properties:
        quartz_cron_expression:
          type: string
          description: 'A Cron expression using Quartz syntax that describes the schedule
            for a job. See [Cron Trigger] for details. This field is required.  [Cron
            Trigger]: http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html'
        timezone_id:
          type: string
          description: 'A Java timezone ID. The schedule for a job is resolved with
            respect to this timezone. See [Java TimeZone] for details. This field
            is required.  [Java TimeZone]: https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html'
        pause_status:
          type: string
          description: 'Indicate whether this schedule is paused or not.  def as_dict(self)
            -> dict: Serializes the CronSchedule into a dictionary suitable for use
            as a JSON request body.'
    DashboardPageSnapshot:
      type: object
      description: 'DashboardPageSnapshot(page_display_name: ''Optional[str]'' = None,
        widget_error_details: ''Optional[List[WidgetErrorDetail]]'' = None)'
      properties:
        page_display_name:
          type: string
          description: ''
        widget_error_details:
          type: string
          description: ''
    DashboardTask:
      type: object
      description: Configures the Lakeview Dashboard job task type.
      properties:
        dashboard_id:
          type: string
          description: 'The identifier of the dashboard to refresh.  subscription:
            Optional[Subscription] = None Optional: subscription configuration for
            sending the dashboard snapshot.'
        subscription:
          type: string
          description: 'Optional: subscription configuration for sending the dashboard
            snapshot.  warehouse_id: Optional[str] = None """Optional: The warehouse
            id to execute the dashboard with for the schedule. If not specified, the
            default warehouse of the dashboard will be used.'
        warehouse_id:
          type: string
          description: 'Optional: The warehouse id to execute the dashboard with for
            the schedule. If not specified, the default warehouse of the dashboard
            will be used.'
    DashboardTaskOutput:
      type: object
      description: 'DashboardTaskOutput(page_snapshots: ''Optional[List[DashboardPageSnapshot]]''
        = None)'
      properties:
        page_snapshots:
          type: string
          description: 'Should only be populated for manual PDF download jobs.  def
            as_dict(self) -> dict: Serializes the DashboardTaskOutput into a dictionary
            suitable for use as a JSON request body.'
    DbtCloudJobRunStep:
      type: object
      description: 'Format of response retrieved from dbt Cloud, for inclusion in
        output Deprecated in favor of

        DbtPlatformJobRunStep'
      properties:
        index:
          type: string
          description: 'Orders the steps in the job  logs: Optional[str] = None Output
            of the step'
        logs:
          type: string
          description: 'Output of the step  name: Optional[str] = None Name of the
            step in the job'
        name:
          type: string
          description: 'Name of the step in the job  status: Optional[DbtPlatformRunStatus]
            = None State of the step'
        status:
          type: string
          description: 'State of the step  def as_dict(self) -> dict: Serializes the
            DbtCloudJobRunStep into a dictionary suitable for use as a JSON request
            body.'
    DbtCloudTask:
      type: object
      description: Deprecated in favor of DbtPlatformTask
      properties:
        connection_resource_name:
          type: string
          description: 'The resource name of the UC connection that authenticates
            the dbt Cloud for this task  dbt_cloud_job_id: Optional[int] = None Id
            of the dbt Cloud job to be triggered'
        dbt_cloud_job_id:
          type: string
          description: 'Id of the dbt Cloud job to be triggered  def as_dict(self)
            -> dict: Serializes the DbtCloudTask into a dictionary suitable for use
            as a JSON request body.'
    DbtCloudTaskOutput:
      type: object
      description: Deprecated in favor of DbtPlatformTaskOutput
      properties:
        dbt_cloud_job_run_id:
          type: string
          description: 'Id of the job run in dbt Cloud  dbt_cloud_job_run_output:
            Optional[List[DbtCloudJobRunStep]] = None Steps of the job run as received
            from dbt Cloud'
        dbt_cloud_job_run_output:
          type: string
          description: 'Steps of the job run as received from dbt Cloud  dbt_cloud_job_run_url:
            Optional[str] = None Url where full run details can be viewed'
        dbt_cloud_job_run_url:
          type: string
          description: 'Url where full run details can be viewed  def as_dict(self)
            -> dict: Serializes the DbtCloudTaskOutput into a dictionary suitable
            for use as a JSON request body.'
    DbtOutput:
      type: object
      description: 'DbtOutput(artifacts_headers: ''Optional[Dict[str, str]]'' = None,
        artifacts_link: ''Optional[str]'' = None)'
      properties:
        artifacts_headers:
          type: string
          description: 'An optional map of headers to send when retrieving the artifact
            from the `artifacts_link`.  artifacts_link: Optional[str] = None """A
            pre-signed URL to download the (compressed) dbt artifacts. This link is
            valid for a limited time (30 minutes). This information is only available
            after the run has finished.'
        artifacts_link:
          type: string
          description: A pre-signed URL to download the (compressed) dbt artifacts.
            This link is valid for a limited time (30 minutes). This information is
            only available after the run has finished.
    DbtPlatformJobRunStep:
      type: object
      description: Format of response retrieved from dbt platform, for inclusion in
        output
      properties:
        index:
          type: string
          description: 'Orders the steps in the job  logs: Optional[str] = None Output
            of the step'
        logs:
          type: string
          description: 'Output of the step  logs_truncated: Optional[bool] = None
            """Whether the logs of this step have been truncated. If true, the logs
            has been truncated to 10000 characters.'
        logs_truncated:
          type: string
          description: Whether the logs of this step have been truncated. If true,
            the logs has been truncated to 10000 characters.
        name:
          type: string
          description: 'Name of the step in the job  name_truncated: Optional[bool]
            = None """Whether the name of the job has been truncated. If true, the
            name has been truncated to 100 characters.'
        name_truncated:
          type: string
          description: Whether the name of the job has been truncated. If true, the
            name has been truncated to 100 characters.
        status:
          type: string
          description: 'State of the step  def as_dict(self) -> dict: Serializes the
            DbtPlatformJobRunStep into a dictionary suitable for use as a JSON request
            body.'
    DbtPlatformRunStatus:
      type: string
      description: Response enumeration from calling the dbt platform API, for inclusion
        in output
      enum:
      - CANCELLED
      - ERROR
      - QUEUED
      - RUNNING
      - STARTING
      - SUCCESS
    DbtPlatformTask:
      type: object
      description: 'DbtPlatformTask(connection_resource_name: ''Optional[str]'' =
        None, dbt_platform_job_id: ''Optional[str]'' = None)'
      properties:
        connection_resource_name:
          type: string
          description: 'The resource name of the UC connection that authenticates
            the dbt platform for this task  dbt_platform_job_id: Optional[str] = None
            """Id of the dbt platform job to be triggered. Specified as a string for
            maximum compatibility with clients.'
        dbt_platform_job_id:
          type: string
          description: Id of the dbt platform job to be triggered. Specified as a
            string for maximum compatibility with clients.
    DbtPlatformTaskOutput:
      type: object
      description: 'DbtPlatformTaskOutput(dbt_platform_job_run_id: ''Optional[str]''
        = None, dbt_platform_job_run_output: ''Optional[List[DbtPlatformJobRunStep]]''
        = None, dbt_platform_job_run_url: ''Optional[str]'' = None, steps_truncated:
        ''Optional[bool]'' = None)'
      properties:
        dbt_platform_job_run_id:
          type: string
          description: 'Id of the job run in dbt platform. Specified as a string for
            maximum compatibility with clients.  dbt_platform_job_run_output: Optional[List[DbtPlatformJobRunStep]]
            = None Steps of the job run as received from dbt platform'
        dbt_platform_job_run_output:
          type: string
          description: 'Steps of the job run as received from dbt platform  dbt_platform_job_run_url:
            Optional[str] = None Url where full run details can be viewed'
        dbt_platform_job_run_url:
          type: string
          description: 'Url where full run details can be viewed  steps_truncated:
            Optional[bool] = None """Whether the number of steps in the output has
            been truncated. If true, the output will contain the first 20 steps of
            the output.'
        steps_truncated:
          type: string
          description: Whether the number of steps in the output has been truncated.
            If true, the output will contain the first 20 steps of the output.
    DbtTask:
      type: object
      description: 'DbtTask(commands: ''List[str]'', catalog: ''Optional[str]'' =
        None, profiles_directory: ''Optional[str]'' = None, project_directory: ''Optional[str]''
        = None, schema: ''Optional[str]'' = None, source: ''Optional[Source]'' = None,
        warehouse_id: ''Optional[str]'' = None)'
      properties:
        commands:
          type: string
          description: A list of dbt commands to execute. All commands must start
            with `dbt`. This parameter must not be empty. A maximum of up to 10 commands
            can be provided.
        catalog:
          type: string
          description: Optional name of the catalog to use. The value is the top level
            in the 3-level namespace of Unity Catalog (catalog / schema / relation).
            The catalog value can only be specified if a warehouse_id is specified.
            Requires dbt-databricks >= 1.1.1.
        profiles_directory:
          type: string
          description: Optional (relative) path to the profiles directory. Can only
            be specified if no warehouse_id is specified. If no warehouse_id is specified
            and this folder is unset, the root directory is used.
        project_directory:
          type: string
          description: Path to the project directory. Optional for Git sourced tasks,
            in which case if no value is provided, the root of the Git repository
            is used.
        schema:
          type: string
          description: Optional schema to write to. This parameter is only used when
            a warehouse_id is also provided. If not provided, the `default` schema
            is used.
        source:
          type: string
          description: 'Optional location type of the project directory. When set
            to `WORKSPACE`, the project will be retrieved from the local Databricks
            workspace. When set to `GIT`, the project will be retrieved from a Git
            repository defined in `git_source`. If the value is empty, the task will
            use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.  * `WORKSPACE`:
            Project is located in Databricks workspace. * `GIT`: Project is located
            in cloud Git provider.'
        warehouse_id:
          type: string
          description: ID of the SQL warehouse to connect to. If provided, we automatically
            generate and provide the profile and connection details to dbt. It can
            be overridden on a per-command basis by using the `--profiles-dir` command
            line argument.
    DeleteResponse:
      type: object
      description: DeleteResponse()
      properties: {}
    DeleteRunResponse:
      type: object
      description: DeleteRunResponse()
      properties: {}
    EnforcePolicyComplianceForJobResponseJobClusterSettingsChange:
      type: object
      description: 'Represents a change to the job cluster''s settings that would
        be required for the job clusters to

        become compliant with their policies.'
      properties:
        field:
          type: string
          description: 'The field where this change would be made, prepended with
            the job cluster key.  new_value: Optional[str] = None """The new value
            of this field after enforcing policy compliance (either a number, a boolean,
            or a string) converted to a string. This is intended to be read by a human.
            The typed new value of this field can be retrieved by reading the settings
            field in the API response.'
        new_value:
          type: string
          description: The new value of this field after enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string. This
            is intended to be read by a human. The typed new value of this field can
            be retrieved by reading the settings field in the API response.
        previous_value:
          type: string
          description: The previous value of this field before enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string. This
            is intended to be read by a human. The type of the field can be retrieved
            by reading the settings field in the API response.
    EnforcePolicyComplianceResponse:
      type: object
      description: 'EnforcePolicyComplianceResponse(has_changes: ''Optional[bool]''
        = None, job_cluster_changes: ''Optional[List[EnforcePolicyComplianceForJobResponseJobClusterSettingsChange]]''
        = None, settings: ''Optional[JobSettings]'' = None)'
      properties:
        has_changes:
          type: string
          description: Whether any changes have been made to the job cluster settings
            for the job to become compliant with its policies.
        job_cluster_changes:
          type: string
          description: A list of job cluster changes that have been made to the job’s
            cluster settings in order for all job clusters to become compliant with
            their policies.
        settings:
          type: string
          description: Updated job settings after policy enforcement. Policy enforcement
            only applies to job clusters that are created when running the job (which
            are specified in new_cluster) and does not apply to existing all-purpose
            clusters. Updated job settings are derived by applying policy default
            values to the existing job clusters in order to satisfy policy requirements.
    ExportRunOutput:
      type: object
      description: Run was exported successfully.
      properties:
        views:
          type: string
          description: The exported content in HTML format (one for every view item).
            To extract the HTML notebook from the JSON response, download and run
            this [Python script](/_static/examples/extract.py).
    FileArrivalTriggerConfiguration:
      type: object
      description: 'FileArrivalTriggerConfiguration(url: ''str'', min_time_between_triggers_seconds:
        ''Optional[int]'' = None, wait_after_last_change_seconds: ''Optional[int]''
        = None)'
      properties:
        url:
          type: string
          description: URL to be monitored for file arrivals. The path must point
            to the root or a subpath of the external location.
        min_time_between_triggers_seconds:
          type: string
          description: If set, the trigger starts a run only after the specified amount
            of time passed since the last time the trigger fired. The minimum allowed
            value is 60 seconds
        wait_after_last_change_seconds:
          type: string
          description: If set, the trigger starts a run only after no file activity
            has occurred for the specified amount of time. This makes it possible
            to wait for a batch of incoming files to arrive before triggering a run.
            The minimum allowed value is 60 seconds.
    FileArrivalTriggerState:
      type: object
      description: 'FileArrivalTriggerState(using_file_events: ''Optional[bool]''
        = None)'
      properties:
        using_file_events:
          type: string
          description: 'Indicates whether the trigger leverages file events to detect
            file arrivals.  def as_dict(self) -> dict: Serializes the FileArrivalTriggerState
            into a dictionary suitable for use as a JSON request body.'
    ForEachStats:
      type: object
      description: 'ForEachStats(error_message_stats: ''Optional[List[ForEachTaskErrorMessageStats]]''
        = None, task_run_stats: ''Optional[ForEachTaskTaskRunStats]'' = None)'
      properties:
        error_message_stats:
          type: string
          description: 'Sample of 3 most common error messages occurred during the
            iteration.  task_run_stats: Optional[ForEachTaskTaskRunStats] = None Describes
            stats of the iteration. Only latest retries are considered.'
        task_run_stats:
          type: string
          description: 'Describes stats of the iteration. Only latest retries are
            considered.  def as_dict(self) -> dict: Serializes the ForEachStats into
            a dictionary suitable for use as a JSON request body.'
    ForEachTask:
      type: object
      description: 'ForEachTask(inputs: ''str'', task: ''Task'', concurrency: ''Optional[int]''
        = None)'
      properties:
        inputs:
          type: string
          description: 'Array for task to iterate on. This can be a JSON string or
            a reference to an array parameter.  task: Task Configuration for the task
            that will be run for each element in the array'
        task:
          type: string
          description: 'Configuration for the task that will be run for each element
            in the array  concurrency: Optional[int] = None """An optional maximum
            allowed number of concurrent runs of the task. Set this value if you want
            to be able to execute multiple runs of the task concurrently.'
        concurrency:
          type: string
          description: An optional maximum allowed number of concurrent runs of the
            task. Set this value if you want to be able to execute multiple runs of
            the task concurrently.
    ForEachTaskErrorMessageStats:
      type: object
      description: 'ForEachTaskErrorMessageStats(count: ''Optional[int]'' = None,
        error_message: ''Optional[str]'' = None, termination_category: ''Optional[str]''
        = None)'
      properties:
        count:
          type: string
          description: 'Describes the count of such error message encountered during
            the iterations.  error_message: Optional[str] = None Describes the error
            message occured during the iterations.'
        error_message:
          type: string
          description: 'Describes the error message occured during the iterations.  termination_category:
            Optional[str] = None Describes the termination reason for the error message.'
        termination_category:
          type: string
          description: 'Describes the termination reason for the error message.  def
            as_dict(self) -> dict: Serializes the ForEachTaskErrorMessageStats into
            a dictionary suitable for use as a JSON request body.'
    ForEachTaskTaskRunStats:
      type: object
      description: 'ForEachTaskTaskRunStats(active_iterations: ''Optional[int]'' =
        None, completed_iterations: ''Optional[int]'' = None, failed_iterations: ''Optional[int]''
        = None, scheduled_iterations: ''Optional[int]'' = None, succeeded_iterations:
        ''Optional[int]'' = None, total_iterations: ''Optional[int]'' = None)'
      properties:
        active_iterations:
          type: string
          description: 'Describes the iteration runs having an active lifecycle state
            or an active run sub state.  completed_iterations: Optional[int] = None
            Describes the number of failed and succeeded iteration runs.'
        completed_iterations:
          type: string
          description: 'Describes the number of failed and succeeded iteration runs.  failed_iterations:
            Optional[int] = None Describes the number of failed iteration runs.'
        failed_iterations:
          type: string
          description: 'Describes the number of failed iteration runs.  scheduled_iterations:
            Optional[int] = None Describes the number of iteration runs that have
            been scheduled.'
        scheduled_iterations:
          type: string
          description: 'Describes the number of iteration runs that have been scheduled.  succeeded_iterations:
            Optional[int] = None Describes the number of succeeded iteration runs.'
        succeeded_iterations:
          type: string
          description: 'Describes the number of succeeded iteration runs.  total_iterations:
            Optional[int] = None Describes the length of the list of items to iterate
            over.'
        total_iterations:
          type: string
          description: 'Describes the length of the list of items to iterate over.  def
            as_dict(self) -> dict: Serializes the ForEachTaskTaskRunStats into a dictionary
            suitable for use as a JSON request body.'
    Format:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - MULTI_TASK
      - SINGLE_TASK
    GenAiComputeTask:
      type: object
      description: 'GenAiComputeTask(dl_runtime_image: ''str'', command: ''Optional[str]''
        = None, compute: ''Optional[ComputeConfig]'' = None, mlflow_experiment_name:
        ''Optional[str]'' = None, source: ''Optional[Source]'' = None, training_script_path:
        ''Optional[str]'' = None, yaml_parameters: ''Optional[str]'' = None, yaml_parameters_file_path:
        ''Optional[str]'' = None)'
      properties:
        dl_runtime_image:
          type: string
          description: 'Runtime image  command: Optional[str] = None Command launcher
            to run the actual script, e.g. bash, python etc.'
        command:
          type: string
          description: 'Command launcher to run the actual script, e.g. bash, python
            etc.  compute: Optional[ComputeConfig] = None  mlflow_experiment_name:
            Optional[str] = None """Optional string containing the name of the MLflow
            experiment to log the run to. If name is not found, backend will create
            the mlflow experiment using the name.'
        compute:
          type: string
          description: ''
        mlflow_experiment_name:
          type: string
          description: Optional string containing the name of the MLflow experiment
            to log the run to. If name is not found, backend will create the mlflow
            experiment using the name.
        source:
          type: string
          description: 'Optional location type of the training script. When set to
            `WORKSPACE`, the script will be retrieved from the local Databricks workspace.
            When set to `GIT`, the script will be retrieved from a Git repository
            defined in `git_source`. If the value is empty, the task will use `GIT`
            if `git_source` is defined and `WORKSPACE` otherwise. * `WORKSPACE`: Script
            is located in Databricks workspace. * `GIT`: Script is located in cloud
            Git provider.'
        training_script_path:
          type: string
          description: The training script file path to be executed. Cloud file URIs
            (such as dbfs:/, s3:/, adls:/, gcs:/) and workspace paths are supported.
            For python files stored in the Databricks workspace, the path must be
            absolute and begin with `/`. For files stored in a remote repository,
            the path must be relative. This field is required.
        yaml_parameters:
          type: string
          description: Optional string containing model parameters passed to the training
            script in yaml format. If present, then the content in yaml_parameters_file_path
            will be ignored.
        yaml_parameters_file_path:
          type: string
          description: 'Optional path to a YAML file containing model parameters passed
            to the training script.  def as_dict(self) -> dict: Serializes the GenAiComputeTask
            into a dictionary suitable for use as a JSON request body.'
    GetJobPermissionLevelsResponse:
      type: object
      description: 'GetJobPermissionLevelsResponse(permission_levels: ''Optional[List[JobPermissionsDescription]]''
        = None)'
      properties:
        permission_levels:
          type: string
          description: 'Specific permission levels  def as_dict(self) -> dict: Serializes
            the GetJobPermissionLevelsResponse into a dictionary suitable for use
            as a JSON request body.'
    GetPolicyComplianceResponse:
      type: object
      description: 'GetPolicyComplianceResponse(is_compliant: ''Optional[bool]'' =
        None, violations: ''Optional[Dict[str, str]]'' = None)'
      properties:
        is_compliant:
          type: string
          description: Whether the job is compliant with its policies or not. Jobs
            could be out of compliance if a policy they are using was updated after
            the job was last edited and some of its job clusters no longer comply
            with their updated policies.
        violations:
          type: string
          description: An object containing key-value mappings representing the first
            200 policy validation errors. The keys indicate the path where the policy
            validation error is occurring. An identifier for the job cluster is prepended
            to the path. The values indicate an error message describing the policy
            validation error.
    GitProvider:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - awsCodeCommit
      - azureDevOpsServices
      - bitbucketCloud
      - bitbucketServer
      - gitHub
      - gitHubEnterprise
      - gitLab
      - gitLabEnterpriseEdition
    GitSnapshot:
      type: object
      description: 'Read-only state of the remote repository at the time the job was
        run. This field is only

        included on job runs.'
      properties:
        used_commit:
          type: string
          description: Commit that was used to execute the run. If git_branch was
            specified, this points to the HEAD of the branch at the time of the run;
            if git_tag was specified, this points to the commit the tag points to.
    GitSource:
      type: object
      description: 'An optional specification for a remote Git repository containing
        the source code used by tasks.

        Version-controlled source code is supported by notebook, dbt, Python script,
        and SQL File tasks.


        If `git_source` is set, these tasks retrieve the file from the remote repository
        by default.

        However, this behavior can be overridden by setting `source` to `WORKSPACE`
        on the task.


        Note: dbt and SQL File tasks support only version-controlled sources. If dbt
        or SQL File tasks

        are used, `git_source` must be defined on the job.'
      properties:
        git_url:
          type: string
          description: 'URL of the repository to be cloned by this job.  git_provider:
            GitProvider Unique identifier of the service used to host the Git repository.
            The value is case insensitive.'
        git_provider:
          type: string
          description: 'Unique identifier of the service used to host the Git repository.
            The value is case insensitive.  git_branch: Optional[str] = None """Name
            of the branch to be checked out and used by this job. This field cannot
            be specified in conjunction with git_tag or git_commit.'
        git_branch:
          type: string
          description: Name of the branch to be checked out and used by this job.
            This field cannot be specified in conjunction with git_tag or git_commit.
        git_commit:
          type: string
          description: Commit to be checked out and used by this job. This field cannot
            be specified in conjunction with git_branch or git_tag.
        git_snapshot:
          type: string
          description: ''
        git_tag:
          type: string
          description: Name of the tag to be checked out and used by this job. This
            field cannot be specified in conjunction with git_branch or git_commit.
        job_source:
          type: string
          description: 'The source of the job specification in the remote repository
            when the job is source controlled.  def as_dict(self) -> dict: Serializes
            the GitSource into a dictionary suitable for use as a JSON request body.'
    Job:
      type: object
      description: Job was retrieved successfully.
      properties:
        created_time:
          type: string
          description: 'The time at which this job was created in epoch milliseconds
            (milliseconds since 1/1/1970 UTC).  creator_user_name: Optional[str] =
            None """The creator user name. This field won’t be included in the response
            if the user has already been deleted.'
        creator_user_name:
          type: string
          description: The creator user name. This field won’t be included in the
            response if the user has already been deleted.
        effective_budget_policy_id:
          type: string
          description: 'The id of the budget policy used by this job for cost attribution
            purposes. This may be set through (in order of precedence): 1. Budget
            admins through the account or workspace console 2. Jobs UI in the job
            details page and Jobs API using `budget_policy_id` 3. Inferred default
            based on accessible budget policies of the run_as identity on job creation
            or modification.'
        effective_usage_policy_id:
          type: string
          description: 'The id of the usage policy used by this job for cost attribution
            purposes.  has_more: Optional[bool] = None """Indicates if the job has
            more array properties (`tasks`, `job_clusters`) that are not shown. They
            can be accessed via :method:jobs/get endpoint. It is only relevant for
            API 2.2 :method:jobs/list requests with `expand_tasks=true`.'
        has_more:
          type: string
          description: Indicates if the job has more array properties (`tasks`, `job_clusters`)
            that are not shown. They can be accessed via :method:jobs/get endpoint.
            It is only relevant for API 2.2 :method:jobs/list requests with `expand_tasks=true`.
        job_id:
          type: string
          description: 'The canonical identifier for this job.  next_page_token: Optional[str]
            = None A token that can be used to list the next page of array properties.'
        next_page_token:
          type: string
          description: 'A token that can be used to list the next page of array properties.  run_as_user_name:
            Optional[str] = None """The email of an active workspace user or the application
            ID of a service principal that the job runs as. This value can be changed
            by setting the `run_as` field when creating or updating a job.  By default,
            `run_as_user_name` is based on the current job settings and is set to
            the creator of the job if job access control is disabled or to the user
            with the `is_owner` permission if job access control is enabled.'
        run_as_user_name:
          type: string
          description: The email of an active workspace user or the application ID
            of a service principal that the job runs as. This value can be changed
            by setting the `run_as` field when creating or updating a job.  By default,
            `run_as_user_name` is based on the current job settings and is set to
            the creator of the job if job access control is disabled or to the user
            with the `is_owner` permission if job access control is enabled.
        settings:
          type: string
          description: Settings for this job and all of its runs. These settings can
            be updated using the `resetJob` method.
        trigger_state:
          type: string
          description: 'State of the trigger associated with the job.  def as_dict(self)
            -> dict: Serializes the Job into a dictionary suitable for use as a JSON
            request body.'
    JobAccessControlRequest:
      type: object
      description: 'JobAccessControlRequest(group_name: ''Optional[str]'' = None,
        permission_level: ''Optional[JobPermissionLevel]'' = None, service_principal_name:
        ''Optional[str]'' = None, user_name: ''Optional[str]'' = None)'
      properties:
        group_name:
          type: string
          description: 'name of the group  permission_level: Optional[JobPermissionLevel]
            = None  service_principal_name: Optional[str] = None application ID of
            a service principal'
        permission_level:
          type: string
          description: ''
        service_principal_name:
          type: string
          description: 'application ID of a service principal  user_name: Optional[str]
            = None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            JobAccessControlRequest into a dictionary suitable for use as a JSON request
            body.'
    JobAccessControlResponse:
      type: object
      description: 'JobAccessControlResponse(all_permissions: ''Optional[List[JobPermission]]''
        = None, display_name: ''Optional[str]'' = None, group_name: ''Optional[str]''
        = None, service_principal_name: ''Optional[str]'' = None, user_name: ''Optional[str]''
        = None)'
      properties:
        all_permissions:
          type: string
          description: 'All permissions.  display_name: Optional[str] = None Display
            name of the user or service principal.'
        display_name:
          type: string
          description: 'Display name of the user or service principal.  group_name:
            Optional[str] = None name of the group'
        group_name:
          type: string
          description: 'name of the group  service_principal_name: Optional[str] =
            None Name of the service principal.'
        service_principal_name:
          type: string
          description: 'Name of the service principal.  user_name: Optional[str] =
            None name of the user'
        user_name:
          type: string
          description: 'name of the user  def as_dict(self) -> dict: Serializes the
            JobAccessControlResponse into a dictionary suitable for use as a JSON
            request body.'
    JobCluster:
      type: object
      description: 'JobCluster(job_cluster_key: ''str'', new_cluster: ''compute.ClusterSpec'')'
      properties:
        job_cluster_key:
          type: string
          description: A unique name for the job cluster. This field is required and
            must be unique within the job. `JobTaskSettings` may refer to this field
            to determine which cluster to launch for the task execution.
        new_cluster:
          type: string
          description: 'If new_cluster, a description of a cluster that is created
            for each task.  def as_dict(self) -> dict: Serializes the JobCluster into
            a dictionary suitable for use as a JSON request body.'
    JobCompliance:
      type: object
      description: 'JobCompliance(job_id: ''int'', is_compliant: ''Optional[bool]''
        = None, violations: ''Optional[Dict[str, str]]'' = None)'
      properties:
        job_id:
          type: string
          description: 'Canonical unique identifier for a job.  is_compliant: Optional[bool]
            = None Whether this job is in compliance with the latest version of its
            policy.'
        is_compliant:
          type: string
          description: 'Whether this job is in compliance with the latest version
            of its policy.  violations: Optional[Dict[str, str]] = None """An object
            containing key-value mappings representing the first 200 policy validation
            errors. The keys indicate the path where the policy validation error is
            occurring. An identifier for the job cluster is prepended to the path.
            The values indicate an error message describing the policy validation
            error.'
        violations:
          type: string
          description: An object containing key-value mappings representing the first
            200 policy validation errors. The keys indicate the path where the policy
            validation error is occurring. An identifier for the job cluster is prepended
            to the path. The values indicate an error message describing the policy
            validation error.
    JobDeployment:
      type: object
      description: 'JobDeployment(kind: ''JobDeploymentKind'', metadata_file_path:
        ''Optional[str]'' = None)'
      properties:
        kind:
          type: string
          description: 'The kind of deployment that manages the job.  * `BUNDLE`:
            The job is managed by Databricks Asset Bundle.'
        metadata_file_path:
          type: string
          description: 'Path of the file that contains deployment metadata.  def as_dict(self)
            -> dict: Serializes the JobDeployment into a dictionary suitable for use
            as a JSON request body.'
    JobDeploymentKind:
      type: string
      description: '* `BUNDLE`: The job is managed by Databricks Asset Bundle.'
      enum:
      - BUNDLE
    JobEditMode:
      type: string
      description: 'Edit mode of the job.


        * `UI_LOCKED`: The job is in a locked UI state and cannot be modified. * `EDITABLE`:
        The job is

        in an editable state and can be modified.'
      enum:
      - EDITABLE
      - UI_LOCKED
    JobEmailNotifications:
      type: object
      description: 'JobEmailNotifications(no_alert_for_skipped_runs: ''Optional[bool]''
        = None, on_duration_warning_threshold_exceeded: ''Optional[List[str]]'' =
        None, on_failure: ''Optional[List[str]]'' = None, on_start: ''Optional[List[str]]''
        = None, on_streaming_backlog_exceeded: ''Optional[List[str]]'' = None, on_success:
        ''Optional[List[str]]'' = None)'
      properties:
        no_alert_for_skipped_runs:
          type: string
          description: If true, do not send email to recipients specified in `on_failure`
            if the run is skipped. This field is `deprecated`. Please use the `notification_settings.no_alert_for_skipped_runs`
            field.
        on_duration_warning_threshold_exceeded:
          type: string
          description: A list of email addresses to be notified when the duration
            of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS`
            metric in the `health` field. If no rule for the `RUN_DURATION_SECONDS`
            metric is specified in the `health` field for the job, notifications are
            not sent.
        on_failure:
          type: string
          description: A list of email addresses to be notified when a run unsuccessfully
            completes. A run is considered to have completed unsuccessfully if it
            ends with an `INTERNAL_ERROR` `life_cycle_state` or a `FAILED`, or `TIMED_OUT`
            result_state. If this is not specified on job creation, reset, or update
            the list is empty, and notifications are not sent.
        on_start:
          type: string
          description: A list of email addresses to be notified when a run begins.
            If not specified on job creation, reset, or update, the list is empty,
            and notifications are not sent.
        on_streaming_backlog_exceeded:
          type: string
          description: 'A list of email addresses to notify when any streaming backlog
            thresholds are exceeded for any stream. Streaming backlog thresholds can
            be set in the `health` field using the following metrics: `STREAMING_BACKLOG_BYTES`,
            `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`.
            Alerting is based on the 10-minute average of these metrics. If the issue
            persists, notifications are resent every 30 minutes.'
        on_success:
          type: string
          description: A list of email addresses to be notified when a run successfully
            completes. A run is considered to have completed successfully if it ends
            with a `TERMINATED` `life_cycle_state` and a `SUCCESS` result_state. If
            not specified on job creation, reset, or update, the list is empty, and
            notifications are not sent.
    JobEnvironment:
      type: object
      description: 'JobEnvironment(environment_key: ''str'', spec: ''Optional[compute.Environment]''
        = None)'
      properties:
        environment_key:
          type: string
          description: 'The key of an environment. It has to be unique within a job.  spec:
            Optional[compute.Environment] = None  def as_dict(self) -> dict: Serializes
            the JobEnvironment into a dictionary suitable for use as a JSON request
            body.'
        spec:
          type: string
          description: ''
    JobNotificationSettings:
      type: object
      description: 'JobNotificationSettings(no_alert_for_canceled_runs: ''Optional[bool]''
        = None, no_alert_for_skipped_runs: ''Optional[bool]'' = None)'
      properties:
        no_alert_for_canceled_runs:
          type: string
          description: If true, do not send notifications to recipients specified
            in `on_failure` if the run is canceled.
        no_alert_for_skipped_runs:
          type: string
          description: If true, do not send notifications to recipients specified
            in `on_failure` if the run is skipped.
    JobParameter:
      type: object
      description: 'JobParameter(default: ''Optional[str]'' = None, name: ''Optional[str]''
        = None, value: ''Optional[str]'' = None)'
      properties:
        default:
          type: string
          description: 'The optional default value of the parameter  name: Optional[str]
            = None The name of the parameter'
        name:
          type: string
          description: 'The name of the parameter  value: Optional[str] = None The
            value used in the run'
        value:
          type: string
          description: 'The value used in the run  def as_dict(self) -> dict: Serializes
            the JobParameter into a dictionary suitable for use as a JSON request
            body.'
    JobParameterDefinition:
      type: object
      description: 'JobParameterDefinition(name: ''str'', default: ''str'')'
      properties:
        name:
          type: string
          description: 'The name of the defined parameter. May only contain alphanumeric
            characters, `_`, `-`, and `.`  default: str Default value of the parameter.'
        default:
          type: string
          description: 'Default value of the parameter.  def as_dict(self) -> dict:
            Serializes the JobParameterDefinition into a dictionary suitable for use
            as a JSON request body.'
    JobPermission:
      type: object
      description: 'JobPermission(inherited: ''Optional[bool]'' = None, inherited_from_object:
        ''Optional[List[str]]'' = None, permission_level: ''Optional[JobPermissionLevel]''
        = None)'
      properties:
        inherited:
          type: string
          description: ''
        inherited_from_object:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    JobPermissionLevel:
      type: string
      description: Permission level
      enum:
      - CAN_MANAGE
      - CAN_MANAGE_RUN
      - CAN_VIEW
      - IS_OWNER
    JobPermissions:
      type: object
      description: 'JobPermissions(access_control_list: ''Optional[List[JobAccessControlResponse]]''
        = None, object_id: ''Optional[str]'' = None, object_type: ''Optional[str]''
        = None)'
      properties:
        access_control_list:
          type: string
          description: ''
        object_id:
          type: string
          description: ''
        object_type:
          type: string
          description: ''
    JobPermissionsDescription:
      type: object
      description: 'JobPermissionsDescription(description: ''Optional[str]'' = None,
        permission_level: ''Optional[JobPermissionLevel]'' = None)'
      properties:
        description:
          type: string
          description: ''
        permission_level:
          type: string
          description: ''
    JobRunAs:
      type: object
      description: 'Write-only setting. Specifies the user or service principal that
        the job runs as. If not

        specified, the job runs as the user who created the job.


        Either `user_name` or `service_principal_name` should be specified. If not,
        an error is thrown.'
      properties:
        service_principal_name:
          type: string
          description: Application ID of an active service principal. Setting this
            field requires the `servicePrincipal/user` role.
        user_name:
          type: string
          description: The email of an active workspace user. Non-admin users can
            only set this field to their own email.
    JobSettings:
      type: object
      description: 'JobSettings(budget_policy_id: ''Optional[str]'' = None, continuous:
        ''Optional[Continuous]'' = None, deployment: ''Optional[JobDeployment]'' =
        None, description: ''Optional[str]'' = None, edit_mode: ''Optional[JobEditMode]''
        = None, email_notifications: ''Optional[JobEmailNotifications]'' = None, environments:
        ''Optional[List[JobEnvironment]]'' = None, format: ''Optional[Format]'' =
        None, git_source: ''Optional[GitSource]'' = None, health: ''Optional[JobsHealthRules]''
        = None, job_clusters: ''Optional[List[JobCluster]]'' = None, max_concurrent_runs:
        ''Optional[int]'' = None, name: ''Optional[str]'' = None, notification_settings:
        ''Optional[JobNotificationSettings]'' = None, parameters: ''Optional[List[JobParameterDefinition]]''
        = None, performance_target: ''Optional[PerformanceTarget]'' = None, queue:
        ''Optional[QueueSettings]'' = None, run_as: ''Optional[JobRunAs]'' = None,
        schedule: ''Optional[CronSchedule]'' = None, tags: ''Optional[Dict[str, str]]''
        = None, tasks: ''Optional[List[Task]]'' = None, timeout_seconds: ''Optional[int]''
        = None, trigger: ''Optional[TriggerSettings]'' = None, usage_policy_id: ''Optional[str]''
        = None, webhook_notifications: ''Optional[WebhookNotifications]'' = None)'
      properties:
        budget_policy_id:
          type: string
          description: The id of the user specified budget policy to use for this
            job. If not specified, a default budget policy may be applied when creating
            or modifying the job. See `effective_budget_policy_id` for the budget
            policy used by this workload.
        continuous:
          type: string
          description: An optional continuous property for this job. The continuous
            property will ensure that there is always one run executing. Only one
            of `schedule` and `continuous` can be used.
        deployment:
          type: string
          description: 'Deployment information for jobs managed by external sources.  description:
            Optional[str] = None An optional description for the job. The maximum
            length is 27700 characters in UTF-8 encoding.'
        description:
          type: string
          description: 'An optional description for the job. The maximum length is
            27700 characters in UTF-8 encoding.  edit_mode: Optional[JobEditMode]
            = None """Edit mode of the job.  * `UI_LOCKED`: The job is in a locked
            UI state and cannot be modified. * `EDITABLE`: The job is in an editable
            state and can be modified.'
        edit_mode:
          type: string
          description: 'Edit mode of the job.  * `UI_LOCKED`: The job is in a locked
            UI state and cannot be modified. * `EDITABLE`: The job is in an editable
            state and can be modified.'
        email_notifications:
          type: string
          description: An optional set of email addresses that is notified when runs
            of this job begin or complete as well as when this job is deleted.
        environments:
          type: string
          description: A list of task execution environment specifications that can
            be referenced by serverless tasks of this job. An environment is required
            to be present for serverless tasks. For serverless notebook tasks, the
            environment is accessible in the notebook environment panel. For other
            serverless tasks, the task environment is required to be specified using
            environment_key in the task settings.
        format:
          type: string
          description: Used to tell what is the format of the job. This field is ignored
            in Create/Update/Reset calls. When using the Jobs API 2.1 this value is
            always set to `"MULTI_TASK"`.
        git_source:
          type: string
          description: 'An optional specification for a remote Git repository containing
            the source code used by tasks. Version-controlled source code is supported
            by notebook, dbt, Python script, and SQL File tasks.  If `git_source`
            is set, these tasks retrieve the file from the remote repository by default.
            However, this behavior can be overridden by setting `source` to `WORKSPACE`
            on the task.  Note: dbt and SQL File tasks support only version-controlled
            sources. If dbt or SQL File tasks are used, `git_source` must be defined
            on the job.'
        health:
          type: string
          description: ''
        job_clusters:
          type: string
          description: A list of job cluster specifications that can be shared and
            reused by tasks of this job. Libraries cannot be declared in a shared
            job cluster. You must declare dependent libraries in task settings.
        max_concurrent_runs:
          type: string
          description: An optional maximum allowed number of concurrent runs of the
            job. Set this value if you want to be able to execute multiple runs of
            the same job concurrently. This is useful for example if you trigger your
            job on a frequent schedule and want to allow consecutive runs to overlap
            with each other, or if you want to trigger multiple runs which differ
            by their input parameters. This setting affects only new runs. For example,
            suppose the job’s concurrency is 4 and there are 4 concurrent active runs.
            Then setting the concurrency to 3 won’t kill any of the active runs. However,
            from then on, new runs are skipped unless there are fewer than 3 active
            runs. This value cannot exceed 1000. Setting this value to `0` causes
            all new runs to be skipped.
        name:
          type: string
          description: 'An optional name for the job. The maximum length is 4096 bytes
            in UTF-8 encoding.  notification_settings: Optional[JobNotificationSettings]
            = None """Optional notification settings that are used when sending notifications
            to each of the `email_notifications` and `webhook_notifications` for this
            job.'
        notification_settings:
          type: string
          description: Optional notification settings that are used when sending notifications
            to each of the `email_notifications` and `webhook_notifications` for this
            job.
        parameters:
          type: string
          description: 'Job-level parameter definitions  performance_target: Optional[PerformanceTarget]
            = None """The performance mode on a serverless job. This field determines
            the level of compute performance or cost-efficiency for the run.  * `STANDARD`:
            Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:
            Prioritizes fast startup and execution times through rapid scaling and
            optimized cluster performance.'
        performance_target:
          type: string
          description: 'The performance mode on a serverless job. This field determines
            the level of compute performance or cost-efficiency for the run.  * `STANDARD`:
            Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:
            Prioritizes fast startup and execution times through rapid scaling and
            optimized cluster performance.'
        queue:
          type: string
          description: 'The queue settings of the job.  run_as: Optional[JobRunAs]
            = None """The user or service principal that the job runs as, if specified
            in the request. This field indicates the explicit configuration of `run_as`
            for the job. To find the value in all cases, explicit or implicit, use
            `run_as_user_name`.'
        run_as:
          type: string
          description: The user or service principal that the job runs as, if specified
            in the request. This field indicates the explicit configuration of `run_as`
            for the job. To find the value in all cases, explicit or implicit, use
            `run_as_user_name`.
        schedule:
          type: string
          description: An optional periodic schedule for this job. The default behavior
            is that the job only runs when triggered by clicking “Run Now” in the
            Jobs UI or sending an API request to `runNow`.
        tags:
          type: string
          description: A map of tags associated with the job. These are forwarded
            to the cluster as cluster tags for jobs clusters, and are subject to the
            same limitations as cluster tags. A maximum of 25 tags can be added to
            the job.
        tasks:
          type: string
          description: A list of task specifications to be executed by this job. It
            supports up to 1000 elements in write endpoints (:method:jobs/create,
            :method:jobs/reset, :method:jobs/update, :method:jobs/submit). Read endpoints
            return only 100 tasks. If more than 100 tasks are available, you can paginate
            through them using :method:jobs/get. Use the `next_page_token` field at
            the object root to determine if more results are available.
        timeout_seconds:
          type: string
          description: 'An optional timeout applied to each run of this job. A value
            of `0` means no timeout.  trigger: Optional[TriggerSettings] = None """A
            configuration to trigger a run when certain conditions are met. The default
            behavior is that the job runs only when triggered by clicking “Run Now”
            in the Jobs UI or sending an API request to `runNow`.'
        trigger:
          type: string
          description: A configuration to trigger a run when certain conditions are
            met. The default behavior is that the job runs only when triggered by
            clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.
        usage_policy_id:
          type: string
          description: The id of the user specified usage policy to use for this job.
            If not specified, a default usage policy may be applied when creating
            or modifying the job. See `effective_usage_policy_id` for the usage policy
            used by this workload.
        webhook_notifications:
          type: string
          description: 'A collection of system notification IDs to notify when runs
            of this job begin or complete.  def as_dict(self) -> dict: Serializes
            the JobSettings into a dictionary suitable for use as a JSON request body.'
    JobSource:
      type: object
      description: The source of the job specification in the remote repository when
        the job is source controlled.
      properties:
        job_config_path:
          type: string
          description: 'Path of the job YAML file that contains the job specification.  import_from_git_branch:
            str Name of the branch which the job is imported from.'
        import_from_git_branch:
          type: string
          description: 'Name of the branch which the job is imported from.  dirty_state:
            Optional[JobSourceDirtyState] = None """Dirty state indicates the job
            is not fully synced with the job specification in the remote repository.  Possible
            values are: * `NOT_SYNCED`: The job is not yet synced with the remote
            job specification. Import the remote job specification from UI to make
            the job fully synced. * `DISCONNECTED`: The job is temporary disconnected
            from the remote job specification and is allowed for live edit. Import
            the remote job specification again from UI to make the job fully synced.'
        dirty_state:
          type: string
          description: 'Dirty state indicates the job is not fully synced with the
            job specification in the remote repository.  Possible values are: * `NOT_SYNCED`:
            The job is not yet synced with the remote job specification. Import the
            remote job specification from UI to make the job fully synced. * `DISCONNECTED`:
            The job is temporary disconnected from the remote job specification and
            is allowed for live edit. Import the remote job specification again from
            UI to make the job fully synced.'
    JobSourceDirtyState:
      type: string
      description: 'Dirty state indicates the job is not fully synced with the job
        specification in the remote

        repository.


        Possible values are: * `NOT_SYNCED`: The job is not yet synced with the remote
        job

        specification. Import the remote job specification from UI to make the job
        fully synced. *

        `DISCONNECTED`: The job is temporary disconnected from the remote job specification
        and is

        allowed for live edit. Import the remote job specification again from UI to
        make the job fully

        synced.'
      enum:
      - DISCONNECTED
      - NOT_SYNCED
    JobsHealthMetric:
      type: string
      description: 'Specifies the health metric that is being evaluated for a particular
        health rule.


        * `RUN_DURATION_SECONDS`: Expected total time for a run in seconds. * `STREAMING_BACKLOG_BYTES`:

        An estimate of the maximum bytes of data waiting to be consumed across all
        streams. This metric

        is in Public Preview. * `STREAMING_BACKLOG_RECORDS`: An estimate of the maximum
        offset lag

        across all streams. This metric is in Public Preview. * `STREAMING_BACKLOG_SECONDS`:
        An estimate

        of the maximum consumer delay across all streams. This metric is in Public
        Preview. *

        `STREAMING_BACKLOG_FILES`: An estimate of the maximum number of outstanding
        files across all

        streams. This metric is in Public Preview.'
      enum:
      - RUN_DURATION_SECONDS
      - STREAMING_BACKLOG_BYTES
      - STREAMING_BACKLOG_FILES
      - STREAMING_BACKLOG_RECORDS
      - STREAMING_BACKLOG_SECONDS
    JobsHealthOperator:
      type: string
      description: Specifies the operator used to compare the health metric value
        with the specified threshold.
      enum:
      - GREATER_THAN
    JobsHealthRule:
      type: object
      description: 'JobsHealthRule(metric: ''JobsHealthMetric'', op: ''JobsHealthOperator'',
        value: ''int'')'
      properties:
        metric:
          type: string
          description: ''
        op:
          type: string
          description: ''
        value:
          type: string
          description: 'Specifies the threshold value that the health metric should
            obey to satisfy the health rule.  def as_dict(self) -> dict: Serializes
            the JobsHealthRule into a dictionary suitable for use as a JSON request
            body.'
    JobsHealthRules:
      type: object
      description: An optional set of health rules that can be defined for this job.
      properties:
        rules:
          type: string
          description: ''
    ListJobComplianceForPolicyResponse:
      type: object
      description: 'ListJobComplianceForPolicyResponse(jobs: ''Optional[List[JobCompliance]]''
        = None, next_page_token: ''Optional[str]'' = None, prev_page_token: ''Optional[str]''
        = None)'
      properties:
        jobs:
          type: string
          description: 'A list of jobs and their policy compliance statuses.  next_page_token:
            Optional[str] = None """This field represents the pagination token to
            retrieve the next page of results. If this field is not in the response,
            it means no further results for the request.'
        next_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            next page of results. If this field is not in the response, it means no
            further results for the request.
        prev_page_token:
          type: string
          description: This field represents the pagination token to retrieve the
            previous page of results. If this field is not in the response, it means
            no further results for the request.
    ListJobsResponse:
      type: object
      description: List of jobs was retrieved successfully.
      properties:
        has_more:
          type: string
          description: 'If true, additional jobs matching the provided filter are
            available for listing.  jobs: Optional[List[BaseJob]] = None The list
            of jobs. Only included in the response if there are jobs to list.'
        jobs:
          type: string
          description: 'The list of jobs. Only included in the response if there are
            jobs to list.  next_page_token: Optional[str] = None A token that can
            be used to list the next page of jobs (if applicable).'
        next_page_token:
          type: string
          description: 'A token that can be used to list the next page of jobs (if
            applicable).  prev_page_token: Optional[str] = None A token that can be
            used to list the previous page of jobs (if applicable).'
        prev_page_token:
          type: string
          description: 'A token that can be used to list the previous page of jobs
            (if applicable).  def as_dict(self) -> dict: Serializes the ListJobsResponse
            into a dictionary suitable for use as a JSON request body.'
    ListRunsResponse:
      type: object
      description: List of runs was retrieved successfully.
      properties:
        has_more:
          type: string
          description: 'If true, additional runs matching the provided filter are
            available for listing.  next_page_token: Optional[str] = None A token
            that can be used to list the next page of runs (if applicable).'
        next_page_token:
          type: string
          description: 'A token that can be used to list the next page of runs (if
            applicable).  prev_page_token: Optional[str] = None A token that can be
            used to list the previous page of runs (if applicable).'
        prev_page_token:
          type: string
          description: 'A token that can be used to list the previous page of runs
            (if applicable).  runs: Optional[List[BaseRun]] = None """A list of runs,
            from most recently started to least. Only included in the response if
            there are runs to list.'
        runs:
          type: string
          description: A list of runs, from most recently started to least. Only included
            in the response if there are runs to list.
    NotebookOutput:
      type: object
      description: 'NotebookOutput(result: ''Optional[str]'' = None, truncated: ''Optional[bool]''
        = None)'
      properties:
        result:
          type: string
          description: The value passed to [dbutils.notebook.exit()](/notebooks/notebook-workflows.html#notebook-workflows-exit).
            Databricks restricts this API to return the first 5 MB of the value. For
            a larger result, your job can store the results in a cloud storage service.
            This field is absent if `dbutils.notebook.exit()` was never called.
        truncated:
          type: string
          description: 'Whether or not the result was truncated.  def as_dict(self)
            -> dict: Serializes the NotebookOutput into a dictionary suitable for
            use as a JSON request body.'
    NotebookTask:
      type: object
      description: 'NotebookTask(notebook_path: ''str'', base_parameters: ''Optional[Dict[str,
        str]]'' = None, source: ''Optional[Source]'' = None, warehouse_id: ''Optional[str]''
        = None)'
      properties:
        notebook_path:
          type: string
          description: The path of the notebook to be run in the Databricks workspace
            or remote repository. For notebooks stored in the Databricks workspace,
            the path must be absolute and begin with a slash. For notebooks stored
            in a remote repository, the path must be relative. This field is required.
        base_parameters:
          type: string
          description: 'Base parameters to be used for each run of this job. If the
            run is initiated by a call to :method:jobs/run Now with parameters specified,
            the two parameters maps are merged. If the same key is specified in `base_parameters`
            and in `run-now`, the value from `run-now` is used. Use [Task parameter
            variables] to set parameters containing information about job runs.  If
            the notebook takes a parameter that is not specified in the job’s `base_parameters`
            or the `run-now` override parameters, the default value from the notebook
            is used.  Retrieve these parameters in a notebook using [dbutils.widgets.get].  The
            JSON representation of this field cannot exceed 1MB.  [Task parameter
            variables]: https://docs.databricks.com/jobs.html#parameter-variables
            [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-widgets'
        source:
          type: string
          description: 'Optional location type of the notebook. When set to `WORKSPACE`,
            the notebook will be retrieved from the local Databricks workspace. When
            set to `GIT`, the notebook will be retrieved from a Git repository defined
            in `git_source`. If the value is empty, the task will use `GIT` if `git_source`
            is defined and `WORKSPACE` otherwise. * `WORKSPACE`: Notebook is located
            in Databricks workspace. * `GIT`: Notebook is located in cloud Git provider.'
        warehouse_id:
          type: string
          description: Optional `warehouse_id` to run the notebook on a SQL warehouse.
            Classic SQL warehouses are NOT supported, please use serverless or pro
            SQL warehouses.  Note that SQL warehouses only support SQL cells; if the
            notebook contains non-SQL cells, the run will fail.
    OutputSchemaInfo:
      type: object
      description: 'Stores the catalog name, schema name, and the output schema expiration
        time for the clean room

        run.'
      properties:
        catalog_name:
          type: string
          description: ''
        expiration_time:
          type: string
          description: 'The expiration time for the output schema as a Unix timestamp
            in milliseconds.  schema_name: Optional[str] = None  def as_dict(self)
            -> dict: Serializes the OutputSchemaInfo into a dictionary suitable for
            use as a JSON request body.'
        schema_name:
          type: string
          description: ''
    PauseStatus:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - PAUSED
      - UNPAUSED
    PerformanceTarget:
      type: string
      description: 'PerformanceTarget defines how performant (lower latency) or cost
        efficient the execution of run

        on serverless compute should be. The performance mode on the job or pipeline
        should map to a

        performance setting that is passed to Cluster Manager (see cluster-common
        PerformanceTarget).'
      enum:
      - PERFORMANCE_OPTIMIZED
      - STANDARD
    PeriodicTriggerConfiguration:
      type: object
      description: 'PeriodicTriggerConfiguration(interval: ''int'', unit: ''PeriodicTriggerConfigurationTimeUnit'')'
      properties:
        interval:
          type: string
          description: 'The interval at which the trigger should run.  unit: PeriodicTriggerConfigurationTimeUnit
            The unit of time for the interval.'
        unit:
          type: string
          description: 'The unit of time for the interval.  def as_dict(self) -> dict:
            Serializes the PeriodicTriggerConfiguration into a dictionary suitable
            for use as a JSON request body.'
    PeriodicTriggerConfigurationTimeUnit:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - DAYS
      - HOURS
      - WEEKS
    PipelineParams:
      type: object
      description: 'PipelineParams(full_refresh: ''Optional[bool]'' = None)'
      properties:
        full_refresh:
          type: string
          description: 'If true, triggers a full refresh on the delta live table.  def
            as_dict(self) -> dict: Serializes the PipelineParams into a dictionary
            suitable for use as a JSON request body.'
    PipelineTask:
      type: object
      description: 'PipelineTask(pipeline_id: ''str'', full_refresh: ''Optional[bool]''
        = None)'
      properties:
        pipeline_id:
          type: string
          description: 'The full name of the pipeline task to execute.  full_refresh:
            Optional[bool] = None If true, triggers a full refresh on the delta live
            table.'
        full_refresh:
          type: string
          description: 'If true, triggers a full refresh on the delta live table.  def
            as_dict(self) -> dict: Serializes the PipelineTask into a dictionary suitable
            for use as a JSON request body.'
    PowerBiModel:
      type: object
      description: 'PowerBiModel(authentication_method: ''Optional[AuthenticationMethod]''
        = None, model_name: ''Optional[str]'' = None, overwrite_existing: ''Optional[bool]''
        = None, storage_mode: ''Optional[StorageMode]'' = None, workspace_name: ''Optional[str]''
        = None)'
      properties:
        authentication_method:
          type: string
          description: 'How the published Power BI model authenticates to Databricks  model_name:
            Optional[str] = None The name of the Power BI model'
        model_name:
          type: string
          description: 'The name of the Power BI model  overwrite_existing: Optional[bool]
            = None Whether to overwrite existing Power BI models'
        overwrite_existing:
          type: string
          description: 'Whether to overwrite existing Power BI models  storage_mode:
            Optional[StorageMode] = None The default storage mode of the Power BI
            model'
        storage_mode:
          type: string
          description: 'The default storage mode of the Power BI model  workspace_name:
            Optional[str] = None The name of the Power BI workspace of the model'
        workspace_name:
          type: string
          description: 'The name of the Power BI workspace of the model  def as_dict(self)
            -> dict: Serializes the PowerBiModel into a dictionary suitable for use
            as a JSON request body.'
    PowerBiTable:
      type: object
      description: 'PowerBiTable(catalog: ''Optional[str]'' = None, name: ''Optional[str]''
        = None, schema: ''Optional[str]'' = None, storage_mode: ''Optional[StorageMode]''
        = None)'
      properties:
        catalog:
          type: string
          description: 'The catalog name in Databricks  name: Optional[str] = None
            The table name in Databricks'
        name:
          type: string
          description: 'The table name in Databricks  schema: Optional[str] = None
            The schema name in Databricks'
        schema:
          type: string
          description: 'The schema name in Databricks  storage_mode: Optional[StorageMode]
            = None The Power BI storage mode of the table'
        storage_mode:
          type: string
          description: 'The Power BI storage mode of the table  def as_dict(self)
            -> dict: Serializes the PowerBiTable into a dictionary suitable for use
            as a JSON request body.'
    PowerBiTask:
      type: object
      description: 'PowerBiTask(connection_resource_name: ''Optional[str]'' = None,
        power_bi_model: ''Optional[PowerBiModel]'' = None, refresh_after_update: ''Optional[bool]''
        = None, tables: ''Optional[List[PowerBiTable]]'' = None, warehouse_id: ''Optional[str]''
        = None)'
      properties:
        connection_resource_name:
          type: string
          description: 'The resource name of the UC connection to authenticate from
            Databricks to Power BI  power_bi_model: Optional[PowerBiModel] = None
            The semantic model to update'
        power_bi_model:
          type: string
          description: 'The semantic model to update  refresh_after_update: Optional[bool]
            = None Whether the model should be refreshed after the update'
        refresh_after_update:
          type: string
          description: 'Whether the model should be refreshed after the update  tables:
            Optional[List[PowerBiTable]] = None The tables to be exported to Power
            BI'
        tables:
          type: string
          description: 'The tables to be exported to Power BI  warehouse_id: Optional[str]
            = None The SQL warehouse ID to use as the Power BI data source'
        warehouse_id:
          type: string
          description: 'The SQL warehouse ID to use as the Power BI data source  def
            as_dict(self) -> dict: Serializes the PowerBiTask into a dictionary suitable
            for use as a JSON request body.'
    PythonWheelTask:
      type: object
      description: 'PythonWheelTask(package_name: ''str'', entry_point: ''str'', named_parameters:
        ''Optional[Dict[str, str]]'' = None, parameters: ''Optional[List[str]]'' =
        None)'
      properties:
        package_name:
          type: string
          description: 'Name of the package to execute  entry_point: str """Named
            entry point to use, if it does not exist in the metadata of the package
            it executes the function from the package directly using `$packageName.$entryPoint()`'
        entry_point:
          type: string
          description: Named entry point to use, if it does not exist in the metadata
            of the package it executes the function from the package directly using
            `$packageName.$entryPoint()`
        named_parameters:
          type: string
          description: Command-line parameters passed to Python wheel task in the
            form of `["--name=task", --data=dbfs:/path/to/data.json"]`. Leave it empty
            if `parameters` is not null.
        parameters:
          type: string
          description: Command-line parameters passed to Python wheel task in the
            form of `["--name=task", --data=dbfs:/path/to/data.json"]`. Leave it empty
            if `parameters` is not null.
    QueueDetails:
      type: object
      description: 'QueueDetails(code: ''Optional[QueueDetailsCodeCode]'' = None,
        message: ''Optional[str]'' = None)'
      properties:
        code:
          type: string
          description: ''
        message:
          type: string
          description: A descriptive message with the queuing details. This field
            is unstructured, and its exact format is subject to change.
    QueueDetailsCodeCode:
      type: string
      description: 'The reason for queuing the run. * `ACTIVE_RUNS_LIMIT_REACHED`:
        The run was queued due to

        reaching the workspace limit of active task runs. * `MAX_CONCURRENT_RUNS_REACHED`:
        The run was

        queued due to reaching the per-job limit of concurrent job runs. *

        `ACTIVE_RUN_JOB_TASKS_LIMIT_REACHED`: The run was queued due to reaching the
        workspace limit of

        active run job tasks.'
      enum:
      - ACTIVE_RUNS_LIMIT_REACHED
      - ACTIVE_RUN_JOB_TASKS_LIMIT_REACHED
      - MAX_CONCURRENT_RUNS_REACHED
    QueueSettings:
      type: object
      description: 'QueueSettings(enabled: ''bool'')'
      properties:
        enabled:
          type: string
          description: 'If true, enable queueing for the job. This is a required field.  def
            as_dict(self) -> dict: Serializes the QueueSettings into a dictionary
            suitable for use as a JSON request body.'
    RepairHistoryItem:
      type: object
      description: 'RepairHistoryItem(effective_performance_target: ''Optional[PerformanceTarget]''
        = None, end_time: ''Optional[int]'' = None, id: ''Optional[int]'' = None,
        start_time: ''Optional[int]'' = None, state: ''Optional[RunState]'' = None,
        status: ''Optional[RunStatus]'' = None, task_run_ids: ''Optional[List[int]]''
        = None, type: ''Optional[RepairHistoryItemType]'' = None)'
      properties:
        effective_performance_target:
          type: string
          description: 'The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on the
            request depending on whether the performance mode is supported by the
            job type.  * `STANDARD`: Enables cost-efficient execution of serverless
            workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
            times through rapid scaling and optimized cluster performance.'
        end_time:
          type: string
          description: 'The end time of the (repaired) run.  id: Optional[int] = None
            The ID of the repair. Only returned for the items that represent a repair
            in `repair_history`.'
        id:
          type: string
          description: 'The ID of the repair. Only returned for the items that represent
            a repair in `repair_history`.  start_time: Optional[int] = None The start
            time of the (repaired) run.'
        start_time:
          type: string
          description: 'The start time of the (repaired) run.  state: Optional[RunState]
            = None Deprecated. Please use the `status` field instead.'
        state:
          type: string
          description: 'Deprecated. Please use the `status` field instead.  status:
            Optional[RunStatus] = None  task_run_ids: Optional[List[int]] = None The
            run IDs of the task runs that ran as part of this repair history item.'
        status:
          type: string
          description: ''
        task_run_ids:
          type: string
          description: 'The run IDs of the task runs that ran as part of this repair
            history item.  type: Optional[RepairHistoryItemType] = None The repair
            history item type. Indicates whether a run is the original run or a repair
            run.'
        type:
          type: string
          description: 'The repair history item type. Indicates whether a run is the
            original run or a repair run.  def as_dict(self) -> dict: Serializes the
            RepairHistoryItem into a dictionary suitable for use as a JSON request
            body.'
    RepairHistoryItemType:
      type: string
      description: The repair history item type. Indicates whether a run is the original
        run or a repair run.
      enum:
      - ORIGINAL
      - REPAIR
    RepairRunResponse:
      type: object
      description: Run repair was initiated.
      properties:
        repair_id:
          type: string
          description: The ID of the repair. Must be provided in subsequent repairs
            using the `latest_repair_id` field to ensure sequential repairs.
    ResetResponse:
      type: object
      description: ResetResponse()
      properties: {}
    ResolvedConditionTaskValues:
      type: object
      description: 'ResolvedConditionTaskValues(left: ''Optional[str]'' = None, right:
        ''Optional[str]'' = None)'
      properties:
        left:
          type: string
          description: ''
        right:
          type: string
          description: ''
    ResolvedDbtTaskValues:
      type: object
      description: 'ResolvedDbtTaskValues(commands: ''Optional[List[str]]'' = None)'
      properties:
        commands:
          type: string
          description: ''
    ResolvedNotebookTaskValues:
      type: object
      description: 'ResolvedNotebookTaskValues(base_parameters: ''Optional[Dict[str,
        str]]'' = None)'
      properties:
        base_parameters:
          type: string
          description: ''
    ResolvedParamPairValues:
      type: object
      description: 'ResolvedParamPairValues(parameters: ''Optional[Dict[str, str]]''
        = None)'
      properties:
        parameters:
          type: string
          description: ''
    ResolvedPythonWheelTaskValues:
      type: object
      description: 'ResolvedPythonWheelTaskValues(named_parameters: ''Optional[Dict[str,
        str]]'' = None, parameters: ''Optional[List[str]]'' = None)'
      properties:
        named_parameters:
          type: string
          description: ''
        parameters:
          type: string
          description: ''
    ResolvedRunJobTaskValues:
      type: object
      description: 'ResolvedRunJobTaskValues(job_parameters: ''Optional[Dict[str,
        str]]'' = None, parameters: ''Optional[Dict[str, str]]'' = None)'
      properties:
        job_parameters:
          type: string
          description: ''
        parameters:
          type: string
          description: ''
    ResolvedStringParamsValues:
      type: object
      description: 'ResolvedStringParamsValues(parameters: ''Optional[List[str]]''
        = None)'
      properties:
        parameters:
          type: string
          description: ''
    ResolvedValues:
      type: object
      description: 'ResolvedValues(condition_task: ''Optional[ResolvedConditionTaskValues]''
        = None, dbt_task: ''Optional[ResolvedDbtTaskValues]'' = None, notebook_task:
        ''Optional[ResolvedNotebookTaskValues]'' = None, python_wheel_task: ''Optional[ResolvedPythonWheelTaskValues]''
        = None, run_job_task: ''Optional[ResolvedRunJobTaskValues]'' = None, simulation_task:
        ''Optional[ResolvedParamPairValues]'' = None, spark_jar_task: ''Optional[ResolvedStringParamsValues]''
        = None, spark_python_task: ''Optional[ResolvedStringParamsValues]'' = None,
        spark_submit_task: ''Optional[ResolvedStringParamsValues]'' = None, sql_task:
        ''Optional[ResolvedParamPairValues]'' = None)'
      properties:
        condition_task:
          type: string
          description: ''
        dbt_task:
          type: string
          description: ''
        notebook_task:
          type: string
          description: ''
        python_wheel_task:
          type: string
          description: ''
        run_job_task:
          type: string
          description: ''
        simulation_task:
          type: string
          description: ''
        spark_jar_task:
          type: string
          description: ''
        spark_python_task:
          type: string
          description: ''
        spark_submit_task:
          type: string
          description: ''
        sql_task:
          type: string
          description: ''
    Run:
      type: object
      description: Run was retrieved successfully
      properties:
        attempt_number:
          type: string
          description: The sequence number of this run attempt for a triggered job
            run. The initial attempt of a run has an attempt_number of 0. If the initial
            run attempt fails, and the job has a retry policy (`max_retries` > 0),
            subsequent runs are created with an `original_attempt_run_id` of the original
            attempt’s ID and an incrementing `attempt_number`. Runs are retried only
            until they succeed, and the maximum `attempt_number` is the same as the
            `max_retries` value for the job.
        cleanup_duration:
          type: string
          description: The time in milliseconds it took to terminate the cluster and
            clean up any associated artifacts. The duration of a task run is the sum
            of the `setup_duration`, `execution_duration`, and the `cleanup_duration`.
            The `cleanup_duration` field is set to 0 for multitask job runs. The total
            duration of a multitask job run is the value of the `run_duration` field.
        cluster_instance:
          type: string
          description: The cluster used for this run. If the run is specified to use
            a new cluster, this field is set once the Jobs service has requested a
            cluster for the run.
        cluster_spec:
          type: string
          description: 'A snapshot of the job’s cluster specification when this run
            was created.  creator_user_name: Optional[str] = None """The creator user
            name. This field won’t be included in the response if the user has already
            been deleted.'
        creator_user_name:
          type: string
          description: The creator user name. This field won’t be included in the
            response if the user has already been deleted.
        description:
          type: string
          description: 'Description of the run  effective_performance_target: Optional[PerformanceTarget]
            = None """The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on the
            request depending on whether the performance mode is supported by the
            job type.  * `STANDARD`: Enables cost-efficient execution of serverless
            workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
            times through rapid scaling and optimized cluster performance.'
        effective_performance_target:
          type: string
          description: 'The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on the
            request depending on whether the performance mode is supported by the
            job type.  * `STANDARD`: Enables cost-efficient execution of serverless
            workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
            times through rapid scaling and optimized cluster performance.'
        effective_usage_policy_id:
          type: string
          description: 'The id of the usage policy used by this run for cost attribution
            purposes.  end_time: Optional[int] = None """The time at which this run
            ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field
            is set to 0 if the job is still running.'
        end_time:
          type: string
          description: The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still running.
        execution_duration:
          type: string
          description: The time in milliseconds it took to execute the commands in
            the JAR or notebook until they completed, failed, timed out, were cancelled,
            or encountered an unexpected error. The duration of a task run is the
            sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`.
            The `execution_duration` field is set to 0 for multitask job runs. The
            total duration of a multitask job run is the value of the `run_duration`
            field.
        git_source:
          type: string
          description: 'An optional specification for a remote Git repository containing
            the source code used by tasks. Version-controlled source code is supported
            by notebook, dbt, Python script, and SQL File tasks.  If `git_source`
            is set, these tasks retrieve the file from the remote repository by default.
            However, this behavior can be overridden by setting `source` to `WORKSPACE`
            on the task.  Note: dbt and SQL File tasks support only version-controlled
            sources. If dbt or SQL File tasks are used, `git_source` must be defined
            on the job.'
        has_more:
          type: string
          description: Indicates if the run has more array properties (`tasks`, `job_clusters`)
            that are not shown. They can be accessed via :method:jobs/getrun endpoint.
            It is only relevant for API 2.2 :method:jobs/listruns requests with `expand_tasks=true`.
        iterations:
          type: string
          description: 'Only populated by for-each iterations. The parent for-each
            task is located in tasks array.  job_clusters: Optional[List[JobCluster]]
            = None """A list of job cluster specifications that can be shared and
            reused by tasks of this job. Libraries cannot be declared in a shared
            job cluster. You must declare dependent libraries in task settings. If
            more than 100 job clusters are available, you can paginate through them
            using :method:jobs/getrun.'
        job_clusters:
          type: string
          description: A list of job cluster specifications that can be shared and
            reused by tasks of this job. Libraries cannot be declared in a shared
            job cluster. You must declare dependent libraries in task settings. If
            more than 100 job clusters are available, you can paginate through them
            using :method:jobs/getrun.
        job_id:
          type: string
          description: 'The canonical identifier of the job that contains this run.  job_parameters:
            Optional[List[JobParameter]] = None Job-level parameters used in the run'
        job_parameters:
          type: string
          description: 'Job-level parameters used in the run  job_run_id: Optional[int]
            = None """ID of the job run that this run belongs to. For legacy and single-task
            job runs the field is populated with the job run ID. For task runs, the
            field is populated with the ID of the job run that the task run belongs
            to.'
        job_run_id:
          type: string
          description: ID of the job run that this run belongs to. For legacy and
            single-task job runs the field is populated with the job run ID. For task
            runs, the field is populated with the ID of the job run that the task
            run belongs to.
        next_page_token:
          type: string
          description: 'A token that can be used to list the next page of array properties.  number_in_job:
            Optional[int] = None A unique identifier for this job run. This is set
            to the same value as `run_id`.'
        number_in_job:
          type: string
          description: 'A unique identifier for this job run. This is set to the same
            value as `run_id`.  original_attempt_run_id: Optional[int] = None """If
            this run is a retry of a prior run attempt, this field contains the run_id
            of the original attempt; otherwise, it is the same as the run_id.'
        original_attempt_run_id:
          type: string
          description: If this run is a retry of a prior run attempt, this field contains
            the run_id of the original attempt; otherwise, it is the same as the run_id.
        overriding_parameters:
          type: string
          description: 'The parameters used for this run.  queue_duration: Optional[int]
            = None The time in milliseconds that the run has spent in the queue.'
        queue_duration:
          type: string
          description: 'The time in milliseconds that the run has spent in the queue.  repair_history:
            Optional[List[RepairHistoryItem]] = None The repair history of the run.'
        repair_history:
          type: string
          description: 'The repair history of the run.  run_duration: Optional[int]
            = None The time in milliseconds it took the job run and all of its repairs
            to finish.'
        run_duration:
          type: string
          description: 'The time in milliseconds it took the job run and all of its
            repairs to finish.  run_id: Optional[int] = None The canonical identifier
            of the run. This ID is unique across all runs of all jobs.'
        run_id:
          type: string
          description: ID of the job run that this run belongs to. For legacy and
            single-task job runs the field is populated with the job run ID. For task
            runs, the field is populated with the ID of the job run that the task
            run belongs to.
        run_name:
          type: string
          description: 'An optional name for the run. The maximum length is 4096 bytes
            in UTF-8 encoding.  run_page_url: Optional[str] = None The URL to the
            detail page of the run.'
        run_page_url:
          type: string
          description: 'The URL to the detail page of the run.  run_type: Optional[RunType]
            = None  schedule: Optional[CronSchedule] = None The cron schedule that
            triggered this run if it was triggered by the periodic scheduler.'
        run_type:
          type: string
          description: ''
        schedule:
          type: string
          description: 'The cron schedule that triggered this run if it was triggered
            by the periodic scheduler.  setup_duration: Optional[int] = None """The
            time in milliseconds it took to set up the cluster. For runs that run
            on new clusters this is the cluster creation time, for runs that run on
            existing clusters this time should be very short. The duration of a task
            run is the sum of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `setup_duration` field is set to 0 for multitask
            job runs. The total duration of a multitask job run is the value of the
            `run_duration` field.'
        setup_duration:
          type: string
          description: The time in milliseconds it took to set up the cluster. For
            runs that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The duration
            of a task run is the sum of the `setup_duration`, `execution_duration`,
            and the `cleanup_duration`. The `setup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the value
            of the `run_duration` field.
        start_time:
          type: string
          description: The time at which this run was started in epoch milliseconds
            (milliseconds since 1/1/1970 UTC). This may not be the time when the job
            task starts executing, for example, if the job is scheduled to run on
            a new cluster, this is the time the cluster creation call is issued.
        state:
          type: string
          description: 'Deprecated. Please use the `status` field instead.  status:
            Optional[RunStatus] = None  tasks: Optional[List[RunTask]] = None """The
            list of tasks performed by the run. Each task has its own `run_id` which
            you can use to call `JobsGetOutput` to retrieve the run resutls. If more
            than 100 tasks are available, you can paginate through them using :method:jobs/getrun.
            Use the `next_page_token` field at the object root to determine if more
            results are available.'
        status:
          type: string
          description: ''
        tasks:
          type: string
          description: The list of tasks performed by the run. Each task has its own
            `run_id` which you can use to call `JobsGetOutput` to retrieve the run
            resutls. If more than 100 tasks are available, you can paginate through
            them using :method:jobs/getrun. Use the `next_page_token` field at the
            object root to determine if more results are available.
        trigger:
          type: string
          description: ''
        trigger_info:
          type: string
          description: ''
    RunConditionTask:
      type: object
      description: 'RunConditionTask(op: ''ConditionTaskOp'', left: ''str'', right:
        ''str'', outcome: ''Optional[str]'' = None)'
      properties:
        op:
          type: string
          description: '* `EQUAL_TO`, `NOT_EQUAL` operators perform string comparison
            of their operands. This means that `“12.0” == “12”` will evaluate to `false`.
            * `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`
            operators perform numeric comparison of their operands. `“12.0” >= “12”`
            will evaluate to `true`, `“10.0” >= “12”` will evaluate to `false`.  The
            boolean comparison to task values can be implemented with operators `EQUAL_TO`,
            `NOT_EQUAL`. If a task value was set to a boolean value, it will be serialized
            to `“true”` or `“false”` for the comparison.'
        left:
          type: string
          description: The left operand of the condition task. Can be either a string
            value or a job state or parameter reference.
        right:
          type: string
          description: The right operand of the condition task. Can be either a string
            value or a job state or parameter reference.
        outcome:
          type: string
          description: The condition expression evaluation result. Filled in if the
            task was successfully completed. Can be `"true"` or `"false"`
    RunForEachTask:
      type: object
      description: 'RunForEachTask(inputs: ''str'', task: ''Task'', concurrency: ''Optional[int]''
        = None, stats: ''Optional[ForEachStats]'' = None)'
      properties:
        inputs:
          type: string
          description: 'Array for task to iterate on. This can be a JSON string or
            a reference to an array parameter.  task: Task Configuration for the task
            that will be run for each element in the array'
        task:
          type: string
          description: 'Configuration for the task that will be run for each element
            in the array  concurrency: Optional[int] = None """An optional maximum
            allowed number of concurrent runs of the task. Set this value if you want
            to be able to execute multiple runs of the task concurrently.'
        concurrency:
          type: string
          description: An optional maximum allowed number of concurrent runs of the
            task. Set this value if you want to be able to execute multiple runs of
            the task concurrently.
        stats:
          type: string
          description: Read only field. Populated for GetRun and ListRuns RPC calls
            and stores the execution stats of an For each task
    RunIf:
      type: string
      description: 'An optional value indicating the condition that determines whether
        the task should be run once

        its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`.


        Possible values are: * `ALL_SUCCESS`: All dependencies have executed and succeeded
        *

        `AT_LEAST_ONE_SUCCESS`: At least one dependency has succeeded * `NONE_FAILED`:
        None of the

        dependencies have failed and at least one was executed * `ALL_DONE`: All dependencies
        have been

        completed * `AT_LEAST_ONE_FAILED`: At least one dependency failed * `ALL_FAILED`:
        ALl

        dependencies have failed'
      enum:
      - ALL_DONE
      - ALL_FAILED
      - ALL_SUCCESS
      - AT_LEAST_ONE_FAILED
      - AT_LEAST_ONE_SUCCESS
      - NONE_FAILED
    RunJobOutput:
      type: object
      description: 'RunJobOutput(run_id: ''Optional[int]'' = None)'
      properties:
        run_id:
          type: string
          description: 'The run id of the triggered job run  def as_dict(self) ->
            dict: Serializes the RunJobOutput into a dictionary suitable for use as
            a JSON request body.'
    RunJobTask:
      type: object
      description: 'RunJobTask(job_id: ''int'', dbt_commands: ''Optional[List[str]]''
        = None, jar_params: ''Optional[List[str]]'' = None, job_parameters: ''Optional[Dict[str,
        str]]'' = None, notebook_params: ''Optional[Dict[str, str]]'' = None, pipeline_params:
        ''Optional[PipelineParams]'' = None, python_named_params: ''Optional[Dict[str,
        str]]'' = None, python_params: ''Optional[List[str]]'' = None, spark_submit_params:
        ''Optional[List[str]]'' = None, sql_params: ''Optional[Dict[str, str]]'' =
        None)'
      properties:
        job_id:
          type: string
          description: 'ID of the job to trigger.  dbt_commands: Optional[List[str]]
            = None """An array of commands to execute for jobs with the dbt task,
            for example `"dbt_commands": ["dbt deps", "dbt seed", "dbt deps", "dbt
            seed", "dbt run"]`  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        dbt_commands:
          type: string
          description: 'An array of commands to execute for jobs with the dbt task,
            for example `"dbt_commands": ["dbt deps", "dbt seed", "dbt deps", "dbt
            seed", "dbt run"]`  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        jar_params:
          type: string
          description: 'A list of parameters for jobs with Spark JAR tasks, for example
            `"jar_params": ["john doe", "35"]`. The parameters are used to invoke
            the main function of the main class specified in the Spark JAR task. If
            not specified upon `run-now`, it defaults to an empty list. jar_params
            cannot be specified in conjunction with notebook_params. The JSON representation
            of this field (for example `{"jar_params":["john doe","35"]}`) cannot
            exceed 10,000 bytes.  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        job_parameters:
          type: string
          description: 'Job-level parameters used to trigger the job.  notebook_params:
            Optional[Dict[str, str]] = None """A map from keys to values for jobs
            with notebook task, for example `"notebook_params": {"name": "john doe",
            "age": "35"}`. The map is passed to the notebook and is accessible through
            the [dbutils.widgets.get] function.  If not specified upon `run-now`,
            the triggered run uses the job’s base parameters.  notebook_params cannot
            be specified in conjunction with jar_params.  ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks.  The JSON representation
            of this field (for example `{"notebook_params":{"name":"john doe","age":"35"}}`)
            cannot exceed 10,000 bytes.  [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
            [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        notebook_params:
          type: string
          description: 'A map from keys to values for jobs with notebook task, for
            example `"notebook_params": {"name": "john doe", "age": "35"}`. The map
            is passed to the notebook and is accessible through the [dbutils.widgets.get]
            function.  If not specified upon `run-now`, the triggered run uses the
            job’s base parameters.  notebook_params cannot be specified in conjunction
            with jar_params.  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  The JSON representation of this field (for
            example `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot exceed
            10,000 bytes.  [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
            [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        pipeline_params:
          type: string
          description: 'Controls whether the pipeline should perform a full refresh  python_named_params:
            Optional[Dict[str, str]] = None  python_params: Optional[List[str]] =
            None """A list of parameters for jobs with Python tasks, for example `"python_params":
            ["john doe", "35"]`. The parameters are passed to Python file as command-line
            parameters. If specified upon `run-now`, it would overwrite the parameters
            specified in job setting. The JSON representation of this field (for example
            `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.  ⚠
            **Deprecation note** Use [job parameters] to pass information down to
            tasks.  Important  These parameters accept only Latin characters (ASCII
            character set). Using non-ASCII characters returns an error. Examples
            of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.  [job
            parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        python_named_params:
          type: string
          description: ''
        python_params:
          type: string
          description: 'A list of parameters for jobs with Python tasks, for example
            `"python_params": ["john doe", "35"]`. The parameters are passed to Python
            file as command-line parameters. If specified upon `run-now`, it would
            overwrite the parameters specified in job setting. The JSON representation
            of this field (for example `{"python_params":["john doe","35"]}`) cannot
            exceed 10,000 bytes.  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  Important  These parameters accept only Latin
            characters (ASCII character set). Using non-ASCII characters returns an
            error. Examples of invalid, non-ASCII characters are Chinese, Japanese
            kanjis, and emojis.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        spark_submit_params:
          type: string
          description: 'A list of parameters for jobs with spark submit task, for
            example `"spark_submit_params": ["--class", "org.apache.spark.examples.SparkPi"]`.
            The parameters are passed to spark-submit script as command-line parameters.
            If specified upon `run-now`, it would overwrite the parameters specified
            in job setting. The JSON representation of this field (for example `{"python_params":["john
            doe","35"]}`) cannot exceed 10,000 bytes.  ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks.  Important  These
            parameters accept only Latin characters (ASCII character set). Using non-ASCII
            characters returns an error. Examples of invalid, non-ASCII characters
            are Chinese, Japanese kanjis, and emojis.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        sql_params:
          type: string
          description: 'A map from keys to values for jobs with SQL task, for example
            `"sql_params": {"name": "john doe", "age": "35"}`. The SQL alert task
            does not support custom parameters.  ⚠ **Deprecation note** Use [job parameters]
            to pass information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
    RunLifeCycleState:
      type: string
      description: 'A value indicating the run''s lifecycle state. The possible values
        are: * `QUEUED`: The run is

        queued. * `PENDING`: The run is waiting to be executed while the cluster and
        execution context

        are being prepared. * `RUNNING`: The task of this run is being executed. *
        `TERMINATING`: The

        task of this run has completed, and the cluster and execution context are
        being cleaned up. *

        `TERMINATED`: The task of this run has completed, and the cluster and execution
        context have

        been cleaned up. This state is terminal. * `SKIPPED`: This run was aborted
        because a previous

        run of the same job was already active. This state is terminal. * `INTERNAL_ERROR`:
        An

        exceptional state that indicates a failure in the Jobs service, such as network
        failure over a

        long period. If a run on a new cluster ends in the `INTERNAL_ERROR` state,
        the Jobs service

        terminates the cluster as soon as possible. This state is terminal. * `BLOCKED`:
        The run is

        blocked on an upstream dependency. * `WAITING_FOR_RETRY`: The run is waiting
        for a retry.'
      enum:
      - BLOCKED
      - INTERNAL_ERROR
      - PENDING
      - QUEUED
      - RUNNING
      - SKIPPED
      - TERMINATED
      - TERMINATING
      - WAITING_FOR_RETRY
    RunLifecycleStateV2State:
      type: string
      description: The current state of the run.
      enum:
      - BLOCKED
      - PENDING
      - QUEUED
      - RUNNING
      - TERMINATED
      - TERMINATING
      - WAITING
    RunNowResponse:
      type: object
      description: Run was started successfully.
      properties:
        number_in_job:
          type: string
          description: 'A unique identifier for this job run. This is set to the same
            value as `run_id`.  run_id: Optional[int] = None The globally unique ID
            of the newly triggered run.'
        run_id:
          type: string
          description: 'The globally unique ID of the newly triggered run.  def as_dict(self)
            -> dict: Serializes the RunNowResponse into a dictionary suitable for
            use as a JSON request body.'
    RunOutput:
      type: object
      description: Run output was retrieved successfully.
      properties:
        clean_rooms_notebook_output:
          type: string
          description: 'The output of a clean rooms notebook task, if available  dashboard_output:
            Optional[DashboardTaskOutput] = None The output of a dashboard task, if
            available'
        dashboard_output:
          type: string
          description: 'The output of a dashboard task, if available  dbt_cloud_output:
            Optional[DbtCloudTaskOutput] = None Deprecated in favor of the new dbt_platform_output'
        dbt_cloud_output:
          type: string
          description: 'Deprecated in favor of the new dbt_platform_output  dbt_output:
            Optional[DbtOutput] = None The output of a dbt task, if available.'
        dbt_output:
          type: string
          description: 'The output of a dbt task, if available.  dbt_platform_output:
            Optional[DbtPlatformTaskOutput] = None  error: Optional[str] = None """An
            error message indicating why a task failed or why output is not available.
            The message is unstructured, and its exact format is subject to change.'
        dbt_platform_output:
          type: string
          description: ''
        error:
          type: string
          description: An error message indicating why a task failed or why output
            is not available. The message is unstructured, and its exact format is
            subject to change.
        error_trace:
          type: string
          description: 'If there was an error executing the run, this field contains
            any available stack traces.  info: Optional[str] = None  logs: Optional[str]
            = None """The output from tasks that write to standard streams (stdout/stderr)
            such as spark_jar_task, spark_python_task, python_wheel_task.  It''s not
            supported for the notebook_task, pipeline_task or spark_submit_task.  Databricks
            restricts this API to return the last 5 MB of these logs.'
        info:
          type: string
          description: ''
        logs:
          type: string
          description: The output from tasks that write to standard streams (stdout/stderr)
            such as spark_jar_task, spark_python_task, python_wheel_task.  It's not
            supported for the notebook_task, pipeline_task or spark_submit_task.  Databricks
            restricts this API to return the last 5 MB of these logs.
        logs_truncated:
          type: string
          description: 'Whether the logs are truncated.  metadata: Optional[Run] =
            None All details of the run except for its output.'
        metadata:
          type: string
          description: 'All details of the run except for its output.  notebook_output:
            Optional[NotebookOutput] = None """The output of a notebook task, if available.
            A notebook task that terminates (either successfully or with a failure)
            without calling `dbutils.notebook.exit()` is considered to have an empty
            output. This field is set but its result value is empty. Databricks restricts
            this API to return the first 5 MB of the output. To return a larger result,
            use the [ClusterLogConf] field to configure log storage for the job cluster.  [ClusterLogConf]:
            https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterlogconf'
        notebook_output:
          type: string
          description: 'The output of a clean rooms notebook task, if available  dashboard_output:
            Optional[DashboardTaskOutput] = None The output of a dashboard task, if
            available'
        run_job_output:
          type: string
          description: 'The output of a run job task, if available  sql_output: Optional[SqlOutput]
            = None The output of a SQL task, if available.'
        sql_output:
          type: string
          description: 'The output of a SQL task, if available.  def as_dict(self)
            -> dict: Serializes the RunOutput into a dictionary suitable for use as
            a JSON request body.'
    RunParameters:
      type: object
      description: 'RunParameters(dbt_commands: ''Optional[List[str]]'' = None, jar_params:
        ''Optional[List[str]]'' = None, notebook_params: ''Optional[Dict[str, str]]''
        = None, pipeline_params: ''Optional[PipelineParams]'' = None, python_named_params:
        ''Optional[Dict[str, str]]'' = None, python_params: ''Optional[List[str]]''
        = None, spark_submit_params: ''Optional[List[str]]'' = None, sql_params: ''Optional[Dict[str,
        str]]'' = None)'
      properties:
        dbt_commands:
          type: string
          description: 'An array of commands to execute for jobs with the dbt task,
            for example `"dbt_commands": ["dbt deps", "dbt seed", "dbt deps", "dbt
            seed", "dbt run"]`  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        jar_params:
          type: string
          description: 'A list of parameters for jobs with Spark JAR tasks, for example
            `"jar_params": ["john doe", "35"]`. The parameters are used to invoke
            the main function of the main class specified in the Spark JAR task. If
            not specified upon `run-now`, it defaults to an empty list. jar_params
            cannot be specified in conjunction with notebook_params. The JSON representation
            of this field (for example `{"jar_params":["john doe","35"]}`) cannot
            exceed 10,000 bytes.  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        notebook_params:
          type: string
          description: 'A map from keys to values for jobs with notebook task, for
            example `"notebook_params": {"name": "john doe", "age": "35"}`. The map
            is passed to the notebook and is accessible through the [dbutils.widgets.get]
            function.  If not specified upon `run-now`, the triggered run uses the
            job’s base parameters.  notebook_params cannot be specified in conjunction
            with jar_params.  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  The JSON representation of this field (for
            example `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot exceed
            10,000 bytes.  [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
            [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        pipeline_params:
          type: string
          description: 'Controls whether the pipeline should perform a full refresh  python_named_params:
            Optional[Dict[str, str]] = None  python_params: Optional[List[str]] =
            None """A list of parameters for jobs with Python tasks, for example `"python_params":
            ["john doe", "35"]`. The parameters are passed to Python file as command-line
            parameters. If specified upon `run-now`, it would overwrite the parameters
            specified in job setting. The JSON representation of this field (for example
            `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.  ⚠
            **Deprecation note** Use [job parameters] to pass information down to
            tasks.  Important  These parameters accept only Latin characters (ASCII
            character set). Using non-ASCII characters returns an error. Examples
            of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.  [job
            parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        python_named_params:
          type: string
          description: ''
        python_params:
          type: string
          description: 'A list of parameters for jobs with Python tasks, for example
            `"python_params": ["john doe", "35"]`. The parameters are passed to Python
            file as command-line parameters. If specified upon `run-now`, it would
            overwrite the parameters specified in job setting. The JSON representation
            of this field (for example `{"python_params":["john doe","35"]}`) cannot
            exceed 10,000 bytes.  ⚠ **Deprecation note** Use [job parameters] to pass
            information down to tasks.  Important  These parameters accept only Latin
            characters (ASCII character set). Using non-ASCII characters returns an
            error. Examples of invalid, non-ASCII characters are Chinese, Japanese
            kanjis, and emojis.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        spark_submit_params:
          type: string
          description: 'A list of parameters for jobs with spark submit task, for
            example `"spark_submit_params": ["--class", "org.apache.spark.examples.SparkPi"]`.
            The parameters are passed to spark-submit script as command-line parameters.
            If specified upon `run-now`, it would overwrite the parameters specified
            in job setting. The JSON representation of this field (for example `{"python_params":["john
            doe","35"]}`) cannot exceed 10,000 bytes.  ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks.  Important  These
            parameters accept only Latin characters (ASCII character set). Using non-ASCII
            characters returns an error. Examples of invalid, non-ASCII characters
            are Chinese, Japanese kanjis, and emojis.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
        sql_params:
          type: string
          description: 'A map from keys to values for jobs with SQL task, for example
            `"sql_params": {"name": "john doe", "age": "35"}`. The SQL alert task
            does not support custom parameters.  ⚠ **Deprecation note** Use [job parameters]
            to pass information down to tasks.  [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown'
    RunResultState:
      type: string
      description: 'A value indicating the run''s result. The possible values are:
        * `SUCCESS`: The task completed

        successfully. * `FAILED`: The task completed with an error. * `TIMEDOUT`:
        The run was stopped

        after reaching the timeout. * `CANCELED`: The run was canceled at user request.
        *

        `MAXIMUM_CONCURRENT_RUNS_REACHED`: The run was skipped because the maximum
        concurrent runs were

        reached. * `EXCLUDED`: The run was skipped because the necessary conditions
        were not met. *

        `SUCCESS_WITH_FAILURES`: The job run completed successfully with some failures;
        leaf tasks were

        successful. * `UPSTREAM_FAILED`: The run was skipped because of an upstream
        failure. *

        `UPSTREAM_CANCELED`: The run was skipped because an upstream task was canceled.
        * `DISABLED`:

        The run was skipped because it was disabled explicitly by the user.'
      enum:
      - CANCELED
      - DISABLED
      - EXCLUDED
      - FAILED
      - MAXIMUM_CONCURRENT_RUNS_REACHED
      - SUCCESS
      - SUCCESS_WITH_FAILURES
      - TIMEDOUT
      - UPSTREAM_CANCELED
      - UPSTREAM_FAILED
    RunState:
      type: object
      description: The current state of the run.
      properties:
        life_cycle_state:
          type: string
          description: 'A value indicating the run''s current lifecycle state. This
            field is always available in the response. Note: Additional states might
            be introduced in future releases.'
        queue_reason:
          type: string
          description: 'The reason indicating why the run was queued.  result_state:
            Optional[RunResultState] = None """A value indicating the run''s result.
            This field is only available for terminal lifecycle states. Note: Additional
            states might be introduced in future releases.'
        result_state:
          type: string
          description: 'A value indicating the run''s result. This field is only available
            for terminal lifecycle states. Note: Additional states might be introduced
            in future releases.'
        state_message:
          type: string
          description: A descriptive message for the current state. This field is
            unstructured, and its exact format is subject to change.
        user_cancelled_or_timedout:
          type: string
          description: A value indicating whether a run was canceled manually by a
            user or by the scheduler because the run timed out.
    RunStatus:
      type: object
      description: The current status of the run
      properties:
        queue_details:
          type: string
          description: 'If the run was queued, details about the reason for queuing
            the run.  state: Optional[RunLifecycleStateV2State] = None  termination_details:
            Optional[TerminationDetails] = None """If the run is in a TERMINATING
            or TERMINATED state, details about the reason for terminating the run.'
        state:
          type: string
          description: ''
        termination_details:
          type: string
          description: If the run is in a TERMINATING or TERMINATED state, details
            about the reason for terminating the run.
    RunTask:
      type: object
      description: Used when outputting a child run, in GetRun or ListRuns.
      properties:
        task_key:
          type: string
          description: A unique name for the task. This field is used to refer to
            this task from other tasks. This field is required and must be unique
            within its parent job. On Update or Reset, this field is used to reference
            the tasks to be updated or reset.
        attempt_number:
          type: string
          description: The sequence number of this run attempt for a triggered job
            run. The initial attempt of a run has an attempt_number of 0. If the initial
            run attempt fails, and the job has a retry policy (`max_retries` > 0),
            subsequent runs are created with an `original_attempt_run_id` of the original
            attempt’s ID and an incrementing `attempt_number`. Runs are retried only
            until they succeed, and the maximum `attempt_number` is the same as the
            `max_retries` value for the job.
        clean_rooms_notebook_task:
          type: string
          description: 'The task runs a [clean rooms] notebook when the `clean_rooms_notebook_task`
            field is present.  [clean rooms]: https://docs.databricks.com/clean-rooms/index.html'
        cleanup_duration:
          type: string
          description: The time in milliseconds it took to terminate the cluster and
            clean up any associated artifacts. The duration of a task run is the sum
            of the `setup_duration`, `execution_duration`, and the `cleanup_duration`.
            The `cleanup_duration` field is set to 0 for multitask job runs. The total
            duration of a multitask job run is the value of the `run_duration` field.
        cluster_instance:
          type: string
          description: The cluster used for this run. If the run is specified to use
            a new cluster, this field is set once the Jobs service has requested a
            cluster for the run.
        condition_task:
          type: string
          description: The task evaluates a condition that can be used to control
            the execution of other tasks when the `condition_task` field is present.
            The condition task does not require a cluster to execute and does not
            support retries or notifications.
        dashboard_task:
          type: string
          description: 'The task refreshes a dashboard and sends a snapshot to subscribers.  dbt_cloud_task:
            Optional[DbtCloudTask] = None Task type for dbt cloud, deprecated in favor
            of the new name dbt_platform_task'
        dbt_cloud_task:
          type: string
          description: 'Task type for dbt cloud, deprecated in favor of the new name
            dbt_platform_task  dbt_platform_task: Optional[DbtPlatformTask] = None  dbt_task:
            Optional[DbtTask] = None """The task runs one or more dbt commands when
            the `dbt_task` field is present. The dbt task requires both Databricks
            SQL and the ability to use a serverless or a pro SQL warehouse.'
        dbt_platform_task:
          type: string
          description: ''
        dbt_task:
          type: string
          description: The task runs one or more dbt commands when the `dbt_task`
            field is present. The dbt task requires both Databricks SQL and the ability
            to use a serverless or a pro SQL warehouse.
        depends_on:
          type: string
          description: An optional array of objects specifying the dependency graph
            of the task. All tasks specified in this field must complete successfully
            before executing this task. The key is `task_key`, and the value is the
            name assigned to the dependent task.
        description:
          type: string
          description: 'An optional description for this task.  effective_performance_target:
            Optional[PerformanceTarget] = None """The actual performance target used
            by the serverless run during execution. This can differ from the client-set
            performance target on the request depending on whether the performance
            mode is supported by the job type.  * `STANDARD`: Enables cost-efficient
            execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes
            fast startup and execution times through rapid scaling and optimized cluster
            performance.'
        effective_performance_target:
          type: string
          description: 'The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on the
            request depending on whether the performance mode is supported by the
            job type.  * `STANDARD`: Enables cost-efficient execution of serverless
            workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and execution
            times through rapid scaling and optimized cluster performance.'
        email_notifications:
          type: string
          description: An optional set of email addresses notified when the task run
            begins or completes. The default behavior is to not send any emails.
        end_time:
          type: string
          description: The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still running.
        environment_key:
          type: string
          description: The key that references an environment spec in a job. This
            field is required for Python script, Python wheel and dbt tasks when using
            serverless compute.
        execution_duration:
          type: string
          description: The time in milliseconds it took to execute the commands in
            the JAR or notebook until they completed, failed, timed out, were cancelled,
            or encountered an unexpected error. The duration of a task run is the
            sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`.
            The `execution_duration` field is set to 0 for multitask job runs. The
            total duration of a multitask job run is the value of the `run_duration`
            field.
        existing_cluster_id:
          type: string
          description: If existing_cluster_id, the ID of an existing cluster that
            is used for all runs. When running jobs or tasks on an existing cluster,
            you may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater reliability
        for_each_task:
          type: string
          description: The task executes a nested task for every input provided when
            the `for_each_task` field is present.
        gen_ai_compute_task:
          type: string
          description: ''
        git_source:
          type: string
          description: 'An optional specification for a remote Git repository containing
            the source code used by tasks. Version-controlled source code is supported
            by notebook, dbt, Python script, and SQL File tasks. If `git_source` is
            set, these tasks retrieve the file from the remote repository by default.
            However, this behavior can be overridden by setting `source` to `WORKSPACE`
            on the task. Note: dbt and SQL File tasks support only version-controlled
            sources. If dbt or SQL File tasks are used, `git_source` must be defined
            on the job.'
        job_cluster_key:
          type: string
          description: If job_cluster_key, this task is executed reusing the cluster
            specified in `job.settings.job_clusters`.
        libraries:
          type: string
          description: An optional list of libraries to be installed on the cluster.
            The default value is an empty list.
        new_cluster:
          type: string
          description: 'If new_cluster, a description of a new cluster that is created
            for each run.  notebook_task: Optional[NotebookTask] = None The task runs
            a notebook when the `notebook_task` field is present.'
        notebook_task:
          type: string
          description: 'The task runs a [clean rooms] notebook when the `clean_rooms_notebook_task`
            field is present.  [clean rooms]: https://docs.databricks.com/clean-rooms/index.html'
        notification_settings:
          type: string
          description: Optional notification settings that are used when sending notifications
            to each of the `email_notifications` and `webhook_notifications` for this
            task run.
        pipeline_task:
          type: string
          description: The task triggers a pipeline update when the `pipeline_task`
            field is present. Only pipelines configured to use triggered more are
            supported.
        power_bi_task:
          type: string
          description: 'The task triggers a Power BI semantic model update when the
            `power_bi_task` field is present.  python_wheel_task: Optional[PythonWheelTask]
            = None The task runs a Python wheel when the `python_wheel_task` field
            is present.'
        python_wheel_task:
          type: string
          description: 'The task runs a Python wheel when the `python_wheel_task`
            field is present.  queue_duration: Optional[int] = None The time in milliseconds
            that the run has spent in the queue.'
        queue_duration:
          type: string
          description: 'The time in milliseconds that the run has spent in the queue.  resolved_values:
            Optional[ResolvedValues] = None Parameter values including resolved references'
        resolved_values:
          type: string
          description: 'Parameter values including resolved references  run_duration:
            Optional[int] = None The time in milliseconds it took the job run and
            all of its repairs to finish.'
        run_duration:
          type: string
          description: 'The time in milliseconds it took the job run and all of its
            repairs to finish.  run_id: Optional[int] = None The ID of the task run.'
        run_id:
          type: string
          description: 'The ID of the task run.  run_if: Optional[RunIf] = None """An
            optional value indicating the condition that determines whether the task
            should be run once its dependencies have been completed. When omitted,
            defaults to `ALL_SUCCESS`. See :method:jobs/create for a list of possible
            values.'
        run_if:
          type: string
          description: An optional value indicating the condition that determines
            whether the task should be run once its dependencies have been completed.
            When omitted, defaults to `ALL_SUCCESS`. See :method:jobs/create for a
            list of possible values.
        run_job_task:
          type: string
          description: 'The task triggers another job when the `run_job_task` field
            is present.  run_page_url: Optional[str] = None  setup_duration: Optional[int]
            = None """The time in milliseconds it took to set up the cluster. For
            runs that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The duration
            of a task run is the sum of the `setup_duration`, `execution_duration`,
            and the `cleanup_duration`. The `setup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the value
            of the `run_duration` field.'
        run_page_url:
          type: string
          description: ''
        setup_duration:
          type: string
          description: The time in milliseconds it took to set up the cluster. For
            runs that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The duration
            of a task run is the sum of the `setup_duration`, `execution_duration`,
            and the `cleanup_duration`. The `setup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the value
            of the `run_duration` field.
        spark_jar_task:
          type: string
          description: 'The task runs a JAR when the `spark_jar_task` field is present.  spark_python_task:
            Optional[SparkPythonTask] = None The task runs a Python file when the
            `spark_python_task` field is present.'
        spark_python_task:
          type: string
          description: 'The task runs a Python file when the `spark_python_task` field
            is present.  spark_submit_task: Optional[SparkSubmitTask] = None """(Legacy)
            The task runs the spark-submit script when the spark_submit_task field
            is present. Databricks recommends using the spark_jar_task instead; see
            [Spark Submit task for jobs](/jobs/spark-submit).'
        spark_submit_task:
          type: string
          description: (Legacy) The task runs the spark-submit script when the spark_submit_task
            field is present. Databricks recommends using the spark_jar_task instead;
            see [Spark Submit task for jobs](/jobs/spark-submit).
        sql_task:
          type: string
          description: The task runs a SQL query or file, or it refreshes a SQL alert
            or a legacy SQL dashboard when the `sql_task` field is present.
        start_time:
          type: string
          description: The time at which this run was started in epoch milliseconds
            (milliseconds since 1/1/1970 UTC). This may not be the time when the job
            task starts executing, for example, if the job is scheduled to run on
            a new cluster, this is the time the cluster creation call is issued.
        state:
          type: string
          description: 'Deprecated. Please use the `status` field instead.  status:
            Optional[RunStatus] = None  timeout_seconds: Optional[int] = None An optional
            timeout applied to each run of this job task. A value of `0` means no
            timeout.'
        status:
          type: string
          description: ''
        timeout_seconds:
          type: string
          description: 'An optional timeout applied to each run of this job task.
            A value of `0` means no timeout.  webhook_notifications: Optional[WebhookNotifications]
            = None """A collection of system notification IDs to notify when the run
            begins or completes. The default behavior is to not send any system notifications.
            Task webhooks respect the task notification settings.'
        webhook_notifications:
          type: string
          description: A collection of system notification IDs to notify when the
            run begins or completes. The default behavior is to not send any system
            notifications. Task webhooks respect the task notification settings.
    RunType:
      type: string
      description: 'The type of a run. * `JOB_RUN`: Normal job run. A run created
        with :method:jobs/runNow. *

        `WORKFLOW_RUN`: Workflow run. A run created with [dbutils.notebook.run]. *
        `SUBMIT_RUN`: Submit

        run. A run created with :method:jobs/submit.


        [dbutils.notebook.run]: https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-workflow'
      enum:
      - JOB_RUN
      - SUBMIT_RUN
      - WORKFLOW_RUN
    Source:
      type: string
      description: 'Optional location type of the SQL file. When set to `WORKSPACE`,
        the SQL file will be retrieved    from the local Databricks workspace. When
        set to `GIT`, the SQL file will be retrieved from a

        Git repository defined in `git_source`. If the value is empty, the task will
        use `GIT` if

        `git_source` is defined and `WORKSPACE` otherwise.


        * `WORKSPACE`: SQL file is located in Databricks workspace. * `GIT`: SQL file
        is located in

        cloud Git provider.'
      enum:
      - GIT
      - WORKSPACE
    SparkJarTask:
      type: object
      description: 'SparkJarTask(jar_uri: ''Optional[str]'' = None, main_class_name:
        ''Optional[str]'' = None, parameters: ''Optional[List[str]]'' = None, run_as_repl:
        ''Optional[bool]'' = None)'
      properties:
        jar_uri:
          type: string
          description: Deprecated since 04/2016. Provide a `jar` through the `libraries`
            field instead. For an example, see :method:jobs/create.
        main_class_name:
          type: string
          description: The full name of the class containing the main method to be
            executed. This class must be contained in a JAR provided as a library.  The
            code must use `SparkContext.getOrCreate` to obtain a Spark context; otherwise,
            runs of the job fail.
        parameters:
          type: string
          description: 'Parameters passed to the main method.  Use [Task parameter
            variables] to set parameters containing information about job runs.  [Task
            parameter variables]: https://docs.databricks.com/jobs.html#parameter-variables'
        run_as_repl:
          type: string
          description: 'Deprecated. A value of `false` is no longer supported.  def
            as_dict(self) -> dict: Serializes the SparkJarTask into a dictionary suitable
            for use as a JSON request body.'
    SparkPythonTask:
      type: object
      description: 'SparkPythonTask(python_file: ''str'', parameters: ''Optional[List[str]]''
        = None, source: ''Optional[Source]'' = None)'
      properties:
        python_file:
          type: string
          description: The Python file to be executed. Cloud file URIs (such as dbfs:/,
            s3:/, adls:/, gcs:/) and workspace paths are supported. For python files
            stored in the Databricks workspace, the path must be absolute and begin
            with `/`. For files stored in a remote repository, the path must be relative.
            This field is required.
        parameters:
          type: string
          description: 'Command line parameters passed to the Python file.  Use [Task
            parameter variables] to set parameters containing information about job
            runs.  [Task parameter variables]: https://docs.databricks.com/jobs.html#parameter-variables'
        source:
          type: string
          description: 'Optional location type of the Python file. When set to `WORKSPACE`
            or not specified, the file will be retrieved from the local Databricks
            workspace or cloud location (if the `python_file` has a URI format). When
            set to `GIT`, the Python file will be retrieved from a Git repository
            defined in `git_source`.  * `WORKSPACE`: The Python file is located in
            a Databricks workspace or at a cloud filesystem URI. * `GIT`: The Python
            file is located in a remote Git repository.'
    SparkSubmitTask:
      type: object
      description: 'SparkSubmitTask(parameters: ''Optional[List[str]]'' = None)'
      properties:
        parameters:
          type: string
          description: 'Command-line parameters passed to spark submit.  Use [Task
            parameter variables] to set parameters containing information about job
            runs.  [Task parameter variables]: https://docs.databricks.com/jobs.html#parameter-variables'
    SqlAlertOutput:
      type: object
      description: 'SqlAlertOutput(alert_state: ''Optional[SqlAlertState]'' = None,
        output_link: ''Optional[str]'' = None, query_text: ''Optional[str]'' = None,
        sql_statements: ''Optional[List[SqlStatementOutput]]'' = None, warehouse_id:
        ''Optional[str]'' = None)'
      properties:
        alert_state:
          type: string
          description: ''
        output_link:
          type: string
          description: 'The link to find the output results.  query_text: Optional[str]
            = None """The text of the SQL query. Can Run permission of the SQL query
            associated with the SQL alert is required to view this field.'
        query_text:
          type: string
          description: The text of the SQL query. Can Run permission of the SQL query
            associated with the SQL alert is required to view this field.
        sql_statements:
          type: string
          description: 'Information about SQL statements executed in the run.  warehouse_id:
            Optional[str] = None The canonical identifier of the SQL warehouse.'
        warehouse_id:
          type: string
          description: 'The canonical identifier of the SQL warehouse.  def as_dict(self)
            -> dict: Serializes the SqlAlertOutput into a dictionary suitable for
            use as a JSON request body.'
    SqlAlertState:
      type: string
      description: 'The state of the SQL alert.


        * UNKNOWN: alert yet to be evaluated * OK: alert evaluated and did not fulfill
        trigger

        conditions * TRIGGERED: alert evaluated and fulfilled trigger conditions'
      enum:
      - OK
      - TRIGGERED
      - UNKNOWN
    SqlDashboardOutput:
      type: object
      description: 'SqlDashboardOutput(warehouse_id: ''Optional[str]'' = None, widgets:
        ''Optional[List[SqlDashboardWidgetOutput]]'' = None)'
      properties:
        warehouse_id:
          type: string
          description: 'The canonical identifier of the SQL warehouse.  widgets: Optional[List[SqlDashboardWidgetOutput]]
            = None Widgets executed in the run. Only SQL query based widgets are listed.'
        widgets:
          type: string
          description: 'Widgets executed in the run. Only SQL query based widgets
            are listed.  def as_dict(self) -> dict: Serializes the SqlDashboardOutput
            into a dictionary suitable for use as a JSON request body.'
    SqlDashboardWidgetOutput:
      type: object
      description: 'SqlDashboardWidgetOutput(end_time: ''Optional[int]'' = None, error:
        ''Optional[SqlOutputError]'' = None, output_link: ''Optional[str]'' = None,
        start_time: ''Optional[int]'' = None, status: ''Optional[SqlDashboardWidgetOutputStatus]''
        = None, widget_id: ''Optional[str]'' = None, widget_title: ''Optional[str]''
        = None)'
      properties:
        end_time:
          type: string
          description: 'Time (in epoch milliseconds) when execution of the SQL widget
            ends.  error: Optional[SqlOutputError] = None The information about the
            error when execution fails.'
        error:
          type: string
          description: 'The information about the error when execution fails.  output_link:
            Optional[str] = None The link to find the output results.'
        output_link:
          type: string
          description: 'The link to find the output results.  start_time: Optional[int]
            = None Time (in epoch milliseconds) when execution of the SQL widget starts.'
        start_time:
          type: string
          description: 'Time (in epoch milliseconds) when execution of the SQL widget
            starts.  status: Optional[SqlDashboardWidgetOutputStatus] = None The execution
            status of the SQL widget.'
        status:
          type: string
          description: 'The execution status of the SQL widget.  widget_id: Optional[str]
            = None The canonical identifier of the SQL widget.'
        widget_id:
          type: string
          description: 'The canonical identifier of the SQL widget.  widget_title:
            Optional[str] = None The title of the SQL widget.'
        widget_title:
          type: string
          description: 'The title of the SQL widget.  def as_dict(self) -> dict: Serializes
            the SqlDashboardWidgetOutput into a dictionary suitable for use as a JSON
            request body.'
    SqlDashboardWidgetOutputStatus:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - CANCELLED
      - FAILED
      - PENDING
      - RUNNING
      - SUCCESS
    SqlOutput:
      type: object
      description: 'SqlOutput(alert_output: ''Optional[SqlAlertOutput]'' = None, dashboard_output:
        ''Optional[SqlDashboardOutput]'' = None, query_output: ''Optional[SqlQueryOutput]''
        = None)'
      properties:
        alert_output:
          type: string
          description: 'The output of a SQL alert task, if available.  dashboard_output:
            Optional[SqlDashboardOutput] = None The output of a SQL dashboard task,
            if available.'
        dashboard_output:
          type: string
          description: 'The output of a SQL dashboard task, if available.  query_output:
            Optional[SqlQueryOutput] = None The output of a SQL query task, if available.'
        query_output:
          type: string
          description: 'The output of a SQL query task, if available.  def as_dict(self)
            -> dict: Serializes the SqlOutput into a dictionary suitable for use as
            a JSON request body.'
    SqlOutputError:
      type: object
      description: 'SqlOutputError(message: ''Optional[str]'' = None)'
      properties:
        message:
          type: string
          description: 'The error message when execution fails.  def as_dict(self)
            -> dict: Serializes the SqlOutputError into a dictionary suitable for
            use as a JSON request body.'
    SqlQueryOutput:
      type: object
      description: 'SqlQueryOutput(endpoint_id: ''Optional[str]'' = None, output_link:
        ''Optional[str]'' = None, query_text: ''Optional[str]'' = None, sql_statements:
        ''Optional[List[SqlStatementOutput]]'' = None, warehouse_id: ''Optional[str]''
        = None)'
      properties:
        endpoint_id:
          type: string
          description: ''
        output_link:
          type: string
          description: 'The link to find the output results.  query_text: Optional[str]
            = None The text of the SQL query. Can Run permission of the SQL query
            is required to view this field.'
        query_text:
          type: string
          description: 'The text of the SQL query. Can Run permission of the SQL query
            is required to view this field.  sql_statements: Optional[List[SqlStatementOutput]]
            = None Information about SQL statements executed in the run.'
        sql_statements:
          type: string
          description: 'Information about SQL statements executed in the run.  warehouse_id:
            Optional[str] = None The canonical identifier of the SQL warehouse.'
        warehouse_id:
          type: string
          description: 'The canonical identifier of the SQL warehouse.  def as_dict(self)
            -> dict: Serializes the SqlQueryOutput into a dictionary suitable for
            use as a JSON request body.'
    SqlStatementOutput:
      type: object
      description: 'SqlStatementOutput(lookup_key: ''Optional[str]'' = None)'
      properties:
        lookup_key:
          type: string
          description: 'A key that can be used to look up query details.  def as_dict(self)
            -> dict: Serializes the SqlStatementOutput into a dictionary suitable
            for use as a JSON request body.'
    SqlTask:
      type: object
      description: 'SqlTask(warehouse_id: ''str'', alert: ''Optional[SqlTaskAlert]''
        = None, dashboard: ''Optional[SqlTaskDashboard]'' = None, file: ''Optional[SqlTaskFile]''
        = None, parameters: ''Optional[Dict[str, str]]'' = None, query: ''Optional[SqlTaskQuery]''
        = None)'
      properties:
        warehouse_id:
          type: string
          description: The canonical identifier of the SQL warehouse. Recommended
            to use with serverless or pro SQL warehouses. Classic SQL warehouses are
            only supported for SQL alert, dashboard and query tasks and are limited
            to scheduled single-task jobs.
        alert:
          type: string
          description: 'If alert, indicates that this job must refresh a SQL alert.  dashboard:
            Optional[SqlTaskDashboard] = None If dashboard, indicates that this job
            must refresh a SQL dashboard.'
        dashboard:
          type: string
          description: 'If dashboard, indicates that this job must refresh a SQL dashboard.  file:
            Optional[SqlTaskFile] = None If file, indicates that this job runs a SQL
            file in a remote Git repository.'
        file:
          type: string
          description: 'If file, indicates that this job runs a SQL file in a remote
            Git repository.  parameters: Optional[Dict[str, str]] = None """Parameters
            to be used for each run of this job. The SQL alert task does not support
            custom parameters.'
        parameters:
          type: string
          description: Parameters to be used for each run of this job. The SQL alert
            task does not support custom parameters.
        query:
          type: string
          description: 'If query, indicates that this job must execute a SQL query.  def
            as_dict(self) -> dict: Serializes the SqlTask into a dictionary suitable
            for use as a JSON request body.'
    SqlTaskAlert:
      type: object
      description: 'SqlTaskAlert(alert_id: ''str'', pause_subscriptions: ''Optional[bool]''
        = None, subscriptions: ''Optional[List[SqlTaskSubscription]]'' = None)'
      properties:
        alert_id:
          type: string
          description: 'The canonical identifier of the SQL alert.  pause_subscriptions:
            Optional[bool] = None If true, the alert notifications are not sent to
            subscribers.'
        pause_subscriptions:
          type: string
          description: 'If true, the alert notifications are not sent to subscribers.  subscriptions:
            Optional[List[SqlTaskSubscription]] = None If specified, alert notifications
            are sent to subscribers.'
        subscriptions:
          type: string
          description: 'If true, the alert notifications are not sent to subscribers.  subscriptions:
            Optional[List[SqlTaskSubscription]] = None If specified, alert notifications
            are sent to subscribers.'
    SqlTaskDashboard:
      type: object
      description: 'SqlTaskDashboard(dashboard_id: ''str'', custom_subject: ''Optional[str]''
        = None, pause_subscriptions: ''Optional[bool]'' = None, subscriptions: ''Optional[List[SqlTaskSubscription]]''
        = None)'
      properties:
        dashboard_id:
          type: string
          description: 'The canonical identifier of the SQL dashboard.  custom_subject:
            Optional[str] = None Subject of the email sent to subscribers of this
            task.'
        custom_subject:
          type: string
          description: 'Subject of the email sent to subscribers of this task.  pause_subscriptions:
            Optional[bool] = None If true, the dashboard snapshot is not taken, and
            emails are not sent to subscribers.'
        pause_subscriptions:
          type: string
          description: 'If true, the dashboard snapshot is not taken, and emails are
            not sent to subscribers.  subscriptions: Optional[List[SqlTaskSubscription]]
            = None If specified, dashboard snapshots are sent to subscriptions.'
        subscriptions:
          type: string
          description: 'If true, the dashboard snapshot is not taken, and emails are
            not sent to subscribers.  subscriptions: Optional[List[SqlTaskSubscription]]
            = None If specified, dashboard snapshots are sent to subscriptions.'
    SqlTaskFile:
      type: object
      description: 'SqlTaskFile(path: ''str'', source: ''Optional[Source]'' = None)'
      properties:
        path:
          type: string
          description: Path of the SQL file. Must be relative if the source is a remote
            Git repository and absolute for workspace paths.
        source:
          type: string
          description: 'Optional location type of the SQL file. When set to `WORKSPACE`,
            the SQL file will be retrieved from the local Databricks workspace. When
            set to `GIT`, the SQL file will be retrieved from a Git repository defined
            in `git_source`. If the value is empty, the task will use `GIT` if `git_source`
            is defined and `WORKSPACE` otherwise.  * `WORKSPACE`: SQL file is located
            in Databricks workspace. * `GIT`: SQL file is located in cloud Git provider.'
    SqlTaskQuery:
      type: object
      description: 'SqlTaskQuery(query_id: ''str'')'
      properties:
        query_id:
          type: string
          description: 'The canonical identifier of the SQL query.  def as_dict(self)
            -> dict: Serializes the SqlTaskQuery into a dictionary suitable for use
            as a JSON request body.'
    SqlTaskSubscription:
      type: object
      description: 'SqlTaskSubscription(destination_id: ''Optional[str]'' = None,
        user_name: ''Optional[str]'' = None)'
      properties:
        destination_id:
          type: string
          description: The canonical identifier of the destination to receive email
            notification. This parameter is mutually exclusive with user_name. You
            cannot set both destination_id and user_name for subscription notifications.
        user_name:
          type: string
          description: The user name to receive the subscription email. This parameter
            is mutually exclusive with destination_id. You cannot set both destination_id
            and user_name for subscription notifications.
    StorageMode:
      type: string
      description: 'Create a collection of name/value pairs.


        Example enumeration:


        >>> class Color(Enum):

        ...     RED = 1

        ...     BLUE = 2

        ...     GREEN = 3


        Access them by:


        - attribute access::


        >>> Color.RED

        <Color.RED: 1>


        - value lookup:


        >>> Color(1)

        <Color.RED: 1>


        - name lookup:


        >>> Color[''RED'']

        <Color.RED: 1>


        Enumerations can be iterated over, and know how many members they have:


        >>> len(Color)

        3


        >>> list(Color)

        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]


        Methods can be added to enumerations, and members can have their own

        attributes -- see the documentation for details.'
      enum:
      - DIRECT_QUERY
      - DUAL
      - IMPORT
    SubmitRunResponse:
      type: object
      description: Run was created and started successfully.
      properties:
        run_id:
          type: string
          description: 'The canonical identifier for the newly submitted run.  def
            as_dict(self) -> dict: Serializes the SubmitRunResponse into a dictionary
            suitable for use as a JSON request body.'
    SubmitTask:
      type: object
      description: 'SubmitTask(task_key: ''str'', clean_rooms_notebook_task: ''Optional[CleanRoomsNotebookTask]''
        = None, condition_task: ''Optional[ConditionTask]'' = None, dashboard_task:
        ''Optional[DashboardTask]'' = None, dbt_cloud_task: ''Optional[DbtCloudTask]''
        = None, dbt_platform_task: ''Optional[DbtPlatformTask]'' = None, dbt_task:
        ''Optional[DbtTask]'' = None, depends_on: ''Optional[List[TaskDependency]]''
        = None, description: ''Optional[str]'' = None, email_notifications: ''Optional[JobEmailNotifications]''
        = None, environment_key: ''Optional[str]'' = None, existing_cluster_id: ''Optional[str]''
        = None, for_each_task: ''Optional[ForEachTask]'' = None, gen_ai_compute_task:
        ''Optional[GenAiComputeTask]'' = None, health: ''Optional[JobsHealthRules]''
        = None, libraries: ''Optional[List[compute.Library]]'' = None, new_cluster:
        ''Optional[compute.ClusterSpec]'' = None, notebook_task: ''Optional[NotebookTask]''
        = None, notification_settings: ''Optional[TaskNotificationSettings]'' = None,
        pipeline_task: ''Optional[PipelineTask]'' = None, power_bi_task: ''Optional[PowerBiTask]''
        = None, python_wheel_task: ''Optional[PythonWheelTask]'' = None, run_if: ''Optional[RunIf]''
        = None, run_job_task: ''Optional[RunJobTask]'' = None, spark_jar_task: ''Optional[SparkJarTask]''
        = None, spark_python_task: ''Optional[SparkPythonTask]'' = None, spark_submit_task:
        ''Optional[SparkSubmitTask]'' = None, sql_task: ''Optional[SqlTask]'' = None,
        timeout_seconds: ''Optional[int]'' = None, webhook_notifications: ''Optional[WebhookNotifications]''
        = None)'
      properties:
        task_key:
          type: string
          description: A unique name for the task. This field is used to refer to
            this task from other tasks. This field is required and must be unique
            within its parent job. On Update or Reset, this field is used to reference
            the tasks to be updated or reset.
        clean_rooms_notebook_task:
          type: string
          description: 'The task runs a [clean rooms] notebook when the `clean_rooms_notebook_task`
            field is present.  [clean rooms]: https://docs.databricks.com/clean-rooms/index.html'
        condition_task:
          type: string
          description: The task evaluates a condition that can be used to control
            the execution of other tasks when the `condition_task` field is present.
            The condition task does not require a cluster to execute and does not
            support retries or notifications.
        dashboard_task:
          type: string
          description: 'The task refreshes a dashboard and sends a snapshot to subscribers.  dbt_cloud_task:
            Optional[DbtCloudTask] = None Task type for dbt cloud, deprecated in favor
            of the new name dbt_platform_task'
        dbt_cloud_task:
          type: string
          description: 'Task type for dbt cloud, deprecated in favor of the new name
            dbt_platform_task  dbt_platform_task: Optional[DbtPlatformTask] = None  dbt_task:
            Optional[DbtTask] = None """The task runs one or more dbt commands when
            the `dbt_task` field is present. The dbt task requires both Databricks
            SQL and the ability to use a serverless or a pro SQL warehouse.'
        dbt_platform_task:
          type: string
          description: ''
        dbt_task:
          type: string
          description: The task runs one or more dbt commands when the `dbt_task`
            field is present. The dbt task requires both Databricks SQL and the ability
            to use a serverless or a pro SQL warehouse.
        depends_on:
          type: string
          description: An optional array of objects specifying the dependency graph
            of the task. All tasks specified in this field must complete successfully
            before executing this task. The key is `task_key`, and the value is the
            name assigned to the dependent task.
        description:
          type: string
          description: 'An optional description for this task.  email_notifications:
            Optional[JobEmailNotifications] = None """An optional set of email addresses
            notified when the task run begins or completes. The default behavior is
            to not send any emails.'
        email_notifications:
          type: string
          description: An optional set of email addresses notified when the task run
            begins or completes. The default behavior is to not send any emails.
        environment_key:
          type: string
          description: The key that references an environment spec in a job. This
            field is required for Python script, Python wheel and dbt tasks when using
            serverless compute.
        existing_cluster_id:
          type: string
          description: If existing_cluster_id, the ID of an existing cluster that
            is used for all runs. When running jobs or tasks on an existing cluster,
            you may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater reliability
        for_each_task:
          type: string
          description: The task executes a nested task for every input provided when
            the `for_each_task` field is present.
        gen_ai_compute_task:
          type: string
          description: ''
        health:
          type: string
          description: ''
        libraries:
          type: string
          description: An optional list of libraries to be installed on the cluster.
            The default value is an empty list.
        new_cluster:
          type: string
          description: 'If new_cluster, a description of a new cluster that is created
            for each run.  notebook_task: Optional[NotebookTask] = None The task runs
            a notebook when the `notebook_task` field is present.'
        notebook_task:
          type: string
          description: 'The task runs a [clean rooms] notebook when the `clean_rooms_notebook_task`
            field is present.  [clean rooms]: https://docs.databricks.com/clean-rooms/index.html'
        notification_settings:
          type: string
          description: Optional notification settings that are used when sending notifications
            to each of the `email_notifications` and `webhook_notifications` for this
            task run.
        pipeline_task:
          type: string
          description: The task triggers a pipeline update when the `pipeline_task`
            field is present. Only pipelines configured to use triggered more are
            supported.
        power_bi_task:
          type: string
          description: 'The task triggers a Power BI semantic model update when the
            `power_bi_task` field is present.  python_wheel_task: Optional[PythonWheelTask]
            = None The task runs a Python wheel when the `python_wheel_task` field
            is present.'
        python_wheel_task:
          type: string
          description: 'The task runs a Python wheel when the `python_wheel_task`
            field is present.  run_if: Optional[RunIf] = None """An optional value
            indicating the condition that determines whether the task should be run
            once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`.
            See :method:jobs/create for a list of possible values.'
        run_if:
          type: string
          description: An optional value indicating the condition that determines
            whether the task should be run once its dependencies have been completed.
            When omitted, defaults to `ALL_SUCCESS`. See :method:jobs/create for a
            list of possible values.
        run_job_task:
          type: string
          description: 'The task triggers another job when the `run_job_task` field
            is present.  spark_jar_task: Optional[SparkJarTask] = None The task runs
            a JAR when the `spark_jar_task` field is present.'
        spark_jar_task:
          type: string
          description: 'The task runs a JAR when the `spark_jar_task` field is present.  spark_python_task:
            Optional[SparkPythonTask] = None The task runs a Python file when the
            `spark_python_task` field is present.'
        spark_python_task:
          type: string
          description: 'The task runs a Python file when the `spark_python_task` field
            is present.  spark_submit_task: Optional[SparkSubmitTask] = None """(Legacy)
            The task runs the spark-submit script when the spark_submit_task field
            is present. Databricks recommends using the spark_jar_task instead; see
            [Spark Submit task for jobs](/jobs/spark-submit).'
        spark_submit_task:
          type: string
          description: (Legacy) The task runs the spark-submit script when the spark_submit_task
            field is present. Databricks recommends using the spark_jar_task instead;
            see [Spark Submit task for jobs](/jobs/spark-submit).
        sql_task:
          type: string
          description: The task runs a SQL query or file, or it refreshes a SQL alert
            or a legacy SQL dashboard when the `sql_task` field is present.
        timeout_seconds:
          type: string
          description: 'An optional timeout applied to each run of this job task.
            A value of `0` means no timeout.  webhook_notifications: Optional[WebhookNotifications]
            = None """A collection of system notification IDs to notify when the run
            begins or completes. The default behavior is to not send any system notifications.
            Task webhooks respect the task notification settings.'
        webhook_notifications:
          type: string
          description: A collection of system notification IDs to notify when the
            run begins or completes. The default behavior is to not send any system
            notifications. Task webhooks respect the task notification settings.
    Subscription:
      type: object
      description: 'Subscription(custom_subject: ''Optional[str]'' = None, paused:
        ''Optional[bool]'' = None, subscribers: ''Optional[List[SubscriptionSubscriber]]''
        = None)'
      properties:
        custom_subject:
          type: string
          description: 'Optional: Allows users to specify a custom subject line on
            the email sent to subscribers.  paused: Optional[bool] = None When true,
            the subscription will not send emails.'
        paused:
          type: string
          description: 'When true, the subscription will not send emails.  subscribers:
            Optional[List[SubscriptionSubscriber]] = None The list of subscribers
            to send the snapshot of the dashboard to.'
        subscribers:
          type: string
          description: 'The list of subscribers to send the snapshot of the dashboard
            to.  def as_dict(self) -> dict: Serializes the Subscription into a dictionary
            suitable for use as a JSON request body.'
    SubscriptionSubscriber:
      type: object
      description: 'SubscriptionSubscriber(destination_id: ''Optional[str]'' = None,
        user_name: ''Optional[str]'' = None)'
      properties:
        destination_id:
          type: string
          description: A snapshot of the dashboard will be sent to the destination
            when the `destination_id` field is present.
        user_name:
          type: string
          description: A snapshot of the dashboard will be sent to the user's email
            when the `user_name` field is present.
    TableState:
      type: object
      description: 'TableState(has_seen_updates: ''Optional[bool]'' = None, table_name:
        ''Optional[str]'' = None)'
      properties:
        has_seen_updates:
          type: string
          description: Whether or not the table has seen updates since either the
            creation of the trigger or the last successful evaluation of the trigger
        table_name:
          type: string
          description: 'Full table name of the table to monitor, e.g. `mycatalog.myschema.mytable`  def
            as_dict(self) -> dict: Serializes the TableState into a dictionary suitable
            for use as a JSON request body.'
    TableTriggerState:
      type: object
      description: 'TableTriggerState(last_seen_table_states: ''Optional[List[TableState]]''
        = None, using_scalable_monitoring: ''Optional[bool]'' = None)'
      properties:
        last_seen_table_states:
          type: string
          description: ''
        using_scalable_monitoring:
          type: string
          description: 'Indicates whether the trigger is using scalable monitoring.  def
            as_dict(self) -> dict: Serializes the TableTriggerState into a dictionary
            suitable for use as a JSON request body.'
    TableUpdateTriggerConfiguration:
      type: object
      description: 'TableUpdateTriggerConfiguration(condition: ''Optional[Condition]''
        = None, min_time_between_triggers_seconds: ''Optional[int]'' = None, table_names:
        ''Optional[List[str]]'' = None, wait_after_last_change_seconds: ''Optional[int]''
        = None)'
      properties:
        condition:
          type: string
          description: 'The table(s) condition based on which to trigger a job run.  min_time_between_triggers_seconds:
            Optional[int] = None """If set, the trigger starts a run only after the
            specified amount of time has passed since the last time the trigger fired.
            The minimum allowed value is 60 seconds.'
        min_time_between_triggers_seconds:
          type: string
          description: If set, the trigger starts a run only after the specified amount
            of time has passed since the last time the trigger fired. The minimum
            allowed value is 60 seconds.
        table_names:
          type: string
          description: A list of tables to monitor for changes. The table name must
            be in the format `catalog_name.schema_name.table_name`.
        wait_after_last_change_seconds:
          type: string
          description: If set, the trigger starts a run only after no table updates
            have occurred for the specified time and can be used to wait for a series
            of table updates before triggering a run. The minimum allowed value is
            60 seconds.
    Task:
      type: object
      description: 'Task(task_key: ''str'', clean_rooms_notebook_task: ''Optional[CleanRoomsNotebookTask]''
        = None, condition_task: ''Optional[ConditionTask]'' = None, dashboard_task:
        ''Optional[DashboardTask]'' = None, dbt_cloud_task: ''Optional[DbtCloudTask]''
        = None, dbt_platform_task: ''Optional[DbtPlatformTask]'' = None, dbt_task:
        ''Optional[DbtTask]'' = None, depends_on: ''Optional[List[TaskDependency]]''
        = None, description: ''Optional[str]'' = None, disable_auto_optimization:
        ''Optional[bool]'' = None, disabled: ''Optional[bool]'' = None, email_notifications:
        ''Optional[TaskEmailNotifications]'' = None, environment_key: ''Optional[str]''
        = None, existing_cluster_id: ''Optional[str]'' = None, for_each_task: ''Optional[ForEachTask]''
        = None, gen_ai_compute_task: ''Optional[GenAiComputeTask]'' = None, health:
        ''Optional[JobsHealthRules]'' = None, job_cluster_key: ''Optional[str]'' =
        None, libraries: ''Optional[List[compute.Library]]'' = None, max_retries:
        ''Optional[int]'' = None, min_retry_interval_millis: ''Optional[int]'' = None,
        new_cluster: ''Optional[compute.ClusterSpec]'' = None, notebook_task: ''Optional[NotebookTask]''
        = None, notification_settings: ''Optional[TaskNotificationSettings]'' = None,
        pipeline_task: ''Optional[PipelineTask]'' = None, power_bi_task: ''Optional[PowerBiTask]''
        = None, python_wheel_task: ''Optional[PythonWheelTask]'' = None, retry_on_timeout:
        ''Optional[bool]'' = None, run_if: ''Optional[RunIf]'' = None, run_job_task:
        ''Optional[RunJobTask]'' = None, spark_jar_task: ''Optional[SparkJarTask]''
        = None, spark_python_task: ''Optional[SparkPythonTask]'' = None, spark_submit_task:
        ''Optional[SparkSubmitTask]'' = None, sql_task: ''Optional[SqlTask]'' = None,
        timeout_seconds: ''Optional[int]'' = None, webhook_notifications: ''Optional[WebhookNotifications]''
        = None)'
      properties:
        task_key:
          type: string
          description: A unique name for the task. This field is used to refer to
            this task from other tasks. This field is required and must be unique
            within its parent job. On Update or Reset, this field is used to reference
            the tasks to be updated or reset.
        clean_rooms_notebook_task:
          type: string
          description: 'The task runs a [clean rooms] notebook when the `clean_rooms_notebook_task`
            field is present.  [clean rooms]: https://docs.databricks.com/clean-rooms/index.html'
        condition_task:
          type: string
          description: The task evaluates a condition that can be used to control
            the execution of other tasks when the `condition_task` field is present.
            The condition task does not require a cluster to execute and does not
            support retries or notifications.
        dashboard_task:
          type: string
          description: 'The task refreshes a dashboard and sends a snapshot to subscribers.  dbt_cloud_task:
            Optional[DbtCloudTask] = None Task type for dbt cloud, deprecated in favor
            of the new name dbt_platform_task'
        dbt_cloud_task:
          type: string
          description: 'Task type for dbt cloud, deprecated in favor of the new name
            dbt_platform_task  dbt_platform_task: Optional[DbtPlatformTask] = None  dbt_task:
            Optional[DbtTask] = None """The task runs one or more dbt commands when
            the `dbt_task` field is present. The dbt task requires both Databricks
            SQL and the ability to use a serverless or a pro SQL warehouse.'
        dbt_platform_task:
          type: string
          description: ''
        dbt_task:
          type: string
          description: The task runs one or more dbt commands when the `dbt_task`
            field is present. The dbt task requires both Databricks SQL and the ability
            to use a serverless or a pro SQL warehouse.
        depends_on:
          type: string
          description: An optional array of objects specifying the dependency graph
            of the task. All tasks specified in this field must complete before executing
            this task. The task will run only if the `run_if` condition is true. The
            key is `task_key`, and the value is the name assigned to the dependent
            task.
        description:
          type: string
          description: 'An optional description for this task.  disable_auto_optimization:
            Optional[bool] = None An option to disable auto optimization in serverless'
        disable_auto_optimization:
          type: string
          description: 'An option to disable auto optimization in serverless  disabled:
            Optional[bool] = None """An optional flag to disable the task. If set
            to true, the task will not run even if it is part of a job.'
        disabled:
          type: string
          description: An optional flag to disable the task. If set to true, the task
            will not run even if it is part of a job.
        email_notifications:
          type: string
          description: An optional set of email addresses that is notified when runs
            of this task begin or complete as well as when this task is deleted. The
            default behavior is to not send any emails.
        environment_key:
          type: string
          description: The key that references an environment spec in a job. This
            field is required for Python script, Python wheel and dbt tasks when using
            serverless compute.
        existing_cluster_id:
          type: string
          description: If existing_cluster_id, the ID of an existing cluster that
            is used for all runs. When running jobs or tasks on an existing cluster,
            you may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater reliability
        for_each_task:
          type: string
          description: The task executes a nested task for every input provided when
            the `for_each_task` field is present.
        gen_ai_compute_task:
          type: string
          description: ''
        health:
          type: string
          description: ''
        job_cluster_key:
          type: string
          description: If job_cluster_key, this task is executed reusing the cluster
            specified in `job.settings.job_clusters`.
        libraries:
          type: string
          description: An optional list of libraries to be installed on the cluster.
            The default value is an empty list.
        max_retries:
          type: string
          description: An optional maximum number of times to retry an unsuccessful
            run. A run is considered to be unsuccessful if it completes with the `FAILED`
            result_state or `INTERNAL_ERROR` `life_cycle_state`. The value `-1` means
            to retry indefinitely and the value `0` means to never retry.
        min_retry_interval_millis:
          type: string
          description: An optional minimal interval in milliseconds between the start
            of the failed run and the subsequent retry run. The default behavior is
            that unsuccessful runs are immediately retried.
        new_cluster:
          type: string
          description: 'If new_cluster, a description of a new cluster that is created
            for each run.  notebook_task: Optional[NotebookTask] = None The task runs
            a notebook when the `notebook_task` field is present.'
        notebook_task:
          type: string
          description: 'The task runs a [clean rooms] notebook when the `clean_rooms_notebook_task`
            field is present.  [clean rooms]: https://docs.databricks.com/clean-rooms/index.html'
        notification_settings:
          type: string
          description: Optional notification settings that are used when sending notifications
            to each of the `email_notifications` and `webhook_notifications` for this
            task.
        pipeline_task:
          type: string
          description: The task triggers a pipeline update when the `pipeline_task`
            field is present. Only pipelines configured to use triggered more are
            supported.
        power_bi_task:
          type: string
          description: 'The task triggers a Power BI semantic model update when the
            `power_bi_task` field is present.  python_wheel_task: Optional[PythonWheelTask]
            = None The task runs a Python wheel when the `python_wheel_task` field
            is present.'
        python_wheel_task:
          type: string
          description: 'The task runs a Python wheel when the `python_wheel_task`
            field is present.  retry_on_timeout: Optional[bool] = None """An optional
            policy to specify whether to retry a job when it times out. The default
            behavior is to not retry on timeout.'
        retry_on_timeout:
          type: string
          description: An optional policy to specify whether to retry a job when it
            times out. The default behavior is to not retry on timeout.
        run_if:
          type: string
          description: 'An optional value specifying the condition determining whether
            the task is run once its dependencies have been completed.  * `ALL_SUCCESS`:
            All dependencies have executed and succeeded * `AT_LEAST_ONE_SUCCESS`:
            At least one dependency has succeeded * `NONE_FAILED`: None of the dependencies
            have failed and at least one was executed * `ALL_DONE`: All dependencies
            have been completed * `AT_LEAST_ONE_FAILED`: At least one dependency failed
            * `ALL_FAILED`: ALl dependencies have failed'
        run_job_task:
          type: string
          description: 'The task triggers another job when the `run_job_task` field
            is present.  spark_jar_task: Optional[SparkJarTask] = None The task runs
            a JAR when the `spark_jar_task` field is present.'
        spark_jar_task:
          type: string
          description: 'The task runs a JAR when the `spark_jar_task` field is present.  spark_python_task:
            Optional[SparkPythonTask] = None The task runs a Python file when the
            `spark_python_task` field is present.'
        spark_python_task:
          type: string
          description: 'The task runs a Python file when the `spark_python_task` field
            is present.  spark_submit_task: Optional[SparkSubmitTask] = None """(Legacy)
            The task runs the spark-submit script when the spark_submit_task field
            is present. Databricks recommends using the spark_jar_task instead; see
            [Spark Submit task for jobs](/jobs/spark-submit).'
        spark_submit_task:
          type: string
          description: (Legacy) The task runs the spark-submit script when the spark_submit_task
            field is present. Databricks recommends using the spark_jar_task instead;
            see [Spark Submit task for jobs](/jobs/spark-submit).
        sql_task:
          type: string
          description: The task runs a SQL query or file, or it refreshes a SQL alert
            or a legacy SQL dashboard when the `sql_task` field is present.
        timeout_seconds:
          type: string
          description: 'An optional timeout applied to each run of this job task.
            A value of `0` means no timeout.  webhook_notifications: Optional[WebhookNotifications]
            = None """A collection of system notification IDs to notify when runs
            of this task begin or complete. The default behavior is to not send any
            system notifications.'
        webhook_notifications:
          type: string
          description: A collection of system notification IDs to notify when runs
            of this task begin or complete. The default behavior is to not send any
            system notifications.
    TaskDependency:
      type: object
      description: 'TaskDependency(task_key: ''str'', outcome: ''Optional[str]'' =
        None)'
      properties:
        task_key:
          type: string
          description: 'The name of the task this task depends on.  outcome: Optional[str]
            = None """Can only be specified on condition task dependencies. The outcome
            of the dependent task that must be met for this task to run.'
        outcome:
          type: string
          description: Can only be specified on condition task dependencies. The outcome
            of the dependent task that must be met for this task to run.
    TaskEmailNotifications:
      type: object
      description: 'TaskEmailNotifications(no_alert_for_skipped_runs: ''Optional[bool]''
        = None, on_duration_warning_threshold_exceeded: ''Optional[List[str]]'' =
        None, on_failure: ''Optional[List[str]]'' = None, on_start: ''Optional[List[str]]''
        = None, on_streaming_backlog_exceeded: ''Optional[List[str]]'' = None, on_success:
        ''Optional[List[str]]'' = None)'
      properties:
        no_alert_for_skipped_runs:
          type: string
          description: If true, do not send email to recipients specified in `on_failure`
            if the run is skipped. This field is `deprecated`. Please use the `notification_settings.no_alert_for_skipped_runs`
            field.
        on_duration_warning_threshold_exceeded:
          type: string
          description: A list of email addresses to be notified when the duration
            of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS`
            metric in the `health` field. If no rule for the `RUN_DURATION_SECONDS`
            metric is specified in the `health` field for the job, notifications are
            not sent.
        on_failure:
          type: string
          description: A list of email addresses to be notified when a run unsuccessfully
            completes. A run is considered to have completed unsuccessfully if it
            ends with an `INTERNAL_ERROR` `life_cycle_state` or a `FAILED`, or `TIMED_OUT`
            result_state. If this is not specified on job creation, reset, or update
            the list is empty, and notifications are not sent.
        on_start:
          type: string
          description: A list of email addresses to be notified when a run begins.
            If not specified on job creation, reset, or update, the list is empty,
            and notifications are not sent.
        on_streaming_backlog_exceeded:
          type: string
          description: 'A list of email addresses to notify when any streaming backlog
            thresholds are exceeded for any stream. Streaming backlog thresholds can
            be set in the `health` field using the following metrics: `STREAMING_BACKLOG_BYTES`,
            `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`.
            Alerting is based on the 10-minute average of these metrics. If the issue
            persists, notifications are resent every 30 minutes.'
        on_success:
          type: string
          description: A list of email addresses to be notified when a run successfully
            completes. A run is considered to have completed successfully if it ends
            with a `TERMINATED` `life_cycle_state` and a `SUCCESS` result_state. If
            not specified on job creation, reset, or update, the list is empty, and
            notifications are not sent.
    TaskNotificationSettings:
      type: object
      description: 'TaskNotificationSettings(alert_on_last_attempt: ''Optional[bool]''
        = None, no_alert_for_canceled_runs: ''Optional[bool]'' = None, no_alert_for_skipped_runs:
        ''Optional[bool]'' = None)'
      properties:
        alert_on_last_attempt:
          type: string
          description: If true, do not send notifications to recipients specified
            in `on_start` for the retried runs and do not send notifications to recipients
            specified in `on_failure` until the last retry of the run.
        no_alert_for_canceled_runs:
          type: string
          description: If true, do not send notifications to recipients specified
            in `on_failure` if the run is canceled.
        no_alert_for_skipped_runs:
          type: string
          description: If true, do not send notifications to recipients specified
            in `on_failure` if the run is skipped.
    TaskRetryMode:
      type: string
      description: 'task retry mode of the continuous job * NEVER: The failed task
        will not be retried. *

        ON_FAILURE: Retry a failed task if at least one other task in the job is still
        running its first

        attempt. When this condition is no longer met or the retry limit is reached,
        the job run is

        cancelled and a new run is started.'
      enum:
      - NEVER
      - ON_FAILURE
    TerminationCodeCode:
      type: string
      description: 'The code indicates why the run was terminated. Additional codes
        might be introduced in future

        releases. * `SUCCESS`: The run was completed successfully. * `SUCCESS_WITH_FAILURES`:
        The run

        was completed successfully but some child runs failed. * `USER_CANCELED`:
        The run was

        successfully canceled during execution by a user. * `CANCELED`: The run was
        canceled during

        execution by the Databricks platform; for example, if the maximum run duration
        was exceeded. *

        `SKIPPED`: Run was never executed, for example, if the upstream task run failed,
        the dependency

        type condition was not met, or there were no material tasks to execute. *
        `INTERNAL_ERROR`: The

        run encountered an unexpected error. Refer to the state message for further
        details. *

        `DRIVER_ERROR`: The run encountered an error while communicating with the
        Spark Driver. *

        `CLUSTER_ERROR`: The run failed due to a cluster error. Refer to the state
        message for further

        details. * `REPOSITORY_CHECKOUT_FAILED`: Failed to complete the checkout due
        to an error when

        communicating with the third party service. * `INVALID_CLUSTER_REQUEST`: The
        run failed because

        it issued an invalid request to start the cluster. * `WORKSPACE_RUN_LIMIT_EXCEEDED`:
        The

        workspace has reached the quota for the maximum number of concurrent active
        runs. Consider

        scheduling the runs over a larger time frame. * `FEATURE_DISABLED`: The run
        failed because it

        tried to access a feature unavailable for the workspace. * `CLUSTER_REQUEST_LIMIT_EXCEEDED`:
        The

        number of cluster creation, start, and upsize requests have exceeded the allotted
        rate limit.

        Consider spreading the run execution over a larger time frame. * `STORAGE_ACCESS_ERROR`:
        The run

        failed due to an error when accessing the customer blob storage. Refer to
        the state message for

        further details. * `RUN_EXECUTION_ERROR`: The run was completed with task
        failures. For more

        details, refer to the state message or run output. * `UNAUTHORIZED_ERROR`:
        The run failed due to

        a permission issue while accessing a resource. Refer to the state message
        for further details. *

        `LIBRARY_INSTALLATION_ERROR`: The run failed while installing the user-requested
        library. Refer

        to the state message for further details. The causes might include, but are
        not limited to: The

        provided library is invalid, there are insufficient permissions to install
        the library, and so

        forth. * `MAX_CONCURRENT_RUNS_EXCEEDED`: The scheduled run exceeds the limit
        of maximum

        concurrent runs set for the job. * `MAX_SPARK_CONTEXTS_EXCEEDED`: The run
        is scheduled on a

        cluster that has already reached the maximum number of contexts it is configured
        to create. See:

        [Link]. * `RESOURCE_NOT_FOUND`: A resource necessary for run execution does
        not exist. Refer to

        the state message for further details. * `INVALID_RUN_CONFIGURATION`: The
        run failed due to an

        invalid configuration. Refer to the state message for further details. * `CLOUD_FAILURE`:
        The

        run failed due to a cloud provider issue. Refer to the state message for further
        details. *

        `MAX_JOB_QUEUE_SIZE_EXCEEDED`: The run was skipped due to reaching the job
        level queue size

        limit. * `DISABLED`: The run was never executed because it was disabled explicitly
        by the user.

        * `BREAKING_CHANGE`: Run failed because of an intentional breaking change
        in Spark, but it will

        be retried with a mitigation config.


        [Link]: https://kb.databricks.com/en_US/notebooks/too-many-execution-contexts-are-open-right-now'
      enum:
      - BUDGET_POLICY_LIMIT_EXCEEDED
      - CANCELED
      - CLOUD_FAILURE
      - CLUSTER_ERROR
      - CLUSTER_REQUEST_LIMIT_EXCEEDED
      - DISABLED
      - DRIVER_ERROR
      - FEATURE_DISABLED
      - INTERNAL_ERROR
      - INVALID_CLUSTER_REQUEST
      - INVALID_RUN_CONFIGURATION
      - LIBRARY_INSTALLATION_ERROR
      - MAX_CONCURRENT_RUNS_EXCEEDED
      - MAX_JOB_QUEUE_SIZE_EXCEEDED
      - MAX_SPARK_CONTEXTS_EXCEEDED
      - REPOSITORY_CHECKOUT_FAILED
      - RESOURCE_NOT_FOUND
      - RUN_EXECUTION_ERROR
      - SKIPPED
      - STORAGE_ACCESS_ERROR
      - SUCCESS
      - SUCCESS_WITH_FAILURES
      - UNAUTHORIZED_ERROR
      - USER_CANCELED
      - WORKSPACE_RUN_LIMIT_EXCEEDED
    TerminationDetails:
      type: object
      description: 'TerminationDetails(code: ''Optional[TerminationCodeCode]'' = None,
        message: ''Optional[str]'' = None, type: ''Optional[TerminationTypeType]''
        = None)'
      properties:
        code:
          type: string
          description: ''
        message:
          type: string
          description: A descriptive message with the termination details. This field
            is unstructured and the format might change.
        type:
          type: string
          description: ''
    TerminationTypeType:
      type: string
      description: '* `SUCCESS`: The run terminated without any issues * `INTERNAL_ERROR`:
        An error occurred in the

        Databricks platform. Please look at the [status page] or contact support if
        the issue persists.

        * `CLIENT_ERROR`: The run was terminated because of an error caused by user
        input or the job

        configuration. * `CLOUD_FAILURE`: The run was terminated because of an issue
        with your cloud

        provider.


        [status page]: https://status.databricks.com/'
      enum:
      - CLIENT_ERROR
      - CLOUD_FAILURE
      - INTERNAL_ERROR
      - SUCCESS
    TriggerInfo:
      type: object
      description: Additional details about what triggered the run
      properties:
        run_id:
          type: string
          description: 'The run id of the Run Job task run  def as_dict(self) -> dict:
            Serializes the TriggerInfo into a dictionary suitable for use as a JSON
            request body.'
    TriggerSettings:
      type: object
      description: 'TriggerSettings(file_arrival: ''Optional[FileArrivalTriggerConfiguration]''
        = None, pause_status: ''Optional[PauseStatus]'' = None, periodic: ''Optional[PeriodicTriggerConfiguration]''
        = None, table: ''Optional[TableUpdateTriggerConfiguration]'' = None, table_update:
        ''Optional[TableUpdateTriggerConfiguration]'' = None)'
      properties:
        file_arrival:
          type: string
          description: 'File arrival trigger settings.  pause_status: Optional[PauseStatus]
            = None Whether this trigger is paused or not.'
        pause_status:
          type: string
          description: 'Whether this trigger is paused or not.  periodic: Optional[PeriodicTriggerConfiguration]
            = None Periodic trigger settings.'
        periodic:
          type: string
          description: 'Periodic trigger settings.  table: Optional[TableUpdateTriggerConfiguration]
            = None Old table trigger settings name. Deprecated in favor of `table_update`.'
        table:
          type: string
          description: 'Old table trigger settings name. Deprecated in favor of `table_update`.  table_update:
            Optional[TableUpdateTriggerConfiguration] = None  def as_dict(self) ->
            dict: Serializes the TriggerSettings into a dictionary suitable for use
            as a JSON request body.'
        table_update:
          type: string
          description: ''
    TriggerStateProto:
      type: object
      description: 'TriggerStateProto(file_arrival: ''Optional[FileArrivalTriggerState]''
        = None, table: ''Optional[TableTriggerState]'' = None)'
      properties:
        file_arrival:
          type: string
          description: ''
        table:
          type: string
          description: ''
    TriggerType:
      type: string
      description: 'The type of trigger that fired this run.


        * `PERIODIC`: Schedules that periodically trigger runs, such as a cron scheduler.
        * `ONE_TIME`:

        One time triggers that fire a single run. This occurs you triggered a single
        run on demand

        through the UI or the API. * `RETRY`: Indicates a run that is triggered as
        a retry of a

        previously failed run. This occurs when you request to re-run the job in case
        of failures. *

        `RUN_JOB_TASK`: Indicates a run that is triggered using a Run Job task. *
        `FILE_ARRIVAL`:

        Indicates a run that is triggered by a file arrival. * `CONTINUOUS`: Indicates
        a run that is

        triggered by a continuous job. * `TABLE`: Indicates a run that is triggered
        by a table update. *

        `CONTINUOUS_RESTART`: Indicates a run created by user to manually restart
        a continuous job run.

        * `MODEL`: Indicates a run that is triggered by a model update.'
      enum:
      - CONTINUOUS
      - CONTINUOUS_RESTART
      - FILE_ARRIVAL
      - ONE_TIME
      - PERIODIC
      - RETRY
      - RUN_JOB_TASK
      - TABLE
    UpdateResponse:
      type: object
      description: UpdateResponse()
      properties: {}
    ViewItem:
      type: object
      description: 'ViewItem(content: ''Optional[str]'' = None, name: ''Optional[str]''
        = None, type: ''Optional[ViewType]'' = None)'
      properties:
        content:
          type: string
          description: 'Content of the view.  name: Optional[str] = None """Name of
            the view item. In the case of code view, it would be the notebook’s name.
            In the case of dashboard view, it would be the dashboard’s name.'
        name:
          type: string
          description: Name of the view item. In the case of code view, it would be
            the notebook’s name. In the case of dashboard view, it would be the dashboard’s
            name.
        type:
          type: string
          description: 'Type of the view item.  def as_dict(self) -> dict: Serializes
            the ViewItem into a dictionary suitable for use as a JSON request body.'
    ViewType:
      type: string
      description: '* `NOTEBOOK`: Notebook view item. * `DASHBOARD`: Dashboard view
        item.'
      enum:
      - DASHBOARD
      - NOTEBOOK
    ViewsToExport:
      type: string
      description: '* `CODE`: Code view of the notebook. * `DASHBOARDS`: All dashboard
        views of the notebook. *

        `ALL`: All views of the notebook.'
      enum:
      - ALL
      - CODE
      - DASHBOARDS
    Webhook:
      type: object
      description: 'Webhook(id: ''str'')'
      properties:
        id:
          type: string
          description: ''
    WebhookNotifications:
      type: object
      description: 'WebhookNotifications(on_duration_warning_threshold_exceeded: ''Optional[List[Webhook]]''
        = None, on_failure: ''Optional[List[Webhook]]'' = None, on_start: ''Optional[List[Webhook]]''
        = None, on_streaming_backlog_exceeded: ''Optional[List[Webhook]]'' = None,
        on_success: ''Optional[List[Webhook]]'' = None)'
      properties:
        on_duration_warning_threshold_exceeded:
          type: string
          description: An optional list of system notification IDs to call when the
            duration of a run exceeds the threshold specified for the `RUN_DURATION_SECONDS`
            metric in the `health` field. A maximum of 3 destinations can be specified
            for the `on_duration_warning_threshold_exceeded` property.
        on_failure:
          type: string
          description: An optional list of system notification IDs to call when the
            run fails. A maximum of 3 destinations can be specified for the `on_failure`
            property.
        on_start:
          type: string
          description: An optional list of system notification IDs to call when the
            run starts. A maximum of 3 destinations can be specified for the `on_start`
            property.
        on_streaming_backlog_exceeded:
          type: string
          description: 'An optional list of system notification IDs to call when any
            streaming backlog thresholds are exceeded for any stream. Streaming backlog
            thresholds can be set in the `health` field using the following metrics:
            `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`,
            or `STREAMING_BACKLOG_FILES`. Alerting is based on the 10-minute average
            of these metrics. If the issue persists, notifications are resent every
            30 minutes. A maximum of 3 destinations can be specified for the `on_streaming_backlog_exceeded`
            property.'
        on_success:
          type: string
          description: An optional list of system notification IDs to call when the
            run completes successfully. A maximum of 3 destinations can be specified
            for the `on_success` property.
    WidgetErrorDetail:
      type: object
      description: 'WidgetErrorDetail(message: ''Optional[str]'' = None)'
      properties:
        message:
          type: string
          description: ''
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
      description: Databricks personal access token
security:
- bearerAuth: []
