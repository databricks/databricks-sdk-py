filename,path,operationId,verb,response_object,tags,params,summary,description,stackql_resource_name,stackql_method_name,stackql_verb,stackql_object_key
jobs.json,/api/2.0/permissions/jobs/{job_id}/permissionLevels,jobs_get_permission_levels,get,GetJobPermissionLevelsResponse,jobs,job_id,Gets the permission levels that a user can have on an object.,Gets the permission levels that a user can have on an object.  :param job_id: str   The job for which to get or manage permissions.  :returns: :class:`GetJobPermissionLevelsResponse`,job_permission_levels,get,select,
jobs.json,/api/2.0/permissions/jobs/{job_id},jobs_get_permissions,get,JobPermissions,jobs,job_id,Gets the permissions of a job. Jobs can inherit permissions from their root object.,Gets the permissions of a job. Jobs can inherit permissions from their root object.  :param job_id: str   The job for which to get or manage permissions.  :returns: :class:`JobPermissions`,job_permissions,get,select,
jobs.json,/api/2.0/permissions/jobs/{job_id},jobs_set_permissions,put,JobPermissions,jobs,"job_id, access_control_list","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct permissions if none are specified. Objects can inherit permissions from their root object.  :param job_id: str   The job for which to get or manage permissions. :param access_control_list: List[:class:`JobAccessControlRequest`] (optional)  :returns: :class:`JobPermissions`",job_permissions,set,replace,
jobs.json,/api/2.0/permissions/jobs/{job_id},jobs_update_permissions,patch,JobPermissions,jobs,"job_id, access_control_list",Updates the permissions on a job. Jobs can inherit permissions from their root object.,Updates the permissions on a job. Jobs can inherit permissions from their root object.  :param job_id: str   The job for which to get or manage permissions. :param access_control_list: List[:class:`JobAccessControlRequest`] (optional)  :returns: :class:`JobPermissions`,job_permissions,update,update,
jobs.json,/api/2.2/jobs/runs/cancel-all,jobs_cancel_all_runs,post,,jobs,"all_queued_runs, job_id","Cancels all active runs of a job. The runs are canceled asynchronously, so it doesn't prevent new runs","Cancels all active runs of a job. The runs are canceled asynchronously, so it doesn't prevent new runs from being started.  :param all_queued_runs: bool (optional)   Optional boolean parameter to cancel all queued runs. If no job_id is provided, all queued runs in   the workspace are canceled. :param job_id: int (optional)   The canonical identifier of the job to cancel all runs of.",job_runs,cancel_all,exec,
jobs.json,/api/2.2/jobs/runs/cancel,jobs_cancel_run,post,Wait[Run],jobs,run_id,"Cancels a job run or a task run. The run is canceled asynchronously, so it may still be running when","Cancels a job run or a task run. The run is canceled asynchronously, so it may still be running when this request completes.  :param run_id: int   This field is required.  :returns:   Long-running operation waiter for :class:`Run`.   See :method:wait_get_run_job_terminated_or_skipped for more details.",job_runs,cancel,exec,
jobs.json,/api/2.2/jobs/runs/delete,jobs_delete_run,post,,jobs,run_id,Deletes a non-active run. Returns an error if the run is active.,Deletes a non-active run. Returns an error if the run is active.  :param run_id: int   ID of the run to delete.,job_runs,delete,delete,
jobs.json,/api/2.2/jobs/runs/export,jobs_export_run,get,ExportRunOutput,jobs,"run_id, views_to_export",Export and retrieve the job run task.,"Export and retrieve the job run task.  :param run_id: int   The canonical identifier for the run. This field is required. :param views_to_export: :class:`ViewsToExport` (optional)   Which views to export (CODE, DASHBOARDS, or ALL). Defaults to CODE.  :returns: :class:`ExportRunOutput`",job_runs,export,exec,
jobs.json,/api/2.2/jobs/runs/get,jobs_get_run,get,Run,jobs,"run_id, include_history, include_resolved_values, page_token",Retrieves the metadata of a run.,"Retrieves the metadata of a run.  Large arrays in the results will be paginated when they exceed 100 elements. A request for a single run will return all properties for that run, and the first 100 elements of array properties (`tasks`, `job_clusters`, `job_parameters` and `repair_history`). Use the next_page_token field to check for more results and pass its value as the page_token in subsequent requests. If any array properties have more than 100 elements, additional results will be returned on subsequent requests. Arrays without additional results will be empty on later pages.  :param run_id: int   The canonical identifier of the run for which to retrieve the metadata. This field is required. :param include_history: bool (optional)   Whether to include the repair history in the response. :param include_resolved_values: bool (optional)   Whether to include resolved parameter values in the response. :param page_token: str (optional)   Use `next_page_token` returned from the previous GetRun response to request the next page of the   run's array properties.  :returns: :class:`Run`",job_runs,get,select,
jobs.json,/api/2.2/jobs/runs/get-output,jobs_get_run_output,get,RunOutput,jobs,run_id,Retrieve the output and metadata of a single task run. When a notebook task returns a value through,"Retrieve the output and metadata of a single task run. When a notebook task returns a value through the `dbutils.notebook.exit()` call, you can use this endpoint to retrieve that value. Databricks restricts this API to returning the first 5 MB of the output. To return a larger result, you can store job results in a cloud storage service.  This endpoint validates that the __run_id__ parameter is valid and returns an HTTP status code 400 if the __run_id__ parameter is invalid. Runs are automatically removed after 60 days. If you to want to reference them beyond 60 days, you must save old run results before they expire.  :param run_id: int   The canonical identifier for the run.  :returns: :class:`RunOutput`",job_run_output,get_output,select,
jobs.json,/api/2.2/jobs/runs/list,jobs_list_runs,get,Iterator[BaseRun],jobs,"active_only, completed_only, expand_tasks, job_id, limit, offset, page_token, run_type, start_time_from, start_time_to",List runs in descending order by start time.,"List runs in descending order by start time.  :param active_only: bool (optional)   If active_only is `true`, only active runs are included in the results; otherwise, lists both active   and completed runs. An active run is a run in the `QUEUED`, `PENDING`, `RUNNING`, or `TERMINATING`.   This field cannot be `true` when completed_only is `true`. :param completed_only: bool (optional)   If completed_only is `true`, only completed runs are included in the results; otherwise, lists both   active and completed runs. This field cannot be `true` when active_only is `true`. :param expand_tasks: bool (optional)   Whether to include task and cluster details in the response. Note that only the first 100 elements   will be shown. Use :method:jobs/getrun to paginate through all tasks and clusters. :param job_id: int (optional)   The job for which to list runs. If omitted, the Jobs service lists runs from all jobs. :param limit: int (optional)   The number of runs to return. This value must be greater than 0 and less than 25. The default value   is 20. If a request specifies a limit of 0, the service instead uses the maximum limit. :param offset: int (optional)   The offset of the first run to return, relative to the most recent run. Deprecated since June 2023.   Use `page_token` to iterate through the pages instead. :param page_token: str (optional)   Use `next_page_token` or `prev_page_token` returned from the previous request to list the next or   previous page of runs respectively. :param run_type: :class:`RunType` (optional)   The type of runs to return. For a description of run types, see :method:jobs/getRun. :param start_time_from: int (optional)   Show runs that started _at or after_ this value. The value must be a UTC timestamp in milliseconds.   Can be combined with _start_time_to_ to filter by a time range. :param start_time_to: int (optional)   Show runs that started _at or before_ this value. The value must be a UTC timestamp in milliseconds.   Can be combined with _start_time_from_ to filter by a time range.  :returns: Iterator over :class:`BaseRun`",job_runs,list,select,$.runs
jobs.json,/api/2.2/jobs/runs/repair,jobs_repair_run,post,Wait[Run],jobs,"run_id, dbt_commands, jar_params, job_parameters, latest_repair_id, notebook_params, performance_target, pipeline_params, python_named_params, python_params, rerun_all_failed_tasks, rerun_dependent_tasks, rerun_tasks, spark_submit_params, sql_params",Re-run one or more tasks. Tasks are re-run as part of the original job run. They use the current job,"Re-run one or more tasks. Tasks are re-run as part of the original job run. They use the current job and task settings, and can be viewed in the history for the original job run.  :param run_id: int   The job run ID of the run to repair. The run must not be in progress. :param dbt_commands: List[str] (optional)   An array of commands to execute for jobs with the dbt task, for example `""dbt_commands"": [""dbt   deps"", ""dbt seed"", ""dbt deps"", ""dbt seed"", ""dbt run""]`    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param jar_params: List[str] (optional)   A list of parameters for jobs with Spark JAR tasks, for example `""jar_params"": [""john doe"", ""35""]`.   The parameters are used to invoke the main function of the main class specified in the Spark JAR   task. If not specified upon `run-now`, it defaults to an empty list. jar_params cannot be specified   in conjunction with notebook_params. The JSON representation of this field (for example   `{""jar_params"":[""john doe"",""35""]}`) cannot exceed 10,000 bytes.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param job_parameters: Dict[str,str] (optional)   Job-level parameters used in the run. for example `""param"": ""overriding_val""` :param latest_repair_id: int (optional)   The ID of the latest repair. This parameter is not required when repairing a run for the first time,   but must be provided on subsequent requests to repair the same run. :param notebook_params: Dict[str,str] (optional)   A map from keys to values for jobs with notebook task, for example `""notebook_params"": {""name"":   ""john doe"", ""age"": ""35""}`. The map is passed to the notebook and is accessible through the   [dbutils.widgets.get] function.    If not specified upon `run-now`, the triggered run uses the job’s base parameters.    notebook_params cannot be specified in conjunction with jar_params.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    The JSON representation of this field (for example `{""notebook_params"":{""name"":""john   doe"",""age"":""35""}}`) cannot exceed 10,000 bytes.    [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html   [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param performance_target: :class:`PerformanceTarget` (optional)   The performance mode on a serverless job. The performance target determines the level of compute   performance or cost-efficiency for the run. This field overrides the performance target defined on   the job level.    * `STANDARD`: Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:   Prioritizes fast startup and execution times through rapid scaling and optimized cluster   performance. :param pipeline_params: :class:`PipelineParams` (optional)   Controls whether the pipeline should perform a full refresh :param python_named_params: Dict[str,str] (optional) :param python_params: List[str] (optional)   A list of parameters for jobs with Python tasks, for example `""python_params"": [""john doe"", ""35""]`.   The parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it   would overwrite the parameters specified in job setting. The JSON representation of this field (for   example `{""python_params"":[""john doe"",""35""]}`) cannot exceed 10,000 bytes.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    Important    These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters   returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and   emojis.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param rerun_all_failed_tasks: bool (optional)   If true, repair all failed tasks. Only one of `rerun_tasks` or `rerun_all_failed_tasks` can be used. :param rerun_dependent_tasks: bool (optional)   If true, repair all tasks that depend on the tasks in `rerun_tasks`, even if they were previously   successful. Can be also used in combination with `rerun_all_failed_tasks`. :param rerun_tasks: List[str] (optional)   The task keys of the task runs to repair. :param spark_submit_params: List[str] (optional)   A list of parameters for jobs with spark submit task, for example `""spark_submit_params"":   [""--class"", ""org.apache.spark.examples.SparkPi""]`. The parameters are passed to spark-submit script   as command-line parameters. If specified upon `run-now`, it would overwrite the parameters specified   in job setting. The JSON representation of this field (for example `{""python_params"":[""john   doe"",""35""]}`) cannot exceed 10,000 bytes.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    Important    These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters   returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and   emojis.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param sql_params: Dict[str,str] (optional)   A map from keys to values for jobs with SQL task, for example `""sql_params"": {""name"": ""john doe"",   ""age"": ""35""}`. The SQL alert task does not support custom parameters.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown  :returns:   Long-running operation waiter for :class:`Run`.   See :method:wait_get_run_job_terminated_or_skipped for more details.",job_runs,repair,exec,
jobs.json,/api/2.2/jobs/runs/submit,jobs_submit,post,Wait[Run],jobs,"access_control_list, budget_policy_id, email_notifications, environments, git_source, health, idempotency_token, notification_settings, queue, run_as, run_name, tasks, timeout_seconds, usage_policy_id, webhook_notifications",Submit a one-time run. This endpoint allows you to submit a workload directly without creating a job.,"Submit a one-time run. This endpoint allows you to submit a workload directly without creating a job. Runs submitted using this endpoint don’t display in the UI. Use the `jobs/runs/get` API to check the run state after the job is submitted.  **Important:** Jobs submitted using this endpoint are not saved as a job. They do not show up in the Jobs UI, and do not retry when they fail. Because they are not saved, Databricks cannot auto-optimize serverless compute in case of failure. If your job fails, you may want to use classic compute to specify the compute needs for the job. Alternatively, use the `POST /jobs/create` and `POST /jobs/run-now` endpoints to create and run a saved job.  :param access_control_list: List[:class:`JobAccessControlRequest`] (optional)   List of permissions to set on the job. :param budget_policy_id: str (optional)   The user specified id of the budget policy to use for this one-time run. If not specified, the run   will be not be attributed to any budget policy. :param email_notifications: :class:`JobEmailNotifications` (optional)   An optional set of email addresses notified when the run begins or completes. :param environments: List[:class:`JobEnvironment`] (optional)   A list of task execution environment specifications that can be referenced by tasks of this run. :param git_source: :class:`GitSource` (optional)   An optional specification for a remote Git repository containing the source code used by tasks.   Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.    If `git_source` is set, these tasks retrieve the file from the remote repository by default.   However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.    Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are   used, `git_source` must be defined on the job. :param health: :class:`JobsHealthRules` (optional) :param idempotency_token: str (optional)   An optional token that can be used to guarantee the idempotency of job run requests. If a run with   the provided token already exists, the request does not create a new run but returns the ID of the   existing run instead. If a run with the provided token is deleted, an error is returned.    If you specify the idempotency token, upon failure you can retry until the request succeeds.   Databricks guarantees that exactly one run is launched with that idempotency token.    This token must have at most 64 characters.    For more information, see [How to ensure idempotency for jobs].    [How to ensure idempotency for jobs]: https://kb.databricks.com/jobs/jobs-idempotency.html :param notification_settings: :class:`JobNotificationSettings` (optional)   Optional notification settings that are used when sending notifications to each of the   `email_notifications` and `webhook_notifications` for this run. :param queue: :class:`QueueSettings` (optional)   The queue settings of the one-time run. :param run_as: :class:`JobRunAs` (optional)   Specifies the user or service principal that the job runs as. If not specified, the job runs as the   user who submits the request. :param run_name: str (optional)   An optional name for the run. The default value is `Untitled`. :param tasks: List[:class:`SubmitTask`] (optional) :param timeout_seconds: int (optional)   An optional timeout applied to each run of this job. A value of `0` means no timeout. :param usage_policy_id: str (optional)   The user specified id of the usage policy to use for this one-time run. If not specified, a default   usage policy may be applied when creating or modifying the job. :param webhook_notifications: :class:`WebhookNotifications` (optional)   A collection of system notification IDs to notify when the run begins or completes.  :returns:   Long-running operation waiter for :class:`Run`.   See :method:wait_get_run_job_terminated_or_skipped for more details.",job_runs,submit,insert,
jobs.json,/api/2.2/jobs/create,jobs_create,post,CreateResponse,jobs,"access_control_list, budget_policy_id, continuous, deployment, description, edit_mode, email_notifications, environments, format, git_source, health, job_clusters, max_concurrent_runs, name, notification_settings, parameters, performance_target, queue, run_as, schedule, tags, tasks, timeout_seconds, trigger, usage_policy_id, webhook_notifications",Create a new job.,"Create a new job.  :param access_control_list: List[:class:`JobAccessControlRequest`] (optional)   List of permissions to set on the job. :param budget_policy_id: str (optional)   The id of the user specified budget policy to use for this job. If not specified, a default budget   policy may be applied when creating or modifying the job. See `effective_budget_policy_id` for the   budget policy used by this workload. :param continuous: :class:`Continuous` (optional)   An optional continuous property for this job. The continuous property will ensure that there is   always one run executing. Only one of `schedule` and `continuous` can be used. :param deployment: :class:`JobDeployment` (optional)   Deployment information for jobs managed by external sources. :param description: str (optional)   An optional description for the job. The maximum length is 27700 characters in UTF-8 encoding. :param edit_mode: :class:`JobEditMode` (optional)   Edit mode of the job.    * `UI_LOCKED`: The job is in a locked UI state and cannot be modified. * `EDITABLE`: The job is in   an editable state and can be modified. :param email_notifications: :class:`JobEmailNotifications` (optional)   An optional set of email addresses that is notified when runs of this job begin or complete as well   as when this job is deleted. :param environments: List[:class:`JobEnvironment`] (optional)   A list of task execution environment specifications that can be referenced by serverless tasks of   this job. For serverless notebook tasks, if the environment_key is not specified, the notebook   environment will be used if present. If a jobs environment is specified, it will override the   notebook environment. For other serverless tasks, the task environment is required to be specified   using environment_key in the task settings. :param format: :class:`Format` (optional)   Used to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When   using the Jobs API 2.1 this value is always set to `""MULTI_TASK""`. :param git_source: :class:`GitSource` (optional)   An optional specification for a remote Git repository containing the source code used by tasks.   Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.    If `git_source` is set, these tasks retrieve the file from the remote repository by default.   However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.    Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are   used, `git_source` must be defined on the job. :param health: :class:`JobsHealthRules` (optional) :param job_clusters: List[:class:`JobCluster`] (optional)   A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries   cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. :param max_concurrent_runs: int (optional)   An optional maximum allowed number of concurrent runs of the job. Set this value if you want to be   able to execute multiple runs of the same job concurrently. This is useful for example if you   trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each   other, or if you want to trigger multiple runs which differ by their input parameters. This setting   affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent   active runs. Then setting the concurrency to 3 won’t kill any of the active runs. However, from   then on, new runs are skipped unless there are fewer than 3 active runs. This value cannot exceed   1000. Setting this value to `0` causes all new runs to be skipped. :param name: str (optional)   An optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding. :param notification_settings: :class:`JobNotificationSettings` (optional)   Optional notification settings that are used when sending notifications to each of the   `email_notifications` and `webhook_notifications` for this job. :param parameters: List[:class:`JobParameterDefinition`] (optional)   Job-level parameter definitions :param performance_target: :class:`PerformanceTarget` (optional)   The performance mode on a serverless job. This field determines the level of compute performance or   cost-efficiency for the run. The performance target does not apply to tasks that run on Serverless   GPU compute.    * `STANDARD`: Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:   Prioritizes fast startup and execution times through rapid scaling and optimized cluster   performance. :param queue: :class:`QueueSettings` (optional)   The queue settings of the job. :param run_as: :class:`JobRunAs` (optional)   The user or service principal that the job runs as, if specified in the request. This field   indicates the explicit configuration of `run_as` for the job. To find the value in all cases,   explicit or implicit, use `run_as_user_name`. :param schedule: :class:`CronSchedule` (optional)   An optional periodic schedule for this job. The default behavior is that the job only runs when   triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`. :param tags: Dict[str,str] (optional)   A map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs   clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added   to the job. :param tasks: List[:class:`Task`] (optional)   A list of task specifications to be executed by this job. It supports up to 1000 elements in write   endpoints (:method:jobs/create, :method:jobs/reset, :method:jobs/update, :method:jobs/submit). Read   endpoints return only 100 tasks. If more than 100 tasks are available, you can paginate through them   using :method:jobs/get. Use the `next_page_token` field at the object root to determine if more   results are available. :param timeout_seconds: int (optional)   An optional timeout applied to each run of this job. A value of `0` means no timeout. :param trigger: :class:`TriggerSettings` (optional)   A configuration to trigger a run when certain conditions are met. The default behavior is that the   job runs only when triggered by clicking “Run Now” in the Jobs UI or sending an API request to   `runNow`. :param usage_policy_id: str (optional)   The id of the user specified usage policy to use for this job. If not specified, a default usage   policy may be applied when creating or modifying the job. See `effective_usage_policy_id` for the   usage policy used by this workload. :param webhook_notifications: :class:`WebhookNotifications` (optional)   A collection of system notification IDs to notify when runs of this job begin or complete.  :returns: :class:`CreateResponse`",jobs,create,insert,
jobs.json,/api/2.2/jobs/delete,jobs_delete,post,,jobs,job_id,Deletes a job.,Deletes a job.  :param job_id: int   The canonical identifier of the job to delete. This field is required.,jobs,delete,delete,
jobs.json,/api/2.2/jobs/get,jobs_get,get,Job,jobs,"job_id, page_token",Retrieves the details for a single job.,"Retrieves the details for a single job.  Large arrays in the results will be paginated when they exceed 100 elements. A request for a single job will return all properties for that job, and the first 100 elements of array properties (`tasks`, `job_clusters`, `environments` and `parameters`). Use the `next_page_token` field to check for more results and pass its value as the `page_token` in subsequent requests. If any array properties have more than 100 elements, additional results will be returned on subsequent requests. Arrays without additional results will be empty on later pages.  :param job_id: int   The canonical identifier of the job to retrieve information about. This field is required. :param page_token: str (optional)   Use `next_page_token` returned from the previous GetJob response to request the next page of the   job's array properties.  :returns: :class:`Job`",jobs,get,select,
jobs.json,/api/2.2/jobs/list,jobs_list,get,Iterator[BaseJob],jobs,"expand_tasks, limit, name, offset, page_token",Retrieves a list of jobs.,"Retrieves a list of jobs.  :param expand_tasks: bool (optional)   Whether to include task and cluster details in the response. Note that only the first 100 elements   will be shown. Use :method:jobs/get to paginate through all tasks and clusters. :param limit: int (optional)   The number of jobs to return. This value must be greater than 0 and less or equal to 100. The   default value is 20. :param name: str (optional)   A filter on the list based on the exact (case insensitive) job name. :param offset: int (optional)   The offset of the first job to return, relative to the most recently created job. Deprecated since   June 2023. Use `page_token` to iterate through the pages instead. :param page_token: str (optional)   Use `next_page_token` or `prev_page_token` returned from the previous request to list the next or   previous page of jobs respectively.  :returns: Iterator over :class:`BaseJob`",jobs,list,select,$.jobs
jobs.json,/api/2.2/jobs/reset,jobs_reset,post,,jobs,"job_id, new_settings",Overwrite all settings for the given job. Use the [_Update_ endpoint](:method:jobs/update) to update,Overwrite all settings for the given job. Use the [_Update_ endpoint](:method:jobs/update) to update job settings partially.  :param job_id: int   The canonical identifier of the job to reset. This field is required. :param new_settings: :class:`JobSettings`   The new settings of the job. These settings completely replace the old settings.    Changes to the field `JobBaseSettings.timeout_seconds` are applied to active runs. Changes to other   fields are applied to future runs only.,jobs,reset,replace,
jobs.json,/api/2.2/jobs/run-now,jobs_run_now,post,Wait[Run],jobs,"job_id, dbt_commands, idempotency_token, jar_params, job_parameters, notebook_params, only, performance_target, pipeline_params, python_named_params, python_params, queue, spark_submit_params, sql_params",Run a job and return the `run_id` of the triggered run.,"Run a job and return the `run_id` of the triggered run.  :param job_id: int   The ID of the job to be executed :param dbt_commands: List[str] (optional)   An array of commands to execute for jobs with the dbt task, for example `""dbt_commands"": [""dbt   deps"", ""dbt seed"", ""dbt deps"", ""dbt seed"", ""dbt run""]`    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param idempotency_token: str (optional)   An optional token to guarantee the idempotency of job run requests. If a run with the provided token   already exists, the request does not create a new run but returns the ID of the existing run   instead. If a run with the provided token is deleted, an error is returned.    If you specify the idempotency token, upon failure you can retry until the request succeeds.   Databricks guarantees that exactly one run is launched with that idempotency token.    This token must have at most 64 characters.    For more information, see [How to ensure idempotency for jobs].    [How to ensure idempotency for jobs]: https://kb.databricks.com/jobs/jobs-idempotency.html :param jar_params: List[str] (optional)   A list of parameters for jobs with Spark JAR tasks, for example `""jar_params"": [""john doe"", ""35""]`.   The parameters are used to invoke the main function of the main class specified in the Spark JAR   task. If not specified upon `run-now`, it defaults to an empty list. jar_params cannot be specified   in conjunction with notebook_params. The JSON representation of this field (for example   `{""jar_params"":[""john doe"",""35""]}`) cannot exceed 10,000 bytes.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param job_parameters: Dict[str,str] (optional)   Job-level parameters used in the run. for example `""param"": ""overriding_val""` :param notebook_params: Dict[str,str] (optional)   A map from keys to values for jobs with notebook task, for example `""notebook_params"": {""name"":   ""john doe"", ""age"": ""35""}`. The map is passed to the notebook and is accessible through the   [dbutils.widgets.get] function.    If not specified upon `run-now`, the triggered run uses the job’s base parameters.    notebook_params cannot be specified in conjunction with jar_params.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    The JSON representation of this field (for example `{""notebook_params"":{""name"":""john   doe"",""age"":""35""}}`) cannot exceed 10,000 bytes.    [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html   [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param only: List[str] (optional)   A list of task keys to run inside of the job. If this field is not provided, all tasks in the job   will be run. :param performance_target: :class:`PerformanceTarget` (optional)   The performance mode on a serverless job. The performance target determines the level of compute   performance or cost-efficiency for the run. This field overrides the performance target defined on   the job level.    * `STANDARD`: Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:   Prioritizes fast startup and execution times through rapid scaling and optimized cluster   performance. :param pipeline_params: :class:`PipelineParams` (optional)   Controls whether the pipeline should perform a full refresh :param python_named_params: Dict[str,str] (optional) :param python_params: List[str] (optional)   A list of parameters for jobs with Python tasks, for example `""python_params"": [""john doe"", ""35""]`.   The parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it   would overwrite the parameters specified in job setting. The JSON representation of this field (for   example `{""python_params"":[""john doe"",""35""]}`) cannot exceed 10,000 bytes.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    Important    These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters   returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and   emojis.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param queue: :class:`QueueSettings` (optional)   The queue settings of the run. :param spark_submit_params: List[str] (optional)   A list of parameters for jobs with spark submit task, for example `""spark_submit_params"":   [""--class"", ""org.apache.spark.examples.SparkPi""]`. The parameters are passed to spark-submit script   as command-line parameters. If specified upon `run-now`, it would overwrite the parameters specified   in job setting. The JSON representation of this field (for example `{""python_params"":[""john   doe"",""35""]}`) cannot exceed 10,000 bytes.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    Important    These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters   returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and   emojis.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown :param sql_params: Dict[str,str] (optional)   A map from keys to values for jobs with SQL task, for example `""sql_params"": {""name"": ""john doe"",   ""age"": ""35""}`. The SQL alert task does not support custom parameters.    ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.    [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown  :returns:   Long-running operation waiter for :class:`Run`.   See :method:wait_get_run_job_terminated_or_skipped for more details.",jobs,run_now,exec,
jobs.json,/api/2.2/jobs/update,jobs_update,post,,jobs,"job_id, fields_to_remove, new_settings","Add, update, or remove specific settings of an existing job. Use the [_Reset_","Add, update, or remove specific settings of an existing job. Use the [_Reset_ endpoint](:method:jobs/reset) to overwrite all job settings.  :param job_id: int   The canonical identifier of the job to update. This field is required. :param fields_to_remove: List[str] (optional)   Remove top-level fields in the job settings. Removing nested fields is not supported, except for   tasks and job clusters (`tasks/task_1`). This field is optional. :param new_settings: :class:`JobSettings` (optional)   The new settings for the job.    Top-level fields specified in `new_settings` are completely replaced, except for arrays which are   merged. That is, new and existing entries are completely replaced based on the respective key   fields, i.e. `task_key` or `job_cluster_key`, while previous entries are kept.    Partially updating nested fields is not supported.    Changes to the field `JobSettings.timeout_seconds` are applied to active runs. Changes to other   fields are applied to future runs only.",jobs,update,update,
jobs.json,/api/2.0/policies/jobs/enforce-compliance,policy_compliance_for_jobs_enforce_compliance,post,EnforcePolicyComplianceResponse,"jobs, policy_compliance_for_jobs","job_id, validate_only",Updates a job so the job clusters that are created when running the job (specified in `new_cluster`),"Updates a job so the job clusters that are created when running the job (specified in `new_cluster`) are compliant with the current versions of their respective cluster policies. All-purpose clusters used in the job will not be updated.  :param job_id: int   The ID of the job you want to enforce policy compliance on. :param validate_only: bool (optional)   If set, previews changes made to the job to comply with its policy, but does not update the job.  :returns: :class:`EnforcePolicyComplianceResponse`",policy_compliance_for_jobs,enforce,insert,
jobs.json,/api/2.0/policies/jobs/get-compliance,policy_compliance_for_jobs_get_compliance,get,GetPolicyComplianceResponse,"jobs, policy_compliance_for_jobs",job_id,Returns the policy compliance status of a job. Jobs could be out of compliance if a cluster policy,Returns the policy compliance status of a job. Jobs could be out of compliance if a cluster policy they use was updated after the job was last edited and some of its job clusters no longer comply with their updated policies.  :param job_id: int   The ID of the job whose compliance status you are requesting.  :returns: :class:`GetPolicyComplianceResponse`,policy_compliance_for_jobs,get,select,
jobs.json,/api/2.0/policies/jobs/list-compliance,policy_compliance_for_jobs_list_compliance,get,Iterator[JobCompliance],"jobs, policy_compliance_for_jobs","policy_id, page_size, page_token",Returns the policy compliance status of all jobs that use a given policy. Jobs could be out of,Returns the policy compliance status of all jobs that use a given policy. Jobs could be out of compliance if a cluster policy they use was updated after the job was last edited and its job clusters no longer comply with the updated policy.  :param policy_id: str   Canonical unique identifier for the cluster policy. :param page_size: int (optional)   Use this field to specify the maximum number of results to be returned by the server. The server may   further constrain the maximum number of results returned in a single page. :param page_token: str (optional)   A page token that can be used to navigate to the next page or previous page as returned by   `next_page_token` or `prev_page_token`.  :returns: Iterator over :class:`JobCompliance`,policy_compliance_for_jobs,list,select,$.jobs
