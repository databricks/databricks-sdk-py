filename,path,operationId,verb,response_object,tags,params,summary,description,stackql_resource_name,stackql_method_name,stackql_verb,stackql_object_key
compute.json,/api/2.0/libraries/all-cluster-statuses,libraries_all_cluster_statuses,get,ListAllClusterLibraryStatusesResponse,"compute, libraries",,Get the status of all libraries on all clusters. A status is returned for all libraries installed on,Get the status of all libraries on all clusters. A status is returned for all libraries installed on this cluster via the API or the libraries UI.   :returns: Iterator over :class:`ClusterLibraryStatuses`,all_cluster_library_statuses,list,select,$.statuses
compute.json,/api/2.1/clusters/list-node-types,clusters_list_node_types,get,ListNodeTypesResponse,"compute, clusters",,Returns a list of supported Spark node types. These node types can be used to launch a cluster.,Returns a list of supported Spark node types. These node types can be used to launch a cluster.   :returns: :class:`ListNodeTypesResponse`,cluster_node_types,list,select,
compute.json,/api/2.0/permissions/clusters/{cluster_id}/permissionLevels,clusters_get_permission_levels,get,GetClusterPermissionLevelsResponse,"compute, clusters",cluster_id,Gets the permission levels that a user can have on an object.,Gets the permission levels that a user can have on an object.  :param cluster_id: str   The cluster for which to get or manage permissions.  :returns: :class:`GetClusterPermissionLevelsResponse`,cluster_permission_levels,get,select,
compute.json,/api/2.0/permissions/clusters/{cluster_id},clusters_get_permissions,get,ClusterPermissions,"compute, clusters",cluster_id,Gets the permissions of a cluster. Clusters can inherit permissions from their root object.,Gets the permissions of a cluster. Clusters can inherit permissions from their root object.  :param cluster_id: str   The cluster for which to get or manage permissions.  :returns: :class:`ClusterPermissions`,cluster_permissions,get,select,
compute.json,/api/2.0/permissions/clusters/{cluster_id},clusters_set_permissions,put,ClusterPermissions,"compute, clusters","cluster_id, access_control_list","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct permissions if none are specified. Objects can inherit permissions from their root object.  :param cluster_id: str   The cluster for which to get or manage permissions. :param access_control_list: List[:class:`ClusterAccessControlRequest`] (optional)  :returns: :class:`ClusterPermissions`",cluster_permissions,set,replace,
compute.json,/api/2.0/permissions/clusters/{cluster_id},clusters_update_permissions,patch,ClusterPermissions,"compute, clusters","cluster_id, access_control_list",Updates the permissions on a cluster. Clusters can inherit permissions from their root object.,Updates the permissions on a cluster. Clusters can inherit permissions from their root object.  :param cluster_id: str   The cluster for which to get or manage permissions. :param access_control_list: List[:class:`ClusterAccessControlRequest`] (optional)  :returns: :class:`ClusterPermissions`,cluster_permissions,update,update,
compute.json,/api/2.0/policies/clusters/create,cluster_policies_create,post,CreatePolicyResponse,"compute, cluster_policies","definition, description, libraries, max_clusters_per_user, name, policy_family_definition_overrides, policy_family_id",Creates a new policy with prescribed settings.,"Creates a new policy with prescribed settings.  :param definition: str (optional)   Policy definition document expressed in [Databricks Cluster Policy Definition Language].    [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html :param description: str (optional)   Additional human-readable description of the cluster policy. :param libraries: List[:class:`Library`] (optional)   A list of libraries to be installed on the next cluster restart that uses this policy. The maximum   number of libraries is 500. :param max_clusters_per_user: int (optional)   Max number of clusters per user that can be active using this policy. If not present, there is no   max limit. :param name: str (optional)   Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100   characters. :param policy_family_definition_overrides: str (optional)   Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON   document must be passed as a string and cannot be embedded in the requests.    You can use this to customize the policy definition inherited from the policy family. Policy rules   specified here are merged into the inherited policy definition.    [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html :param policy_family_id: str (optional)   ID of the policy family. The cluster policy's policy definition inherits the policy family's policy   definition.    Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the   policy definition.  :returns: :class:`CreatePolicyResponse`",cluster_policies,create,insert,
compute.json,/api/2.0/policies/clusters/delete,cluster_policies_delete,post,,"compute, cluster_policies",policy_id,"Delete a policy for a cluster. Clusters governed by this policy can still run, but cannot be edited.","Delete a policy for a cluster. Clusters governed by this policy can still run, but cannot be edited.  :param policy_id: str   The ID of the policy to delete.",cluster_policies,delete,delete,
compute.json,/api/2.0/policies/clusters/edit,cluster_policies_edit,post,,"compute, cluster_policies","policy_id, definition, description, libraries, max_clusters_per_user, name, policy_family_definition_overrides, policy_family_id",Update an existing policy for cluster. This operation may make some clusters governed by the previous,"Update an existing policy for cluster. This operation may make some clusters governed by the previous policy invalid.  :param policy_id: str   The ID of the policy to update. :param definition: str (optional)   Policy definition document expressed in [Databricks Cluster Policy Definition Language].    [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html :param description: str (optional)   Additional human-readable description of the cluster policy. :param libraries: List[:class:`Library`] (optional)   A list of libraries to be installed on the next cluster restart that uses this policy. The maximum   number of libraries is 500. :param max_clusters_per_user: int (optional)   Max number of clusters per user that can be active using this policy. If not present, there is no   max limit. :param name: str (optional)   Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100   characters. :param policy_family_definition_overrides: str (optional)   Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON   document must be passed as a string and cannot be embedded in the requests.    You can use this to customize the policy definition inherited from the policy family. Policy rules   specified here are merged into the inherited policy definition.    [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html :param policy_family_id: str (optional)   ID of the policy family. The cluster policy's policy definition inherits the policy family's policy   definition.    Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the   policy definition.",cluster_policies,replace,replace,
compute.json,/api/2.0/policies/clusters/get,cluster_policies_get,get,Policy,"compute, cluster_policies",policy_id,Get a cluster policy entity. Creation and editing is available to admins only.,Get a cluster policy entity. Creation and editing is available to admins only.  :param policy_id: str   Canonical unique identifier for the Cluster Policy.  :returns: :class:`Policy`,cluster_policies,get,select,
compute.json,/api/2.0/policies/clusters/list,cluster_policies_list,get,ListPoliciesResponse,"compute, cluster_policies","sort_column, sort_order",Returns a list of policies accessible by the requesting user.,Returns a list of policies accessible by the requesting user.  :param sort_column: :class:`ListSortColumn` (optional)   The cluster policy attribute to sort by. * `POLICY_CREATION_TIME` - Sort result list by policy   creation time. * `POLICY_NAME` - Sort result list by policy name. :param sort_order: :class:`ListSortOrder` (optional)   The order in which the policies get listed. * `DESC` - Sort result list in descending order. * `ASC`   - Sort result list in ascending order.  :returns: Iterator over :class:`Policy`,cluster_policies,list,select,$.policies
compute.json,/api/2.0/permissions/cluster-policies/{cluster_policy_id}/permissionLevels,cluster_policies_get_permission_levels,get,GetClusterPolicyPermissionLevelsResponse,"compute, cluster_policies",cluster_policy_id,Gets the permission levels that a user can have on an object.,Gets the permission levels that a user can have on an object.  :param cluster_policy_id: str   The cluster policy for which to get or manage permissions.  :returns: :class:`GetClusterPolicyPermissionLevelsResponse`,cluster_policy_permission_levels,get,select,
compute.json,/api/2.0/permissions/cluster-policies/{cluster_policy_id},cluster_policies_get_permissions,get,ClusterPolicyPermissions,"compute, cluster_policies",cluster_policy_id,Gets the permissions of a cluster policy. Cluster policies can inherit permissions from their root,Gets the permissions of a cluster policy. Cluster policies can inherit permissions from their root object.  :param cluster_policy_id: str   The cluster policy for which to get or manage permissions.  :returns: :class:`ClusterPolicyPermissions`,cluster_policy_permissions,get,select,
compute.json,/api/2.0/permissions/cluster-policies/{cluster_policy_id},cluster_policies_set_permissions,put,ClusterPolicyPermissions,"compute, cluster_policies","cluster_policy_id, access_control_list","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct permissions if none are specified. Objects can inherit permissions from their root object.  :param cluster_policy_id: str   The cluster policy for which to get or manage permissions. :param access_control_list: List[:class:`ClusterPolicyAccessControlRequest`] (optional)  :returns: :class:`ClusterPolicyPermissions`",cluster_policy_permissions,set,replace,
compute.json,/api/2.0/permissions/cluster-policies/{cluster_policy_id},cluster_policies_update_permissions,patch,ClusterPolicyPermissions,"compute, cluster_policies","cluster_policy_id, access_control_list",Updates the permissions on a cluster policy. Cluster policies can inherit permissions from their root,Updates the permissions on a cluster policy. Cluster policies can inherit permissions from their root object.  :param cluster_policy_id: str   The cluster policy for which to get or manage permissions. :param access_control_list: List[:class:`ClusterPolicyAccessControlRequest`] (optional)  :returns: :class:`ClusterPolicyPermissions`,cluster_policy_permissions,update,update,
compute.json,/api/2.1/clusters/list-zones,clusters_list_zones,get,ListAvailableZonesResponse,"compute, clusters",,"Returns a list of availability zones where clusters can be created in (For example, us-west-2a). These","Returns a list of availability zones where clusters can be created in (For example, us-west-2a). These zones can be used to launch a cluster.   :returns: :class:`ListAvailableZonesResponse`",cluster_zones,list,select,
compute.json,/api/2.1/clusters/change-owner,clusters_change_owner,post,,"compute, clusters","cluster_id, owner_username",Change the owner of the cluster. You must be an admin and the cluster must be terminated to perform,Change the owner of the cluster. You must be an admin and the cluster must be terminated to perform this operation. The service principal application ID can be supplied as an argument to `owner_username`.  :param cluster_id: str :param owner_username: str   New owner of the cluster_id after this RPC.,clusters,change_owner,exec,
compute.json,/api/2.1/clusters/create,clusters_create,post,ClusterDetails,"compute, clusters","spark_version, apply_policy_default_values, autoscale, autotermination_minutes, aws_attributes, azure_attributes, clone_from, cluster_log_conf, cluster_name, custom_tags, data_security_mode, docker_image, driver_instance_pool_id, driver_node_type_flexibility, driver_node_type_id, enable_elastic_disk, enable_local_disk_encryption, gcp_attributes, init_scripts, instance_pool_id, is_single_node, kind, node_type_id, num_workers, policy_id, remote_disk_throughput, runtime_engine, single_user_name, spark_conf, spark_env_vars, ssh_public_keys, total_initial_remote_disk_size, use_ml_runtime, worker_node_type_flexibility, workload_type",Creates a new Spark cluster. This method will acquire new instances from the cloud provider if,"Creates a new Spark cluster. This method will acquire new instances from the cloud provider if necessary. This method is asynchronous; the returned ``cluster_id`` can be used to poll the cluster status. When this method returns, the cluster will be in a ``PENDING`` state. The cluster will be usable once it enters a ``RUNNING`` state. Note: Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations (account limits, spot price, etc.) or transient network issues.  If Databricks acquires at least 85% of the requested on-demand nodes, cluster creation will succeed. Otherwise the cluster will terminate with an informative error message.  Rather than authoring the cluster's JSON definition from scratch, Databricks recommends filling out the [create compute UI] and then copying the generated JSON definition from the UI.  [create compute UI]: https://docs.databricks.com/compute/configure.html  :param spark_version: str   The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be   retrieved by using the :method:clusters/sparkVersions API call. :param apply_policy_default_values: bool (optional)   When set to true, fixed and default values from the policy will be used for fields that are omitted.   When set to false, only fixed values from the policy will be applied. :param autoscale: :class:`AutoScale` (optional)   Parameters needed in order to automatically scale clusters up and down based on load. Note:   autoscaling works best with DB runtime versions 3.0 or later. :param autotermination_minutes: int (optional)   Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this   cluster will not be automatically terminated. If specified, the threshold must be between 10 and   10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination. :param aws_attributes: :class:`AwsAttributes` (optional)   Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation,   a set of default values will be used. :param azure_attributes: :class:`AzureAttributes` (optional)   Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a   set of default values will be used. :param clone_from: :class:`CloneCluster` (optional)   When specified, this clones libraries from a source cluster during the creation of a new cluster. :param cluster_log_conf: :class:`ClusterLogConf` (optional)   The configuration for delivering spark logs to a long-term storage destination. Three kinds of   destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be   specified for one cluster. If the conf is given, the logs will be delivered to the destination every   `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination   of executor logs is `$destination/$clusterId/executor`. :param cluster_name: str (optional)   Cluster name requested by the user. This doesn't have to be unique. If not specified at creation,   the cluster name will be an empty string. For job clusters, the cluster name is automatically set   based on the job and job run IDs. :param custom_tags: Dict[str,str] (optional)   Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS   instances and EBS volumes) with these tags in addition to `default_tags`. Notes:    - Currently, Databricks allows at most 45 custom tags    - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags :param data_security_mode: :class:`DataSecurityMode` (optional) :param docker_image: :class:`DockerImage` (optional)   Custom docker image BYOC :param driver_instance_pool_id: str (optional)   The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses   the instance pool with id (instance_pool_id) if the driver pool is not assigned. :param driver_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)   Flexible node type configuration for the driver node. :param driver_node_type_id: str (optional)   The node type of the Spark driver. Note that this field is optional; if unset, the driver node type   will be set as the same value as `node_type_id` defined above.    This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both   driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and   node_type_id take precedence. :param enable_elastic_disk: bool (optional)   Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space   when its Spark workers are running low on disk space. :param enable_local_disk_encryption: bool (optional)   Whether to enable LUKS on cluster VMs' local disks :param gcp_attributes: :class:`GcpAttributes` (optional)   Attributes related to clusters running on Google Cloud Platform. If not specified at cluster   creation, a set of default values will be used. :param init_scripts: List[:class:`InitScriptInfo`] (optional)   The configuration for storing init scripts. Any number of destinations can be specified. The scripts   are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script   logs are sent to `<destination>/<cluster-ID>/init_scripts`. :param instance_pool_id: str (optional)   The optional ID of the instance pool to which the cluster belongs. :param is_single_node: bool (optional)   This field can only be used when `kind = CLASSIC_PREVIEW`.    When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`,   and `num_workers` :param kind: :class:`Kind` (optional) :param node_type_id: str (optional)   This field encodes, through a single value, the resources available to each of the Spark nodes in   this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute   intensive workloads. A list of available node types can be retrieved by using the   :method:clusters/listNodeTypes API call. :param num_workers: int (optional)   Number of worker nodes that this cluster should have. A cluster has one Spark Driver and   `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.    Note: When reading the properties of a cluster, this field reflects the desired number of workers   rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10   workers, this field will immediately be updated to reflect the target size of 10 workers, whereas   the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are   provisioned. :param policy_id: str (optional)   The ID of the cluster policy used to create the cluster if applicable. :param remote_disk_throughput: int (optional)   If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported   for GCP HYPERDISK_BALANCED disks. :param runtime_engine: :class:`RuntimeEngine` (optional)   Determines the cluster's runtime engine, either standard or Photon.    This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove   `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`.    If left unspecified, the runtime engine defaults to standard unless the spark_version contains   -photon-, in which case Photon will be used. :param single_user_name: str (optional)   Single user name if data_security_mode is `SINGLE_USER` :param spark_conf: Dict[str,str] (optional)   An object containing a set of optional, user-specified Spark configuration key-value pairs. Users   can also pass in a string of extra JVM options to the driver and the executors via   `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively. :param spark_env_vars: Dict[str,str] (optional)   An object containing a set of optional, user-specified environment variable key-value pairs. Please   note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while   launching the driver and workers.    In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to   `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks   managed environmental variables are included as well.    Example Spark environment variables: `{""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"":   ""/local_disk0""}` or `{""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS   -Dspark.shuffle.service.enabled=true""}` :param ssh_public_keys: List[str] (optional)   SSH public key contents that will be added to each Spark node in this cluster. The corresponding   private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be   specified. :param total_initial_remote_disk_size: int (optional)   If set, what the total initial volume size (in GB) of the remote disks should be. Currently only   supported for GCP HYPERDISK_BALANCED disks. :param use_ml_runtime: bool (optional)   This field can only be used when `kind = CLASSIC_PREVIEW`.    `effective_spark_version` is determined by `spark_version` (DBR release), this field   `use_ml_runtime`, and whether `node_type_id` is gpu node or not. :param worker_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)   Flexible node type configuration for worker nodes. :param workload_type: :class:`WorkloadType` (optional)  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_running for more details.",clusters,create,insert,
compute.json,/api/2.1/clusters/delete,clusters_delete,post,ClusterDetails,"compute, clusters",cluster_id,Terminates the Spark cluster with the specified ID. The cluster is removed asynchronously. Once the,"Terminates the Spark cluster with the specified ID. The cluster is removed asynchronously. Once the termination has completed, the cluster will be in a `TERMINATED` state. If the cluster is already in a `TERMINATING` or `TERMINATED` state, nothing will happen.  :param cluster_id: str   The cluster to be terminated.  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_terminated for more details.",clusters,delete,exec,
compute.json,/api/2.1/clusters/edit,clusters_edit,post,ClusterDetails,"compute, clusters","cluster_id, spark_version, apply_policy_default_values, autoscale, autotermination_minutes, aws_attributes, azure_attributes, cluster_log_conf, cluster_name, custom_tags, data_security_mode, docker_image, driver_instance_pool_id, driver_node_type_flexibility, driver_node_type_id, enable_elastic_disk, enable_local_disk_encryption, gcp_attributes, init_scripts, instance_pool_id, is_single_node, kind, node_type_id, num_workers, policy_id, remote_disk_throughput, runtime_engine, single_user_name, spark_conf, spark_env_vars, ssh_public_keys, total_initial_remote_disk_size, use_ml_runtime, worker_node_type_flexibility, workload_type",Updates the configuration of a cluster to match the provided attributes and size. A cluster can be,"Updates the configuration of a cluster to match the provided attributes and size. A cluster can be updated if it is in a `RUNNING` or `TERMINATED` state.  If a cluster is updated while in a `RUNNING` state, it will be restarted so that the new attributes can take effect.  If a cluster is updated while in a `TERMINATED` state, it will remain `TERMINATED`. The next time it is started using the `clusters/start` API, the new attributes will take effect. Any attempt to update a cluster in any other state will be rejected with an `INVALID_STATE` error code.  Clusters created by the Databricks Jobs service cannot be edited.  :param cluster_id: str   ID of the cluster :param spark_version: str   The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be   retrieved by using the :method:clusters/sparkVersions API call. :param apply_policy_default_values: bool (optional)   When set to true, fixed and default values from the policy will be used for fields that are omitted.   When set to false, only fixed values from the policy will be applied. :param autoscale: :class:`AutoScale` (optional)   Parameters needed in order to automatically scale clusters up and down based on load. Note:   autoscaling works best with DB runtime versions 3.0 or later. :param autotermination_minutes: int (optional)   Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this   cluster will not be automatically terminated. If specified, the threshold must be between 10 and   10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination. :param aws_attributes: :class:`AwsAttributes` (optional)   Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation,   a set of default values will be used. :param azure_attributes: :class:`AzureAttributes` (optional)   Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a   set of default values will be used. :param cluster_log_conf: :class:`ClusterLogConf` (optional)   The configuration for delivering spark logs to a long-term storage destination. Three kinds of   destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be   specified for one cluster. If the conf is given, the logs will be delivered to the destination every   `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination   of executor logs is `$destination/$clusterId/executor`. :param cluster_name: str (optional)   Cluster name requested by the user. This doesn't have to be unique. If not specified at creation,   the cluster name will be an empty string. For job clusters, the cluster name is automatically set   based on the job and job run IDs. :param custom_tags: Dict[str,str] (optional)   Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS   instances and EBS volumes) with these tags in addition to `default_tags`. Notes:    - Currently, Databricks allows at most 45 custom tags    - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags :param data_security_mode: :class:`DataSecurityMode` (optional) :param docker_image: :class:`DockerImage` (optional)   Custom docker image BYOC :param driver_instance_pool_id: str (optional)   The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses   the instance pool with id (instance_pool_id) if the driver pool is not assigned. :param driver_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)   Flexible node type configuration for the driver node. :param driver_node_type_id: str (optional)   The node type of the Spark driver. Note that this field is optional; if unset, the driver node type   will be set as the same value as `node_type_id` defined above.    This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both   driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and   node_type_id take precedence. :param enable_elastic_disk: bool (optional)   Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space   when its Spark workers are running low on disk space. :param enable_local_disk_encryption: bool (optional)   Whether to enable LUKS on cluster VMs' local disks :param gcp_attributes: :class:`GcpAttributes` (optional)   Attributes related to clusters running on Google Cloud Platform. If not specified at cluster   creation, a set of default values will be used. :param init_scripts: List[:class:`InitScriptInfo`] (optional)   The configuration for storing init scripts. Any number of destinations can be specified. The scripts   are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script   logs are sent to `<destination>/<cluster-ID>/init_scripts`. :param instance_pool_id: str (optional)   The optional ID of the instance pool to which the cluster belongs. :param is_single_node: bool (optional)   This field can only be used when `kind = CLASSIC_PREVIEW`.    When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`,   and `num_workers` :param kind: :class:`Kind` (optional) :param node_type_id: str (optional)   This field encodes, through a single value, the resources available to each of the Spark nodes in   this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute   intensive workloads. A list of available node types can be retrieved by using the   :method:clusters/listNodeTypes API call. :param num_workers: int (optional)   Number of worker nodes that this cluster should have. A cluster has one Spark Driver and   `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.    Note: When reading the properties of a cluster, this field reflects the desired number of workers   rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10   workers, this field will immediately be updated to reflect the target size of 10 workers, whereas   the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are   provisioned. :param policy_id: str (optional)   The ID of the cluster policy used to create the cluster if applicable. :param remote_disk_throughput: int (optional)   If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported   for GCP HYPERDISK_BALANCED disks. :param runtime_engine: :class:`RuntimeEngine` (optional)   Determines the cluster's runtime engine, either standard or Photon.    This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove   `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`.    If left unspecified, the runtime engine defaults to standard unless the spark_version contains   -photon-, in which case Photon will be used. :param single_user_name: str (optional)   Single user name if data_security_mode is `SINGLE_USER` :param spark_conf: Dict[str,str] (optional)   An object containing a set of optional, user-specified Spark configuration key-value pairs. Users   can also pass in a string of extra JVM options to the driver and the executors via   `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively. :param spark_env_vars: Dict[str,str] (optional)   An object containing a set of optional, user-specified environment variable key-value pairs. Please   note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while   launching the driver and workers.    In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to   `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks   managed environmental variables are included as well.    Example Spark environment variables: `{""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"":   ""/local_disk0""}` or `{""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS   -Dspark.shuffle.service.enabled=true""}` :param ssh_public_keys: List[str] (optional)   SSH public key contents that will be added to each Spark node in this cluster. The corresponding   private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be   specified. :param total_initial_remote_disk_size: int (optional)   If set, what the total initial volume size (in GB) of the remote disks should be. Currently only   supported for GCP HYPERDISK_BALANCED disks. :param use_ml_runtime: bool (optional)   This field can only be used when `kind = CLASSIC_PREVIEW`.    `effective_spark_version` is determined by `spark_version` (DBR release), this field   `use_ml_runtime`, and whether `node_type_id` is gpu node or not. :param worker_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)   Flexible node type configuration for worker nodes. :param workload_type: :class:`WorkloadType` (optional)  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_running for more details.",clusters,edit,exec,
compute.json,/api/2.1/clusters/events,clusters_events,post,GetEventsResponse,"compute, clusters","cluster_id, end_time, event_types, limit, offset, order, page_size, page_token, start_time",Retrieves a list of events about the activity of a cluster. This API is paginated. If there are more,"Retrieves a list of events about the activity of a cluster. This API is paginated. If there are more events to read, the response includes all the parameters necessary to request the next page of events.  :param cluster_id: str   The ID of the cluster to retrieve events about. :param end_time: int (optional)   The end time in epoch milliseconds. If empty, returns events up to the current time. :param event_types: List[:class:`EventType`] (optional)   An optional set of event types to filter on. If empty, all event types are returned. :param limit: int (optional)   Deprecated: use page_token in combination with page_size instead.    The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed   value is 500. :param offset: int (optional)   Deprecated: use page_token in combination with page_size instead.    The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results   are requested in descending order, the end_time field is required. :param order: :class:`GetEventsOrder` (optional)   The order to list events in; either ""ASC"" or ""DESC"". Defaults to ""DESC"". :param page_size: int (optional)   The maximum number of events to include in a page of events. The server may further constrain the   maximum number of results returned in a single page. If the page_size is empty or 0, the server will   decide the number of results to be returned. The field has to be in the range [0,500]. If the value   is outside the range, the server enforces 0 or 500. :param page_token: str (optional)   Use next_page_token or prev_page_token returned from the previous request to list the next or   previous page of events respectively. If page_token is empty, the first page is returned. :param start_time: int (optional)   The start time in epoch milliseconds. If empty, returns events starting from the beginning of time.  :returns: Iterator over :class:`ClusterEvent`",clusters,events,exec,$.events
compute.json,/api/2.1/clusters/get,clusters_get,get,ClusterDetails,"compute, clusters",cluster_id,Retrieves the information for a cluster given its identifier. Clusters can be described while they are,"Retrieves the information for a cluster given its identifier. Clusters can be described while they are running, or up to 60 days after they are terminated.  :param cluster_id: str   The cluster about which to retrieve information.  :returns: :class:`ClusterDetails`",clusters,get,select,
compute.json,/api/2.1/clusters/list,clusters_list,get,ListClustersResponse,"compute, clusters","filter_by, page_size, page_token, sort_by","Return information about all pinned and active clusters, and all clusters terminated within the last","Return information about all pinned and active clusters, and all clusters terminated within the last 30 days. Clusters terminated prior to this period are not included.  :param filter_by: :class:`ListClustersFilterBy` (optional)   Filters to apply to the list of clusters. :param page_size: int (optional)   Use this field to specify the maximum number of results to be returned by the server. The server may   further constrain the maximum number of results returned in a single page. :param page_token: str (optional)   Use next_page_token or prev_page_token returned from the previous request to list the next or   previous page of clusters respectively. :param sort_by: :class:`ListClustersSortBy` (optional)   Sort the list of clusters by a specific criteria.  :returns: Iterator over :class:`ClusterDetails`",clusters,list,select,$.clusters
compute.json,/api/2.1/clusters/permanent-delete,clusters_permanent_delete,post,,"compute, clusters",cluster_id,Permanently deletes a Spark cluster. This cluster is terminated and resources are asynchronously,"Permanently deletes a Spark cluster. This cluster is terminated and resources are asynchronously removed.  In addition, users will no longer see permanently deleted clusters in the cluster list, and API users can no longer perform any action on permanently deleted clusters.  :param cluster_id: str   The cluster to be deleted.",clusters,permanent_delete,exec,
compute.json,/api/2.1/clusters/pin,clusters_pin,post,,"compute, clusters",cluster_id,Pinning a cluster ensures that the cluster will always be returned by the ListClusters API. Pinning a,Pinning a cluster ensures that the cluster will always be returned by the ListClusters API. Pinning a cluster that is already pinned will have no effect. This API can only be called by workspace admins.  :param cluster_id: str,clusters,pin,exec,
compute.json,/api/2.1/clusters/resize,clusters_resize,post,ClusterDetails,"compute, clusters","cluster_id, autoscale, num_workers",Resizes a cluster to have a desired number of workers. This will fail unless the cluster is in a,"Resizes a cluster to have a desired number of workers. This will fail unless the cluster is in a `RUNNING` state.  :param cluster_id: str   The cluster to be resized. :param autoscale: :class:`AutoScale` (optional)   Parameters needed in order to automatically scale clusters up and down based on load. Note:   autoscaling works best with DB runtime versions 3.0 or later. :param num_workers: int (optional)   Number of worker nodes that this cluster should have. A cluster has one Spark Driver and   `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.    Note: When reading the properties of a cluster, this field reflects the desired number of workers   rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10   workers, this field will immediately be updated to reflect the target size of 10 workers, whereas   the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are   provisioned.  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_running for more details.",clusters,resize,exec,
compute.json,/api/2.1/clusters/restart,clusters_restart,post,ClusterDetails,"compute, clusters","cluster_id, restart_user","Restarts a Spark cluster with the supplied ID. If the cluster is not currently in a `RUNNING` state,","Restarts a Spark cluster with the supplied ID. If the cluster is not currently in a `RUNNING` state, nothing will happen.  :param cluster_id: str   The cluster to be started. :param restart_user: str (optional)  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_running for more details.",clusters,restart,exec,
compute.json,/api/2.1/clusters/start,clusters_start,post,ClusterDetails,"compute, clusters",cluster_id,Starts a terminated Spark cluster with the supplied ID. This works similar to `createCluster` except:,"Starts a terminated Spark cluster with the supplied ID. This works similar to `createCluster` except: - The previous cluster id and attributes are preserved. - The cluster starts with the last specified cluster size. - If the previous cluster was an autoscaling cluster, the current cluster starts with the minimum number of nodes. - If the cluster is not currently in a ``TERMINATED`` state, nothing will happen. - Clusters launched to run a job cannot be started.  :param cluster_id: str   The cluster to be started.  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_running for more details.",clusters,start,exec,
compute.json,/api/2.1/clusters/unpin,clusters_unpin,post,,"compute, clusters",cluster_id,Unpinning a cluster will allow the cluster to eventually be removed from the ListClusters API.,Unpinning a cluster will allow the cluster to eventually be removed from the ListClusters API. Unpinning a cluster that is not pinned will have no effect. This API can only be called by workspace admins.  :param cluster_id: str,clusters,unpin,exec,
compute.json,/api/2.1/clusters/update,clusters_update,post,ClusterDetails,"compute, clusters","cluster_id, update_mask, cluster",Updates the configuration of a cluster to match the partial set of attributes and size. Denote which,"Updates the configuration of a cluster to match the partial set of attributes and size. Denote which fields to update using the `update_mask` field in the request body. A cluster can be updated if it is in a `RUNNING` or `TERMINATED` state. If a cluster is updated while in a `RUNNING` state, it will be restarted so that the new attributes can take effect. If a cluster is updated while in a `TERMINATED` state, it will remain `TERMINATED`. The updated attributes will take effect the next time the cluster is started using the `clusters/start` API. Attempts to update a cluster in any other state will be rejected with an `INVALID_STATE` error code. Clusters created by the Databricks Jobs service cannot be updated.  :param cluster_id: str   ID of the cluster. :param update_mask: str   Used to specify which cluster attributes and size fields to update. See https://google.aip.dev/161   for more details.    The field mask must be a single string, with multiple fields separated by commas (no spaces). The   field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,   `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only   the entire collection field can be specified. Field names must exactly match the resource field   names.    A field mask of `*` indicates full replacement. Itâ€™s recommended to always explicitly list the   fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API   changes in the future. :param cluster: :class:`UpdateClusterResource` (optional)   The cluster to be updated.  :returns:   Long-running operation waiter for :class:`ClusterDetails`.   See :method:wait_get_cluster_running for more details.",clusters,update,exec,
compute.json,/api/1.2/commands/cancel,command_execution_cancel,post,CommandStatusResponse,"compute, command_execution","cluster_id, command_id, context_id",Cancels a currently running command within an execution context.,Cancels a currently running command within an execution context.  The command ID is obtained from a prior successful call to __execute__.  :param cluster_id: str (optional) :param command_id: str (optional) :param context_id: str (optional)  :returns:   Long-running operation waiter for :class:`CommandStatusResponse`.   See :method:wait_command_status_command_execution_cancelled for more details.,command_execution,cancel,exec,
compute.json,/api/1.2/commands/status,command_execution_command_status,get,CommandStatusResponse,"compute, command_execution","cluster_id, context_id, command_id","Gets the status of and, if available, the results from a currently executing command.","Gets the status of and, if available, the results from a currently executing command.  The command ID is obtained from a prior successful call to __execute__.  :param cluster_id: str :param context_id: str :param command_id: str  :returns: :class:`CommandStatusResponse`",command_execution,command_status,select,
compute.json,/api/1.2/contexts/create,command_execution_create,post,ContextStatusResponse,"compute, command_execution","cluster_id, language",Creates an execution context for running cluster commands.,"Creates an execution context for running cluster commands.  If successful, this method returns the ID of the new execution context.  :param cluster_id: str (optional)   Running cluster id :param language: :class:`Language` (optional)  :returns:   Long-running operation waiter for :class:`ContextStatusResponse`.   See :method:wait_context_status_command_execution_running for more details.",command_execution,create,insert,
compute.json,/api/1.2/contexts/destroy,command_execution_destroy,post,,"compute, command_execution","cluster_id, context_id",Deletes an execution context.,Deletes an execution context.  :param cluster_id: str :param context_id: str,command_execution,destroy,exec,
compute.json,/api/1.2/commands/execute,command_execution_execute,post,CommandStatusResponse,"compute, command_execution","cluster_id, command, context_id, language","Runs a cluster command in the given execution context, using the provided language.","Runs a cluster command in the given execution context, using the provided language.  If successful, it returns an ID for tracking the status of the command's execution.  :param cluster_id: str (optional)   Running cluster id :param command: str (optional)   Executable code :param context_id: str (optional)   Running context id :param language: :class:`Language` (optional)  :returns:   Long-running operation waiter for :class:`CommandStatusResponse`.   See :method:wait_command_status_command_execution_finished_or_error for more details.",command_execution,execute,exec,
compute.json,/api/1.2/contexts/status,command_execution_context_status,get,ContextStatusResponse,"compute, command_execution","cluster_id, context_id",Gets the status for an execution context.,Gets the status for an execution context.  :param cluster_id: str :param context_id: str  :returns: :class:`ContextStatusResponse`,execution_contexts,get,select,
compute.json,/api/2.0/global-init-scripts,global_init_scripts_create,post,CreateResponse,"compute, global_init_scripts","name, script, enabled, position",Creates a new global init script in this workspace.,"Creates a new global init script in this workspace.  :param name: str   The name of the script :param script: str   The Base64-encoded content of the script. :param enabled: bool (optional)   Specifies whether the script is enabled. The script runs only if enabled. :param position: int (optional)   The position of a global init script, where 0 represents the first script to run, 1 is the second   script to run, in ascending order.    If you omit the numeric position for a new global init script, it defaults to last position. It will   run after all current scripts. Setting any value greater than the position of the last script is   equivalent to the last position. Example: Take three existing scripts with positions 0, 1, and 2.   Any position of (3) or greater puts the script in the last position. If an explicit position value   conflicts with an existing script value, your request succeeds, but the original script at that   position and all later scripts have their positions incremented by 1.  :returns: :class:`CreateResponse`",global_init_scripts,create,insert,
compute.json,/api/2.0/global-init-scripts,global_init_scripts_list,get,ListGlobalInitScriptsResponse,"compute, global_init_scripts",,Get a list of all global init scripts for this workspace. This returns all properties for each script,"Get a list of all global init scripts for this workspace. This returns all properties for each script but **not** the script contents. To retrieve the contents of a script, use the [get a global init script](:method:globalinitscripts/get) operation.   :returns: Iterator over :class:`GlobalInitScriptDetails`",global_init_scripts,list,select,$.scripts
compute.json,/api/2.0/global-init-scripts/{script_id},global_init_scripts_delete,delete,,"compute, global_init_scripts",script_id,Deletes a global init script.,Deletes a global init script.  :param script_id: str   The ID of the global init script.,global_init_scripts,delete,delete,
compute.json,/api/2.0/global-init-scripts/{script_id},global_init_scripts_get,get,GlobalInitScriptDetailsWithContent,"compute, global_init_scripts",script_id,"Gets all the details of a script, including its Base64-encoded contents.","Gets all the details of a script, including its Base64-encoded contents.  :param script_id: str   The ID of the global init script.  :returns: :class:`GlobalInitScriptDetailsWithContent`",global_init_scripts,get,select,
compute.json,/api/2.0/global-init-scripts/{script_id},global_init_scripts_update,patch,,"compute, global_init_scripts","script_id, name, script, enabled, position","Updates a global init script, specifying only the fields to change. All fields are optional.","Updates a global init script, specifying only the fields to change. All fields are optional. Unspecified fields retain their current value.  :param script_id: str   The ID of the global init script. :param name: str   The name of the script :param script: str   The Base64-encoded content of the script. :param enabled: bool (optional)   Specifies whether the script is enabled. The script runs only if enabled. :param position: int (optional)   The position of a script, where 0 represents the first script to run, 1 is the second script to run,   in ascending order. To move the script to run first, set its position to 0.    To move the script to the end, set its position to any value greater or equal to the position of the   last script. Example, three existing scripts with positions 0, 1, and 2. Any position value of 2 or   greater puts the script in the last position (2).    If an explicit position value conflicts with an existing script, your request succeeds, but the   original script at that position and all later scripts have their positions incremented by 1.",global_init_scripts,update,update,
compute.json,/api/2.0/permissions/instance-pools/{instance_pool_id}/permissionLevels,instance_pools_get_permission_levels,get,GetInstancePoolPermissionLevelsResponse,"compute, instance_pools",instance_pool_id,Gets the permission levels that a user can have on an object.,Gets the permission levels that a user can have on an object.  :param instance_pool_id: str   The instance pool for which to get or manage permissions.  :returns: :class:`GetInstancePoolPermissionLevelsResponse`,instance_pool_permission_levels,get,select,
compute.json,/api/2.0/permissions/instance-pools/{instance_pool_id},instance_pools_get_permissions,get,InstancePoolPermissions,"compute, instance_pools",instance_pool_id,Gets the permissions of an instance pool. Instance pools can inherit permissions from their root,Gets the permissions of an instance pool. Instance pools can inherit permissions from their root object.  :param instance_pool_id: str   The instance pool for which to get or manage permissions.  :returns: :class:`InstancePoolPermissions`,instance_pool_permissions,get,select,
compute.json,/api/2.0/permissions/instance-pools/{instance_pool_id},instance_pools_set_permissions,put,InstancePoolPermissions,"compute, instance_pools","instance_pool_id, access_control_list","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct","Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct permissions if none are specified. Objects can inherit permissions from their root object.  :param instance_pool_id: str   The instance pool for which to get or manage permissions. :param access_control_list: List[:class:`InstancePoolAccessControlRequest`] (optional)  :returns: :class:`InstancePoolPermissions`",instance_pool_permissions,set,replace,
compute.json,/api/2.0/permissions/instance-pools/{instance_pool_id},instance_pools_update_permissions,patch,InstancePoolPermissions,"compute, instance_pools","instance_pool_id, access_control_list",Updates the permissions on an instance pool. Instance pools can inherit permissions from their root,Updates the permissions on an instance pool. Instance pools can inherit permissions from their root object.  :param instance_pool_id: str   The instance pool for which to get or manage permissions. :param access_control_list: List[:class:`InstancePoolAccessControlRequest`] (optional)  :returns: :class:`InstancePoolPermissions`,instance_pool_permissions,update,update,
compute.json,/api/2.0/instance-pools/create,instance_pools_create,post,CreateInstancePoolResponse,"compute, instance_pools","instance_pool_name, node_type_id, aws_attributes, azure_attributes, custom_tags, disk_spec, enable_elastic_disk, gcp_attributes, idle_instance_autotermination_minutes, max_capacity, min_idle_instances, node_type_flexibility, preloaded_docker_images, preloaded_spark_versions, remote_disk_throughput, total_initial_remote_disk_size",Creates a new instance pool using idle and ready-to-use cloud instances.,"Creates a new instance pool using idle and ready-to-use cloud instances.  :param instance_pool_name: str   Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100   characters. :param node_type_id: str   This field encodes, through a single value, the resources available to each of the Spark nodes in   this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute   intensive workloads. A list of available node types can be retrieved by using the   :method:clusters/listNodeTypes API call. :param aws_attributes: :class:`InstancePoolAwsAttributes` (optional)   Attributes related to instance pools running on Amazon Web Services. If not specified at pool   creation, a set of default values will be used. :param azure_attributes: :class:`InstancePoolAzureAttributes` (optional)   Attributes related to instance pools running on Azure. If not specified at pool creation, a set of   default values will be used. :param custom_tags: Dict[str,str] (optional)   Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and   EBS volumes) with these tags in addition to `default_tags`. Notes:    - Currently, Databricks allows at most 45 custom tags :param disk_spec: :class:`DiskSpec` (optional)   Defines the specification of the disks that will be attached to all spark containers. :param enable_elastic_disk: bool (optional)   Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire   additional disk space when its Spark workers are running low on disk space. In AWS, this feature   requires specific AWS permissions to function correctly - refer to the User Guide for more details. :param gcp_attributes: :class:`InstancePoolGcpAttributes` (optional)   Attributes related to instance pools running on Google Cloud Platform. If not specified at pool   creation, a set of default values will be used. :param idle_instance_autotermination_minutes: int (optional)   Automatically terminates the extra instances in the pool cache after they are inactive for this time   in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances   will be automatically terminated after a default timeout. If specified, the threshold must be   between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances   from the cache if min cache size could still hold. :param max_capacity: int (optional)   Maximum number of outstanding instances to keep in the pool, including both instances used by   clusters and idle instances. Clusters that require further instance provisioning will fail during   upsize requests. :param min_idle_instances: int (optional)   Minimum number of idle instances to keep in the instance pool :param node_type_flexibility: :class:`NodeTypeFlexibility` (optional)   Flexible node type configuration for the pool. :param preloaded_docker_images: List[:class:`DockerImage`] (optional)   Custom Docker Image BYOC :param preloaded_spark_versions: List[str] (optional)   A list containing at most one preloaded Spark image version for the pool. Pool-backed clusters   started with the preloaded Spark version will start faster. A list of available Spark versions can   be retrieved by using the :method:clusters/sparkVersions API call. :param remote_disk_throughput: int (optional)   If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported   for GCP HYPERDISK_BALANCED types. :param total_initial_remote_disk_size: int (optional)   If set, what the total initial volume size (in GB) of the remote disks should be. Currently only   supported for GCP HYPERDISK_BALANCED types.  :returns: :class:`CreateInstancePoolResponse`",instance_pools,create,insert,
compute.json,/api/2.0/instance-pools/delete,instance_pools_delete,post,,"compute, instance_pools",instance_pool_id,Deletes the instance pool permanently. The idle instances in the pool are terminated asynchronously.,Deletes the instance pool permanently. The idle instances in the pool are terminated asynchronously.  :param instance_pool_id: str   The instance pool to be terminated.,instance_pools,delete,delete,
compute.json,/api/2.0/instance-pools/edit,instance_pools_edit,post,,"compute, instance_pools","instance_pool_id, instance_pool_name, node_type_id, custom_tags, idle_instance_autotermination_minutes, max_capacity, min_idle_instances, node_type_flexibility, remote_disk_throughput, total_initial_remote_disk_size",Modifies the configuration of an existing instance pool.,"Modifies the configuration of an existing instance pool.  :param instance_pool_id: str   Instance pool ID :param instance_pool_name: str   Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100   characters. :param node_type_id: str   This field encodes, through a single value, the resources available to each of the Spark nodes in   this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute   intensive workloads. A list of available node types can be retrieved by using the   :method:clusters/listNodeTypes API call. :param custom_tags: Dict[str,str] (optional)   Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and   EBS volumes) with these tags in addition to `default_tags`. Notes:    - Currently, Databricks allows at most 45 custom tags :param idle_instance_autotermination_minutes: int (optional)   Automatically terminates the extra instances in the pool cache after they are inactive for this time   in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances   will be automatically terminated after a default timeout. If specified, the threshold must be   between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances   from the cache if min cache size could still hold. :param max_capacity: int (optional)   Maximum number of outstanding instances to keep in the pool, including both instances used by   clusters and idle instances. Clusters that require further instance provisioning will fail during   upsize requests. :param min_idle_instances: int (optional)   Minimum number of idle instances to keep in the instance pool :param node_type_flexibility: :class:`NodeTypeFlexibility` (optional)   Flexible node type configuration for the pool. :param remote_disk_throughput: int (optional)   If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported   for GCP HYPERDISK_BALANCED types. :param total_initial_remote_disk_size: int (optional)   If set, what the total initial volume size (in GB) of the remote disks should be. Currently only   supported for GCP HYPERDISK_BALANCED types.",instance_pools,replace,replace,
compute.json,/api/2.0/instance-pools/get,instance_pools_get,get,GetInstancePool,"compute, instance_pools",instance_pool_id,Retrieve the information for an instance pool based on its identifier.,Retrieve the information for an instance pool based on its identifier.  :param instance_pool_id: str   The canonical unique identifier for the instance pool.  :returns: :class:`GetInstancePool`,instance_pools,get,select,
compute.json,/api/2.0/instance-pools/list,instance_pools_list,get,ListInstancePools,"compute, instance_pools",,Gets a list of instance pools with their statistics.,Gets a list of instance pools with their statistics.   :returns: Iterator over :class:`InstancePoolAndStats`,instance_pools,list,select,$.instance_pools
compute.json,/api/2.0/instance-profiles/add,instance_profiles_add,post,,"compute, instance_profiles","instance_profile_arn, iam_role_arn, is_meta_instance_profile, skip_validation","Registers an instance profile in Databricks. In the UI, you can then give users the permission to use","Registers an instance profile in Databricks. In the UI, you can then give users the permission to use this instance profile when launching clusters.  This API is only available to admin users.  :param instance_profile_arn: str   The AWS ARN of the instance profile to register with Databricks. This field is required. :param iam_role_arn: str (optional)   The AWS IAM role ARN of the role associated with the instance profile. This field is required if   your role name and instance profile name do not match and you want to use the instance profile with   [Databricks SQL Serverless].    Otherwise, this field is optional.    [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html :param is_meta_instance_profile: bool (optional)   Boolean flag indicating whether the instance profile should only be used in credential passthrough   scenarios. If true, it means the instance profile contains an meta IAM role which could assume a   wide range of roles. Therefore it should always be used with authorization. This field is optional,   the default value is `false`. :param skip_validation: bool (optional)   By default, Databricks validates that it has sufficient permissions to launch instances with the   instance profile. This validation uses AWS dry-run mode for the RunInstances API. If validation   fails with an error message that does not indicate an IAM related permission issue, (e.g. â€œYour   requested instance type is not supported in your requested availability zoneâ€), you can pass this   flag to skip the validation and forcibly add the instance profile.",instance_profiles,add,insert,
compute.json,/api/2.0/instance-profiles/edit,instance_profiles_edit,post,,"compute, instance_profiles","instance_profile_arn, iam_role_arn, is_meta_instance_profile",The only supported field to change is the optional IAM role ARN associated with the instance profile.,"The only supported field to change is the optional IAM role ARN associated with the instance profile. It is required to specify the IAM role ARN if both of the following are true:  * Your role name and instance profile name do not match. The name is the part after the last slash in each ARN. * You want to use the instance profile with [Databricks SQL Serverless].  To understand where these fields are in the AWS console, see [Enable serverless SQL warehouses].  This API is only available to admin users.  [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html [Enable serverless SQL warehouses]: https://docs.databricks.com/sql/admin/serverless.html  :param instance_profile_arn: str   The AWS ARN of the instance profile to register with Databricks. This field is required. :param iam_role_arn: str (optional)   The AWS IAM role ARN of the role associated with the instance profile. This field is required if   your role name and instance profile name do not match and you want to use the instance profile with   [Databricks SQL Serverless].    Otherwise, this field is optional.    [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html :param is_meta_instance_profile: bool (optional)   Boolean flag indicating whether the instance profile should only be used in credential passthrough   scenarios. If true, it means the instance profile contains an meta IAM role which could assume a   wide range of roles. Therefore it should always be used with authorization. This field is optional,   the default value is `false`.",instance_profiles,edit,replace,
compute.json,/api/2.0/instance-profiles/list,instance_profiles_list,get,ListInstanceProfilesResponse,"compute, instance_profiles",,List the instance profiles that the calling user can use to launch a cluster.,List the instance profiles that the calling user can use to launch a cluster.  This API is available to all users.   :returns: Iterator over :class:`InstanceProfile`,instance_profiles,list,select,$.instance_profiles
compute.json,/api/2.0/instance-profiles/remove,instance_profiles_remove,post,,"compute, instance_profiles",instance_profile_arn,Remove the instance profile with the provided ARN. Existing clusters with this instance profile will,Remove the instance profile with the provided ARN. Existing clusters with this instance profile will continue to function.  This API is only accessible to admin users.  :param instance_profile_arn: str   The ARN of the instance profile to remove. This field is required.,instance_profiles,remove,delete,
compute.json,/api/2.0/libraries/cluster-status,libraries_cluster_status,get,ClusterLibraryStatuses,"compute, libraries",cluster_id,Get the status of libraries on a cluster. A status is returned for all libraries installed on this,"Get the status of libraries on a cluster. A status is returned for all libraries installed on this cluster via the API or the libraries UI. The order of returned libraries is as follows: 1. Libraries set to be installed on this cluster, in the order that the libraries were added to the cluster, are returned first. 2. Libraries that were previously requested to be installed on this cluster or, but are now marked for removal, in no particular order, are returned last.  :param cluster_id: str   Unique identifier of the cluster whose status should be retrieved.  :returns: Iterator over :class:`LibraryFullStatus`",libraries,get,select,$.library_statuses
compute.json,/api/2.0/libraries/install,libraries_install,post,,"compute, libraries","cluster_id, libraries",Add libraries to install on a cluster. The installation is asynchronous; it happens in the background,Add libraries to install on a cluster. The installation is asynchronous; it happens in the background after the completion of this request.  :param cluster_id: str   Unique identifier for the cluster on which to install these libraries. :param libraries: List[:class:`Library`]   The libraries to install.,libraries,install,insert,
compute.json,/api/2.0/libraries/uninstall,libraries_uninstall,post,,"compute, libraries","cluster_id, libraries",Set libraries to uninstall from a cluster. The libraries won't be uninstalled until the cluster is,Set libraries to uninstall from a cluster. The libraries won't be uninstalled until the cluster is restarted. A request to uninstall a library that is not currently installed is ignored.  :param cluster_id: str   Unique identifier for the cluster on which to uninstall these libraries. :param libraries: List[:class:`Library`]   The libraries to uninstall.,libraries,uninstall,exec,
compute.json,/api/2.0/policies/clusters/enforce-compliance,policy_compliance_for_clusters_enforce_compliance,post,EnforceClusterComplianceResponse,"compute, policy_compliance_for_clusters","cluster_id, validate_only",Updates a cluster to be compliant with the current version of its policy. A cluster can be updated if,"Updates a cluster to be compliant with the current version of its policy. A cluster can be updated if it is in a `RUNNING` or `TERMINATED` state.  If a cluster is updated while in a `RUNNING` state, it will be restarted so that the new attributes can take effect.  If a cluster is updated while in a `TERMINATED` state, it will remain `TERMINATED`. The next time the cluster is started, the new attributes will take effect.  Clusters created by the Databricks Jobs, DLT, or Models services cannot be enforced by this API. Instead, use the ""Enforce job policy compliance"" API to enforce policy compliance on jobs.  :param cluster_id: str   The ID of the cluster you want to enforce policy compliance on. :param validate_only: bool (optional)   If set, previews the changes that would be made to a cluster to enforce compliance but does not   update the cluster.  :returns: :class:`EnforceClusterComplianceResponse`",policy_compliance_for_clusters,enforce,insert,
compute.json,/api/2.0/policies/clusters/get-compliance,policy_compliance_for_clusters_get_compliance,get,GetClusterComplianceResponse,"compute, policy_compliance_for_clusters",cluster_id,Returns the policy compliance status of a cluster. Clusters could be out of compliance if their policy,Returns the policy compliance status of a cluster. Clusters could be out of compliance if their policy was updated after the cluster was last edited.  :param cluster_id: str   The ID of the cluster to get the compliance status  :returns: :class:`GetClusterComplianceResponse`,policy_compliance_for_clusters,get,select,
compute.json,/api/2.0/policies/clusters/list-compliance,policy_compliance_for_clusters_list_compliance,get,ListClusterCompliancesResponse,"compute, policy_compliance_for_clusters","policy_id, page_size, page_token",Returns the policy compliance status of all clusters that use a given policy. Clusters could be out of,Returns the policy compliance status of all clusters that use a given policy. Clusters could be out of compliance if their policy was updated after the cluster was last edited.  :param policy_id: str   Canonical unique identifier for the cluster policy. :param page_size: int (optional)   Use this field to specify the maximum number of results to be returned by the server. The server may   further constrain the maximum number of results returned in a single page. :param page_token: str (optional)   A page token that can be used to navigate to the next page or previous page as returned by   `next_page_token` or `prev_page_token`.  :returns: Iterator over :class:`ClusterCompliance`,policy_compliance_for_clusters,list,select,$.clusters
compute.json,/api/2.0/policy-families/{policy_family_id},policy_families_get,get,PolicyFamily,"compute, policy_families","policy_family_id, version",Retrieve the information for an policy family based on its identifier and version,Retrieve the information for an policy family based on its identifier and version  :param policy_family_id: str   The family ID about which to retrieve information. :param version: int (optional)   The version number for the family to fetch. Defaults to the latest version.  :returns: :class:`PolicyFamily`,policy_families,get,select,
compute.json,/api/2.0/policy-families,policy_families_list,get,ListPolicyFamiliesResponse,"compute, policy_families","max_results, page_token",Returns the list of policy definition types available to use at their latest version. This API is,Returns the list of policy definition types available to use at their latest version. This API is paginated.  :param max_results: int (optional)   Maximum number of policy families to return. :param page_token: str (optional)   A token that can be used to get the next page of results.  :returns: Iterator over :class:`PolicyFamily`,policy_families,list,select,$.policy_families
compute.json,/api/2.1/clusters/spark-versions,clusters_spark_versions,get,GetSparkVersionsResponse,"compute, clusters",,Returns the list of available Spark versions. These versions can be used to launch a cluster.,Returns the list of available Spark versions. These versions can be used to launch a cluster.   :returns: :class:`GetSparkVersionsResponse`,spark_versions,list,select,
