openapi: 3.0.0
info:
  title: Databricks Compute API (workspace)
  description: >-
    OpenAPI specification for the Databricks compute service (workspace-level
    APIs), generated from the Databricks Python SDK.
  version: 0.1.0
servers:
  url: https://{deployment_name}.cloud.databricks.com
  variables:
    deployment_name:
      description: The Databricks Workspace Deployment Name
      default: dbc-abcd0123-a1bc
paths:
  /api/2.0/policies/clusters/create:
    post:
      operationId: cluster_policies_create
      summary: Creates a new policy with prescribed settings.
      tags:
        - compute
        - cluster_policies
      description: |-
        Creates a new policy with prescribed settings.

        :param definition: str (optional)
          Policy definition document expressed in [Databricks Cluster Policy Definition Language].

          [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html
        :param description: str (optional)
          Additional human-readable description of the cluster policy.
        :param libraries: List[:class:`Library`] (optional)
          A list of libraries to be installed on the next cluster restart that uses this policy. The maximum
          number of libraries is 500.
        :param max_clusters_per_user: int (optional)
          Max number of clusters per user that can be active using this policy. If not present, there is no
          max limit.
        :param name: str (optional)
          Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100
          characters.
        :param policy_family_definition_overrides: str (optional)
          Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON
          document must be passed as a string and cannot be embedded in the requests.

          You can use this to customize the policy definition inherited from the policy family. Policy rules
          specified here are merged into the inherited policy definition.

          [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html
        :param policy_family_id: str (optional)
          ID of the policy family. The cluster policy's policy definition inherits the policy family's policy
          definition.

          Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the
          policy definition.

        :returns: :class:`CreatePolicyResponse`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                definition:
                  type: string
                  description: >-
                    Policy definition document expressed in [Databricks Cluster
                    Policy Definition Language]. [Databricks Cluster Policy
                    Definition Language]:
                    https://docs.databricks.com/administration-guide/clusters/policy-definition.html
                description:
                  type: string
                  description: Additional human-readable description of the cluster policy.
                libraries:
                  type: string
                  description: >-
                    A list of libraries to be installed on the next cluster
                    restart that uses this policy. The maximum number of
                    libraries is 500.
                max_clusters_per_user:
                  type: string
                  description: >-
                    Max number of clusters per user that can be active using
                    this policy. If not present, there is no max limit.
                name:
                  type: string
                  description: >-
                    Cluster Policy name requested by the user. This has to be
                    unique. Length must be between 1 and 100 characters.
                policy_family_definition_overrides:
                  type: string
                  description: >-
                    Policy definition JSON document expressed in [Databricks
                    Policy Definition Language]. The JSON document must be
                    passed as a string and cannot be embedded in the requests.
                    You can use this to customize the policy definition
                    inherited from the policy family. Policy rules specified
                    here are merged into the inherited policy definition.
                    [Databricks Policy Definition Language]:
                    https://docs.databricks.com/administration-guide/clusters/policy-definition.html
                policy_family_id:
                  type: string
                  description: >-
                    ID of the policy family. The cluster policy's policy
                    definition inherits the policy family's policy definition.
                    Cannot be used with `definition`. Use
                    `policy_family_definition_overrides` instead to customize
                    the policy definition.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreatePolicyResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/delete:
    post:
      operationId: cluster_policies_delete
      summary: >-
        Delete a policy for a cluster. Clusters governed by this policy can
        still run, but cannot be edited.
      tags:
        - compute
        - cluster_policies
      description: >-
        Delete a policy for a cluster. Clusters governed by this policy can
        still run, but cannot be edited.


        :param policy_id: str
          The ID of the policy to delete.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                policy_id:
                  type: string
                  description: The ID of the policy to delete.
              required:
                - policy_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/edit:
    post:
      operationId: cluster_policies_edit
      summary: >-
        Update an existing policy for cluster. This operation may make some
        clusters governed by the previous
      tags:
        - compute
        - cluster_policies
      description: >-
        Update an existing policy for cluster. This operation may make some
        clusters governed by the previous

        policy invalid.


        :param policy_id: str
          The ID of the policy to update.
        :param definition: str (optional)
          Policy definition document expressed in [Databricks Cluster Policy Definition Language].

          [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html
        :param description: str (optional)
          Additional human-readable description of the cluster policy.
        :param libraries: List[:class:`Library`] (optional)
          A list of libraries to be installed on the next cluster restart that uses this policy. The maximum
          number of libraries is 500.
        :param max_clusters_per_user: int (optional)
          Max number of clusters per user that can be active using this policy. If not present, there is no
          max limit.
        :param name: str (optional)
          Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100
          characters.
        :param policy_family_definition_overrides: str (optional)
          Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON
          document must be passed as a string and cannot be embedded in the requests.

          You can use this to customize the policy definition inherited from the policy family. Policy rules
          specified here are merged into the inherited policy definition.

          [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html
        :param policy_family_id: str (optional)
          ID of the policy family. The cluster policy's policy definition inherits the policy family's policy
          definition.

          Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the
          policy definition.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                policy_id:
                  type: string
                  description: The ID of the policy to update.
                definition:
                  type: string
                  description: >-
                    Policy definition document expressed in [Databricks Cluster
                    Policy Definition Language]. [Databricks Cluster Policy
                    Definition Language]:
                    https://docs.databricks.com/administration-guide/clusters/policy-definition.html
                description:
                  type: string
                  description: Additional human-readable description of the cluster policy.
                libraries:
                  type: string
                  description: >-
                    A list of libraries to be installed on the next cluster
                    restart that uses this policy. The maximum number of
                    libraries is 500.
                max_clusters_per_user:
                  type: string
                  description: >-
                    Max number of clusters per user that can be active using
                    this policy. If not present, there is no max limit.
                name:
                  type: string
                  description: >-
                    Cluster Policy name requested by the user. This has to be
                    unique. Length must be between 1 and 100 characters.
                policy_family_definition_overrides:
                  type: string
                  description: >-
                    Policy definition JSON document expressed in [Databricks
                    Policy Definition Language]. The JSON document must be
                    passed as a string and cannot be embedded in the requests.
                    You can use this to customize the policy definition
                    inherited from the policy family. Policy rules specified
                    here are merged into the inherited policy definition.
                    [Databricks Policy Definition Language]:
                    https://docs.databricks.com/administration-guide/clusters/policy-definition.html
                policy_family_id:
                  type: string
                  description: >-
                    ID of the policy family. The cluster policy's policy
                    definition inherits the policy family's policy definition.
                    Cannot be used with `definition`. Use
                    `policy_family_definition_overrides` instead to customize
                    the policy definition.
              required:
                - policy_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/get:
    get:
      operationId: cluster_policies_get
      summary: >-
        Get a cluster policy entity. Creation and editing is available to admins
        only.
      tags:
        - compute
        - cluster_policies
      description: >-
        Get a cluster policy entity. Creation and editing is available to admins
        only.


        :param policy_id: str
          Canonical unique identifier for the Cluster Policy.

        :returns: :class:`Policy`
      parameters:
        - name: policy_id
          in: query
          required: true
          schema:
            type: string
          description: Canonical unique identifier for the Cluster Policy.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/cluster-policies/{cluster_policy_id}/permissionLevels:
    get:
      operationId: cluster_policies_get_permission_levels
      summary: Gets the permission levels that a user can have on an object.
      tags:
        - compute
        - cluster_policies
      description: |-
        Gets the permission levels that a user can have on an object.

        :param cluster_policy_id: str
          The cluster policy for which to get or manage permissions.

        :returns: :class:`GetClusterPolicyPermissionLevelsResponse`
      parameters:
        - name: cluster_policy_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster policy for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetClusterPolicyPermissionLevelsResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/cluster-policies/{cluster_policy_id}:
    get:
      operationId: cluster_policies_get_permissions
      summary: >-
        Gets the permissions of a cluster policy. Cluster policies can inherit
        permissions from their root
      tags:
        - compute
        - cluster_policies
      description: >-
        Gets the permissions of a cluster policy. Cluster policies can inherit
        permissions from their root

        object.


        :param cluster_policy_id: str
          The cluster policy for which to get or manage permissions.

        :returns: :class:`ClusterPolicyPermissions`
      parameters:
        - name: cluster_policy_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster policy for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPolicyPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    put:
      operationId: cluster_policies_set_permissions
      summary: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct
      tags:
        - compute
        - cluster_policies
      description: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from
        their root object.


        :param cluster_policy_id: str
          The cluster policy for which to get or manage permissions.
        :param access_control_list:
        List[:class:`ClusterPolicyAccessControlRequest`] (optional)


        :returns: :class:`ClusterPolicyPermissions`
      parameters:
        - name: cluster_policy_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster policy for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`ClusterPolicyPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPolicyPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    patch:
      operationId: cluster_policies_update_permissions
      summary: >-
        Updates the permissions on a cluster policy. Cluster policies can
        inherit permissions from their root
      tags:
        - compute
        - cluster_policies
      description: >-
        Updates the permissions on a cluster policy. Cluster policies can
        inherit permissions from their root

        object.


        :param cluster_policy_id: str
          The cluster policy for which to get or manage permissions.
        :param access_control_list:
        List[:class:`ClusterPolicyAccessControlRequest`] (optional)


        :returns: :class:`ClusterPolicyPermissions`
      parameters:
        - name: cluster_policy_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster policy for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`ClusterPolicyPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPolicyPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/list:
    get:
      operationId: cluster_policies_list
      summary: Returns a list of policies accessible by the requesting user.
      tags:
        - compute
        - cluster_policies
      description: |-
        Returns a list of policies accessible by the requesting user.

        :param sort_column: :class:`ListSortColumn` (optional)
          The cluster policy attribute to sort by. * `POLICY_CREATION_TIME` - Sort result list by policy
          creation time. * `POLICY_NAME` - Sort result list by policy name.
        :param sort_order: :class:`ListSortOrder` (optional)
          The order in which the policies get listed. * `DESC` - Sort result list in descending order. * `ASC`
          - Sort result list in ascending order.

        :returns: Iterator over :class:`Policy`
      parameters:
        - name: sort_column
          in: query
          required: false
          schema:
            type: string
          description: >-
            The cluster policy attribute to sort by. * `POLICY_CREATION_TIME` -
            Sort result list by policy creation time. * `POLICY_NAME` - Sort
            result list by policy name.
        - name: sort_order
          in: query
          required: false
          schema:
            type: string
          description: >-
            The order in which the policies get listed. * `DESC` - Sort result
            list in descending order. * `ASC` - Sort result list in ascending
            order.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[Policy]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/change-owner:
    post:
      operationId: clusters_change_owner
      summary: >-
        Change the owner of the cluster. You must be an admin and the cluster
        must be terminated to perform
      tags:
        - compute
        - clusters
      description: >-
        Change the owner of the cluster. You must be an admin and the cluster
        must be terminated to perform

        this operation. The service principal application ID can be supplied as
        an argument to

        `owner_username`.


        :param cluster_id: str

        :param owner_username: str
          New owner of the cluster_id after this RPC.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: >-
                    :param owner_username: str New owner of the cluster_id after
                    this RPC.
                owner_username:
                  type: string
              required:
                - cluster_id
                - owner_username
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/create:
    post:
      operationId: clusters_create
      summary: >-
        Creates a new Spark cluster. This method will acquire new instances from
        the cloud provider if
      tags:
        - compute
        - clusters
      description: >-
        Creates a new Spark cluster. This method will acquire new instances from
        the cloud provider if

        necessary. This method is asynchronous; the returned ``cluster_id`` can
        be used to poll the cluster

        status. When this method returns, the cluster will be in a ``PENDING``
        state. The cluster will be

        usable once it enters a ``RUNNING`` state. Note: Databricks may not be
        able to acquire some of the

        requested nodes, due to cloud provider limitations (account limits, spot
        price, etc.) or transient

        network issues.


        If Databricks acquires at least 85% of the requested on-demand nodes,
        cluster creation will succeed.

        Otherwise the cluster will terminate with an informative error message.


        Rather than authoring the cluster's JSON definition from scratch,
        Databricks recommends filling out

        the [create compute UI] and then copying the generated JSON definition
        from the UI.


        [create compute UI]: https://docs.databricks.com/compute/configure.html


        :param spark_version: str
          The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be
          retrieved by using the :method:clusters/sparkVersions API call.
        :param apply_policy_default_values: bool (optional)
          When set to true, fixed and default values from the policy will be used for fields that are omitted.
          When set to false, only fixed values from the policy will be applied.
        :param autoscale: :class:`AutoScale` (optional)
          Parameters needed in order to automatically scale clusters up and down based on load. Note:
          autoscaling works best with DB runtime versions 3.0 or later.
        :param autotermination_minutes: int (optional)
          Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this
          cluster will not be automatically terminated. If specified, the threshold must be between 10 and
          10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination.
        :param aws_attributes: :class:`AwsAttributes` (optional)
          Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation,
          a set of default values will be used.
        :param azure_attributes: :class:`AzureAttributes` (optional)
          Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a
          set of default values will be used.
        :param clone_from: :class:`CloneCluster` (optional)
          When specified, this clones libraries from a source cluster during the creation of a new cluster.
        :param cluster_log_conf: :class:`ClusterLogConf` (optional)
          The configuration for delivering spark logs to a long-term storage destination. Three kinds of
          destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be
          specified for one cluster. If the conf is given, the logs will be delivered to the destination every
          `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination
          of executor logs is `$destination/$clusterId/executor`.
        :param cluster_name: str (optional)
          Cluster name requested by the user. This doesn't have to be unique. If not specified at creation,
          the cluster name will be an empty string. For job clusters, the cluster name is automatically set
          based on the job and job run IDs.
        :param custom_tags: Dict[str,str] (optional)
          Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS
          instances and EBS volumes) with these tags in addition to `default_tags`. Notes:

          - Currently, Databricks allows at most 45 custom tags

          - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags
        :param data_security_mode: :class:`DataSecurityMode` (optional)

        :param docker_image: :class:`DockerImage` (optional)
          Custom docker image BYOC
        :param driver_instance_pool_id: str (optional)
          The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses
          the instance pool with id (instance_pool_id) if the driver pool is not assigned.
        :param driver_node_type_flexibility: :class:`NodeTypeFlexibility`
        (optional)
          Flexible node type configuration for the driver node.
        :param driver_node_type_id: str (optional)
          The node type of the Spark driver. Note that this field is optional; if unset, the driver node type
          will be set as the same value as `node_type_id` defined above.

          This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both
          driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and
          node_type_id take precedence.
        :param enable_elastic_disk: bool (optional)
          Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space
          when its Spark workers are running low on disk space.
        :param enable_local_disk_encryption: bool (optional)
          Whether to enable LUKS on cluster VMs' local disks
        :param gcp_attributes: :class:`GcpAttributes` (optional)
          Attributes related to clusters running on Google Cloud Platform. If not specified at cluster
          creation, a set of default values will be used.
        :param init_scripts: List[:class:`InitScriptInfo`] (optional)
          The configuration for storing init scripts. Any number of destinations can be specified. The scripts
          are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script
          logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        :param instance_pool_id: str (optional)
          The optional ID of the instance pool to which the cluster belongs.
        :param is_single_node: bool (optional)
          This field can only be used when `kind = CLASSIC_PREVIEW`.

          When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`,
          and `num_workers`
        :param kind: :class:`Kind` (optional)

        :param node_type_id: str (optional)
          This field encodes, through a single value, the resources available to each of the Spark nodes in
          this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute
          intensive workloads. A list of available node types can be retrieved by using the
          :method:clusters/listNodeTypes API call.
        :param num_workers: int (optional)
          Number of worker nodes that this cluster should have. A cluster has one Spark Driver and
          `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.

          Note: When reading the properties of a cluster, this field reflects the desired number of workers
          rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10
          workers, this field will immediately be updated to reflect the target size of 10 workers, whereas
          the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are
          provisioned.
        :param policy_id: str (optional)
          The ID of the cluster policy used to create the cluster if applicable.
        :param remote_disk_throughput: int (optional)
          If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported
          for GCP HYPERDISK_BALANCED disks.
        :param runtime_engine: :class:`RuntimeEngine` (optional)
          Determines the cluster's runtime engine, either standard or Photon.

          This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove
          `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`.

          If left unspecified, the runtime engine defaults to standard unless the spark_version contains
          -photon-, in which case Photon will be used.
        :param single_user_name: str (optional)
          Single user name if data_security_mode is `SINGLE_USER`
        :param spark_conf: Dict[str,str] (optional)
          An object containing a set of optional, user-specified Spark configuration key-value pairs. Users
          can also pass in a string of extra JVM options to the driver and the executors via
          `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.
        :param spark_env_vars: Dict[str,str] (optional)
          An object containing a set of optional, user-specified environment variable key-value pairs. Please
          note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while
          launching the driver and workers.

          In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
          `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks
          managed environmental variables are included as well.

          Example Spark environment variables: `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS":
          "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS
          -Dspark.shuffle.service.enabled=true"}`
        :param ssh_public_keys: List[str] (optional)
          SSH public key contents that will be added to each Spark node in this cluster. The corresponding
          private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be
          specified.
        :param total_initial_remote_disk_size: int (optional)
          If set, what the total initial volume size (in GB) of the remote disks should be. Currently only
          supported for GCP HYPERDISK_BALANCED disks.
        :param use_ml_runtime: bool (optional)
          This field can only be used when `kind = CLASSIC_PREVIEW`.

          `effective_spark_version` is determined by `spark_version` (DBR release), this field
          `use_ml_runtime`, and whether `node_type_id` is gpu node or not.
        :param worker_node_type_flexibility: :class:`NodeTypeFlexibility`
        (optional)
          Flexible node type configuration for worker nodes.
        :param workload_type: :class:`WorkloadType` (optional)


        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                spark_version:
                  type: string
                  description: >-
                    The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A
                    list of available Spark versions can be retrieved by using
                    the :method:clusters/sparkVersions API call.
                apply_policy_default_values:
                  type: string
                  description: >-
                    When set to true, fixed and default values from the policy
                    will be used for fields that are omitted. When set to false,
                    only fixed values from the policy will be applied.
                autoscale:
                  type: string
                  description: >-
                    Parameters needed in order to automatically scale clusters
                    up and down based on load. Note: autoscaling works best with
                    DB runtime versions 3.0 or later.
                autotermination_minutes:
                  type: string
                  description: >-
                    Automatically terminates the cluster after it is inactive
                    for this time in minutes. If not set, this cluster will not
                    be automatically terminated. If specified, the threshold
                    must be between 10 and 10000 minutes. Users can also set
                    this value to 0 to explicitly disable automatic termination.
                aws_attributes:
                  type: string
                  description: >-
                    Attributes related to clusters running on Amazon Web
                    Services. If not specified at cluster creation, a set of
                    default values will be used.
                azure_attributes:
                  type: string
                  description: >-
                    Attributes related to clusters running on Microsoft Azure.
                    If not specified at cluster creation, a set of default
                    values will be used.
                clone_from:
                  type: string
                  description: >-
                    When specified, this clones libraries from a source cluster
                    during the creation of a new cluster.
                cluster_log_conf:
                  type: string
                  description: >-
                    The configuration for delivering spark logs to a long-term
                    storage destination. Three kinds of destinations (DBFS, S3
                    and Unity Catalog volumes) are supported. Only one
                    destination can be specified for one cluster. If the conf is
                    given, the logs will be delivered to the destination every
                    `5 mins`. The destination of driver logs is
                    `$destination/$clusterId/driver`, while the destination of
                    executor logs is `$destination/$clusterId/executor`.
                cluster_name:
                  type: string
                  description: >-
                    Cluster name requested by the user. This doesn't have to be
                    unique. If not specified at creation, the cluster name will
                    be an empty string. For job clusters, the cluster name is
                    automatically set based on the job and job run IDs.
                custom_tags:
                  type: string
                  description: >-
                    Additional tags for cluster resources. Databricks will tag
                    all cluster resources (e.g., AWS instances and EBS volumes)
                    with these tags in addition to `default_tags`. Notes: -
                    Currently, Databricks allows at most 45 custom tags -
                    Clusters can only reuse cloud resources if the resources'
                    tags are a subset of the cluster tags
                data_security_mode:
                  type: string
                  description: >-
                    :param docker_image: :class:`DockerImage` (optional) Custom
                    docker image BYOC
                docker_image:
                  type: string
                driver_instance_pool_id:
                  type: string
                  description: >-
                    The optional ID of the instance pool for the driver of the
                    cluster belongs. The pool cluster uses the instance pool
                    with id (instance_pool_id) if the driver pool is not
                    assigned.
                driver_node_type_flexibility:
                  type: string
                  description: Flexible node type configuration for the driver node.
                driver_node_type_id:
                  type: string
                  description: >-
                    The node type of the Spark driver. Note that this field is
                    optional; if unset, the driver node type will be set as the
                    same value as `node_type_id` defined above. This field,
                    along with node_type_id, should not be set if
                    virtual_cluster_size is set. If both driver_node_type_id,
                    node_type_id, and virtual_cluster_size are specified,
                    driver_node_type_id and node_type_id take precedence.
                enable_elastic_disk:
                  type: string
                  description: >-
                    Autoscaling Local Storage: when enabled, this cluster will
                    dynamically acquire additional disk space when its Spark
                    workers are running low on disk space.
                enable_local_disk_encryption:
                  type: string
                  description: Whether to enable LUKS on cluster VMs' local disks
                gcp_attributes:
                  type: string
                  description: >-
                    Attributes related to clusters running on Google Cloud
                    Platform. If not specified at cluster creation, a set of
                    default values will be used.
                init_scripts:
                  type: string
                  description: >-
                    The configuration for storing init scripts. Any number of
                    destinations can be specified. The scripts are executed
                    sequentially in the order provided. If `cluster_log_conf` is
                    specified, init script logs are sent to
                    `<destination>/<cluster-ID>/init_scripts`.
                instance_pool_id:
                  type: string
                  description: >-
                    The optional ID of the instance pool to which the cluster
                    belongs.
                is_single_node:
                  type: string
                  description: >-
                    This field can only be used when `kind = CLASSIC_PREVIEW`.
                    When set to true, Databricks will automatically set single
                    node related `custom_tags`, `spark_conf`, and `num_workers`
                kind:
                  type: string
                  description: >-
                    :param node_type_id: str (optional) This field encodes,
                    through a single value, the resources available to each of
                    the Spark nodes in this cluster. For example, the Spark
                    nodes can be provisioned and optimized for memory or compute
                    intensive workloads. A list of available node types can be
                    retrieved by using the :method:clusters/listNodeTypes API
                    call.
                node_type_id:
                  type: string
                num_workers:
                  type: string
                  description: >-
                    Number of worker nodes that this cluster should have. A
                    cluster has one Spark Driver and `num_workers` Executors for
                    a total of `num_workers` + 1 Spark nodes. Note: When reading
                    the properties of a cluster, this field reflects the desired
                    number of workers rather than the actual current number of
                    workers. For instance, if a cluster is resized from 5 to 10
                    workers, this field will immediately be updated to reflect
                    the target size of 10 workers, whereas the workers listed in
                    `spark_info` will gradually increase from 5 to 10 as the new
                    nodes are provisioned.
                policy_id:
                  type: string
                  description: >-
                    The ID of the cluster policy used to create the cluster if
                    applicable.
                remote_disk_throughput:
                  type: string
                  description: >-
                    If set, what the configurable throughput (in Mb/s) for the
                    remote disk is. Currently only supported for GCP
                    HYPERDISK_BALANCED disks.
                runtime_engine:
                  type: string
                  description: >-
                    Determines the cluster's runtime engine, either standard or
                    Photon. This field is not compatible with legacy
                    `spark_version` values that contain `-photon-`. Remove
                    `-photon-` from the `spark_version` and set `runtime_engine`
                    to `PHOTON`. If left unspecified, the runtime engine
                    defaults to standard unless the spark_version contains
                    -photon-, in which case Photon will be used.
                single_user_name:
                  type: string
                  description: Single user name if data_security_mode is `SINGLE_USER`
                spark_conf:
                  type: string
                  description: >-
                    An object containing a set of optional, user-specified Spark
                    configuration key-value pairs. Users can also pass in a
                    string of extra JVM options to the driver and the executors
                    via `spark.driver.extraJavaOptions` and
                    `spark.executor.extraJavaOptions` respectively.
                spark_env_vars:
                  type: string
                  description: >-
                    An object containing a set of optional, user-specified
                    environment variable key-value pairs. Please note that
                    key-value pair of the form (X,Y) will be exported as is
                    (i.e., `export X='Y'`) while launching the driver and
                    workers. In order to specify an additional set of
                    `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
                    `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below.
                    This ensures that all default databricks managed
                    environmental variables are included as well. Example Spark
                    environment variables: `{"SPARK_WORKER_MEMORY": "28000m",
                    "SPARK_LOCAL_DIRS": "/local_disk0"}` or
                    `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS
                    -Dspark.shuffle.service.enabled=true"}`
                ssh_public_keys:
                  type: string
                  description: >-
                    SSH public key contents that will be added to each Spark
                    node in this cluster. The corresponding private keys can be
                    used to login with the user name `ubuntu` on port `2200`. Up
                    to 10 keys can be specified.
                total_initial_remote_disk_size:
                  type: string
                  description: >-
                    If set, what the total initial volume size (in GB) of the
                    remote disks should be. Currently only supported for GCP
                    HYPERDISK_BALANCED disks.
                use_ml_runtime:
                  type: string
                  description: >-
                    This field can only be used when `kind = CLASSIC_PREVIEW`.
                    `effective_spark_version` is determined by `spark_version`
                    (DBR release), this field `use_ml_runtime`, and whether
                    `node_type_id` is gpu node or not.
                worker_node_type_flexibility:
                  type: string
                  description: Flexible node type configuration for worker nodes.
                workload_type:
                  type: string
                  description: >-
                    :returns: Long-running operation waiter for
                    :class:`ClusterDetails`. See
                    :method:wait_get_cluster_running for more details.
              required:
                - spark_version
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/delete:
    post:
      operationId: clusters_delete
      summary: >-
        Terminates the Spark cluster with the specified ID. The cluster is
        removed asynchronously. Once the
      tags:
        - compute
        - clusters
      description: >-
        Terminates the Spark cluster with the specified ID. The cluster is
        removed asynchronously. Once the

        termination has completed, the cluster will be in a `TERMINATED` state.
        If the cluster is already in a

        `TERMINATING` or `TERMINATED` state, nothing will happen.


        :param cluster_id: str
          The cluster to be terminated.

        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_terminated for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: The cluster to be terminated.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/edit:
    post:
      operationId: clusters_edit
      summary: >-
        Updates the configuration of a cluster to match the provided attributes
        and size. A cluster can be
      tags:
        - compute
        - clusters
      description: >-
        Updates the configuration of a cluster to match the provided attributes
        and size. A cluster can be

        updated if it is in a `RUNNING` or `TERMINATED` state.


        If a cluster is updated while in a `RUNNING` state, it will be restarted
        so that the new attributes

        can take effect.


        If a cluster is updated while in a `TERMINATED` state, it will remain
        `TERMINATED`. The next time it

        is started using the `clusters/start` API, the new attributes will take
        effect. Any attempt to update

        a cluster in any other state will be rejected with an `INVALID_STATE`
        error code.


        Clusters created by the Databricks Jobs service cannot be edited.


        :param cluster_id: str
          ID of the cluster
        :param spark_version: str
          The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be
          retrieved by using the :method:clusters/sparkVersions API call.
        :param apply_policy_default_values: bool (optional)
          When set to true, fixed and default values from the policy will be used for fields that are omitted.
          When set to false, only fixed values from the policy will be applied.
        :param autoscale: :class:`AutoScale` (optional)
          Parameters needed in order to automatically scale clusters up and down based on load. Note:
          autoscaling works best with DB runtime versions 3.0 or later.
        :param autotermination_minutes: int (optional)
          Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this
          cluster will not be automatically terminated. If specified, the threshold must be between 10 and
          10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination.
        :param aws_attributes: :class:`AwsAttributes` (optional)
          Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation,
          a set of default values will be used.
        :param azure_attributes: :class:`AzureAttributes` (optional)
          Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a
          set of default values will be used.
        :param cluster_log_conf: :class:`ClusterLogConf` (optional)
          The configuration for delivering spark logs to a long-term storage destination. Three kinds of
          destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be
          specified for one cluster. If the conf is given, the logs will be delivered to the destination every
          `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination
          of executor logs is `$destination/$clusterId/executor`.
        :param cluster_name: str (optional)
          Cluster name requested by the user. This doesn't have to be unique. If not specified at creation,
          the cluster name will be an empty string. For job clusters, the cluster name is automatically set
          based on the job and job run IDs.
        :param custom_tags: Dict[str,str] (optional)
          Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS
          instances and EBS volumes) with these tags in addition to `default_tags`. Notes:

          - Currently, Databricks allows at most 45 custom tags

          - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags
        :param data_security_mode: :class:`DataSecurityMode` (optional)

        :param docker_image: :class:`DockerImage` (optional)
          Custom docker image BYOC
        :param driver_instance_pool_id: str (optional)
          The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses
          the instance pool with id (instance_pool_id) if the driver pool is not assigned.
        :param driver_node_type_flexibility: :class:`NodeTypeFlexibility`
        (optional)
          Flexible node type configuration for the driver node.
        :param driver_node_type_id: str (optional)
          The node type of the Spark driver. Note that this field is optional; if unset, the driver node type
          will be set as the same value as `node_type_id` defined above.

          This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both
          driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and
          node_type_id take precedence.
        :param enable_elastic_disk: bool (optional)
          Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space
          when its Spark workers are running low on disk space.
        :param enable_local_disk_encryption: bool (optional)
          Whether to enable LUKS on cluster VMs' local disks
        :param gcp_attributes: :class:`GcpAttributes` (optional)
          Attributes related to clusters running on Google Cloud Platform. If not specified at cluster
          creation, a set of default values will be used.
        :param init_scripts: List[:class:`InitScriptInfo`] (optional)
          The configuration for storing init scripts. Any number of destinations can be specified. The scripts
          are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script
          logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        :param instance_pool_id: str (optional)
          The optional ID of the instance pool to which the cluster belongs.
        :param is_single_node: bool (optional)
          This field can only be used when `kind = CLASSIC_PREVIEW`.

          When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`,
          and `num_workers`
        :param kind: :class:`Kind` (optional)

        :param node_type_id: str (optional)
          This field encodes, through a single value, the resources available to each of the Spark nodes in
          this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute
          intensive workloads. A list of available node types can be retrieved by using the
          :method:clusters/listNodeTypes API call.
        :param num_workers: int (optional)
          Number of worker nodes that this cluster should have. A cluster has one Spark Driver and
          `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.

          Note: When reading the properties of a cluster, this field reflects the desired number of workers
          rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10
          workers, this field will immediately be updated to reflect the target size of 10 workers, whereas
          the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are
          provisioned.
        :param policy_id: str (optional)
          The ID of the cluster policy used to create the cluster if applicable.
        :param remote_disk_throughput: int (optional)
          If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported
          for GCP HYPERDISK_BALANCED disks.
        :param runtime_engine: :class:`RuntimeEngine` (optional)
          Determines the cluster's runtime engine, either standard or Photon.

          This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove
          `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`.

          If left unspecified, the runtime engine defaults to standard unless the spark_version contains
          -photon-, in which case Photon will be used.
        :param single_user_name: str (optional)
          Single user name if data_security_mode is `SINGLE_USER`
        :param spark_conf: Dict[str,str] (optional)
          An object containing a set of optional, user-specified Spark configuration key-value pairs. Users
          can also pass in a string of extra JVM options to the driver and the executors via
          `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.
        :param spark_env_vars: Dict[str,str] (optional)
          An object containing a set of optional, user-specified environment variable key-value pairs. Please
          note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while
          launching the driver and workers.

          In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
          `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks
          managed environmental variables are included as well.

          Example Spark environment variables: `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS":
          "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS
          -Dspark.shuffle.service.enabled=true"}`
        :param ssh_public_keys: List[str] (optional)
          SSH public key contents that will be added to each Spark node in this cluster. The corresponding
          private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be
          specified.
        :param total_initial_remote_disk_size: int (optional)
          If set, what the total initial volume size (in GB) of the remote disks should be. Currently only
          supported for GCP HYPERDISK_BALANCED disks.
        :param use_ml_runtime: bool (optional)
          This field can only be used when `kind = CLASSIC_PREVIEW`.

          `effective_spark_version` is determined by `spark_version` (DBR release), this field
          `use_ml_runtime`, and whether `node_type_id` is gpu node or not.
        :param worker_node_type_flexibility: :class:`NodeTypeFlexibility`
        (optional)
          Flexible node type configuration for worker nodes.
        :param workload_type: :class:`WorkloadType` (optional)


        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: ID of the cluster
                spark_version:
                  type: string
                  description: >-
                    The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A
                    list of available Spark versions can be retrieved by using
                    the :method:clusters/sparkVersions API call.
                apply_policy_default_values:
                  type: string
                  description: >-
                    When set to true, fixed and default values from the policy
                    will be used for fields that are omitted. When set to false,
                    only fixed values from the policy will be applied.
                autoscale:
                  type: string
                  description: >-
                    Parameters needed in order to automatically scale clusters
                    up and down based on load. Note: autoscaling works best with
                    DB runtime versions 3.0 or later.
                autotermination_minutes:
                  type: string
                  description: >-
                    Automatically terminates the cluster after it is inactive
                    for this time in minutes. If not set, this cluster will not
                    be automatically terminated. If specified, the threshold
                    must be between 10 and 10000 minutes. Users can also set
                    this value to 0 to explicitly disable automatic termination.
                aws_attributes:
                  type: string
                  description: >-
                    Attributes related to clusters running on Amazon Web
                    Services. If not specified at cluster creation, a set of
                    default values will be used.
                azure_attributes:
                  type: string
                  description: >-
                    Attributes related to clusters running on Microsoft Azure.
                    If not specified at cluster creation, a set of default
                    values will be used.
                cluster_log_conf:
                  type: string
                  description: >-
                    The configuration for delivering spark logs to a long-term
                    storage destination. Three kinds of destinations (DBFS, S3
                    and Unity Catalog volumes) are supported. Only one
                    destination can be specified for one cluster. If the conf is
                    given, the logs will be delivered to the destination every
                    `5 mins`. The destination of driver logs is
                    `$destination/$clusterId/driver`, while the destination of
                    executor logs is `$destination/$clusterId/executor`.
                cluster_name:
                  type: string
                  description: >-
                    Cluster name requested by the user. This doesn't have to be
                    unique. If not specified at creation, the cluster name will
                    be an empty string. For job clusters, the cluster name is
                    automatically set based on the job and job run IDs.
                custom_tags:
                  type: string
                  description: >-
                    Additional tags for cluster resources. Databricks will tag
                    all cluster resources (e.g., AWS instances and EBS volumes)
                    with these tags in addition to `default_tags`. Notes: -
                    Currently, Databricks allows at most 45 custom tags -
                    Clusters can only reuse cloud resources if the resources'
                    tags are a subset of the cluster tags
                data_security_mode:
                  type: string
                  description: >-
                    :param docker_image: :class:`DockerImage` (optional) Custom
                    docker image BYOC
                docker_image:
                  type: string
                driver_instance_pool_id:
                  type: string
                  description: >-
                    The optional ID of the instance pool for the driver of the
                    cluster belongs. The pool cluster uses the instance pool
                    with id (instance_pool_id) if the driver pool is not
                    assigned.
                driver_node_type_flexibility:
                  type: string
                  description: Flexible node type configuration for the driver node.
                driver_node_type_id:
                  type: string
                  description: >-
                    The node type of the Spark driver. Note that this field is
                    optional; if unset, the driver node type will be set as the
                    same value as `node_type_id` defined above. This field,
                    along with node_type_id, should not be set if
                    virtual_cluster_size is set. If both driver_node_type_id,
                    node_type_id, and virtual_cluster_size are specified,
                    driver_node_type_id and node_type_id take precedence.
                enable_elastic_disk:
                  type: string
                  description: >-
                    Autoscaling Local Storage: when enabled, this cluster will
                    dynamically acquire additional disk space when its Spark
                    workers are running low on disk space.
                enable_local_disk_encryption:
                  type: string
                  description: Whether to enable LUKS on cluster VMs' local disks
                gcp_attributes:
                  type: string
                  description: >-
                    Attributes related to clusters running on Google Cloud
                    Platform. If not specified at cluster creation, a set of
                    default values will be used.
                init_scripts:
                  type: string
                  description: >-
                    The configuration for storing init scripts. Any number of
                    destinations can be specified. The scripts are executed
                    sequentially in the order provided. If `cluster_log_conf` is
                    specified, init script logs are sent to
                    `<destination>/<cluster-ID>/init_scripts`.
                instance_pool_id:
                  type: string
                  description: >-
                    The optional ID of the instance pool to which the cluster
                    belongs.
                is_single_node:
                  type: string
                  description: >-
                    This field can only be used when `kind = CLASSIC_PREVIEW`.
                    When set to true, Databricks will automatically set single
                    node related `custom_tags`, `spark_conf`, and `num_workers`
                kind:
                  type: string
                  description: >-
                    :param node_type_id: str (optional) This field encodes,
                    through a single value, the resources available to each of
                    the Spark nodes in this cluster. For example, the Spark
                    nodes can be provisioned and optimized for memory or compute
                    intensive workloads. A list of available node types can be
                    retrieved by using the :method:clusters/listNodeTypes API
                    call.
                node_type_id:
                  type: string
                num_workers:
                  type: string
                  description: >-
                    Number of worker nodes that this cluster should have. A
                    cluster has one Spark Driver and `num_workers` Executors for
                    a total of `num_workers` + 1 Spark nodes. Note: When reading
                    the properties of a cluster, this field reflects the desired
                    number of workers rather than the actual current number of
                    workers. For instance, if a cluster is resized from 5 to 10
                    workers, this field will immediately be updated to reflect
                    the target size of 10 workers, whereas the workers listed in
                    `spark_info` will gradually increase from 5 to 10 as the new
                    nodes are provisioned.
                policy_id:
                  type: string
                  description: >-
                    The ID of the cluster policy used to create the cluster if
                    applicable.
                remote_disk_throughput:
                  type: string
                  description: >-
                    If set, what the configurable throughput (in Mb/s) for the
                    remote disk is. Currently only supported for GCP
                    HYPERDISK_BALANCED disks.
                runtime_engine:
                  type: string
                  description: >-
                    Determines the cluster's runtime engine, either standard or
                    Photon. This field is not compatible with legacy
                    `spark_version` values that contain `-photon-`. Remove
                    `-photon-` from the `spark_version` and set `runtime_engine`
                    to `PHOTON`. If left unspecified, the runtime engine
                    defaults to standard unless the spark_version contains
                    -photon-, in which case Photon will be used.
                single_user_name:
                  type: string
                  description: Single user name if data_security_mode is `SINGLE_USER`
                spark_conf:
                  type: string
                  description: >-
                    An object containing a set of optional, user-specified Spark
                    configuration key-value pairs. Users can also pass in a
                    string of extra JVM options to the driver and the executors
                    via `spark.driver.extraJavaOptions` and
                    `spark.executor.extraJavaOptions` respectively.
                spark_env_vars:
                  type: string
                  description: >-
                    An object containing a set of optional, user-specified
                    environment variable key-value pairs. Please note that
                    key-value pair of the form (X,Y) will be exported as is
                    (i.e., `export X='Y'`) while launching the driver and
                    workers. In order to specify an additional set of
                    `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
                    `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below.
                    This ensures that all default databricks managed
                    environmental variables are included as well. Example Spark
                    environment variables: `{"SPARK_WORKER_MEMORY": "28000m",
                    "SPARK_LOCAL_DIRS": "/local_disk0"}` or
                    `{"SPARK_DAEMON_JAVA_OPTS": "$SPARK_DAEMON_JAVA_OPTS
                    -Dspark.shuffle.service.enabled=true"}`
                ssh_public_keys:
                  type: string
                  description: >-
                    SSH public key contents that will be added to each Spark
                    node in this cluster. The corresponding private keys can be
                    used to login with the user name `ubuntu` on port `2200`. Up
                    to 10 keys can be specified.
                total_initial_remote_disk_size:
                  type: string
                  description: >-
                    If set, what the total initial volume size (in GB) of the
                    remote disks should be. Currently only supported for GCP
                    HYPERDISK_BALANCED disks.
                use_ml_runtime:
                  type: string
                  description: >-
                    This field can only be used when `kind = CLASSIC_PREVIEW`.
                    `effective_spark_version` is determined by `spark_version`
                    (DBR release), this field `use_ml_runtime`, and whether
                    `node_type_id` is gpu node or not.
                worker_node_type_flexibility:
                  type: string
                  description: Flexible node type configuration for worker nodes.
                workload_type:
                  type: string
                  description: >-
                    :returns: Long-running operation waiter for
                    :class:`ClusterDetails`. See
                    :method:wait_get_cluster_running for more details.
              required:
                - cluster_id
                - spark_version
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/events:
    post:
      operationId: clusters_events
      summary: >-
        Retrieves a list of events about the activity of a cluster. This API is
        paginated. If there are more
      tags:
        - compute
        - clusters
      description: >-
        Retrieves a list of events about the activity of a cluster. This API is
        paginated. If there are more

        events to read, the response includes all the parameters necessary to
        request the next page of events.


        :param cluster_id: str
          The ID of the cluster to retrieve events about.
        :param end_time: int (optional)
          The end time in epoch milliseconds. If empty, returns events up to the current time.
        :param event_types: List[:class:`EventType`] (optional)
          An optional set of event types to filter on. If empty, all event types are returned.
        :param limit: int (optional)
          Deprecated: use page_token in combination with page_size instead.

          The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed
          value is 500.
        :param offset: int (optional)
          Deprecated: use page_token in combination with page_size instead.

          The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results
          are requested in descending order, the end_time field is required.
        :param order: :class:`GetEventsOrder` (optional)
          The order to list events in; either "ASC" or "DESC". Defaults to "DESC".
        :param page_size: int (optional)
          The maximum number of events to include in a page of events. The server may further constrain the
          maximum number of results returned in a single page. If the page_size is empty or 0, the server will
          decide the number of results to be returned. The field has to be in the range [0,500]. If the value
          is outside the range, the server enforces 0 or 500.
        :param page_token: str (optional)
          Use next_page_token or prev_page_token returned from the previous request to list the next or
          previous page of events respectively. If page_token is empty, the first page is returned.
        :param start_time: int (optional)
          The start time in epoch milliseconds. If empty, returns events starting from the beginning of time.

        :returns: Iterator over :class:`ClusterEvent`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: The ID of the cluster to retrieve events about.
                end_time:
                  type: string
                  description: >-
                    The end time in epoch milliseconds. If empty, returns events
                    up to the current time.
                event_types:
                  type: string
                  description: >-
                    An optional set of event types to filter on. If empty, all
                    event types are returned.
                limit:
                  type: string
                  description: >-
                    Deprecated: use page_token in combination with page_size
                    instead. The maximum number of events to include in a page
                    of events. Defaults to 50, and maximum allowed value is 500.
                offset:
                  type: string
                  description: >-
                    Deprecated: use page_token in combination with page_size
                    instead. The offset in the result set. Defaults to 0 (no
                    offset). When an offset is specified and the results are
                    requested in descending order, the end_time field is
                    required.
                order:
                  type: string
                  description: >-
                    The order to list events in; either "ASC" or "DESC".
                    Defaults to "DESC".
                page_size:
                  type: string
                  description: >-
                    The maximum number of events to include in a page of events.
                    The server may further constrain the maximum number of
                    results returned in a single page. If the page_size is empty
                    or 0, the server will decide the number of results to be
                    returned. The field has to be in the range [0,500]. If the
                    value is outside the range, the server enforces 0 or 500.
                page_token:
                  type: string
                  description: >-
                    Use next_page_token or prev_page_token returned from the
                    previous request to list the next or previous page of events
                    respectively. If page_token is empty, the first page is
                    returned.
                start_time:
                  type: string
                  description: >-
                    The start time in epoch milliseconds. If empty, returns
                    events starting from the beginning of time.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[ClusterEvent]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/get:
    get:
      operationId: clusters_get
      summary: >-
        Retrieves the information for a cluster given its identifier. Clusters
        can be described while they are
      tags:
        - compute
        - clusters
      description: >-
        Retrieves the information for a cluster given its identifier. Clusters
        can be described while they are

        running, or up to 60 days after they are terminated.


        :param cluster_id: str
          The cluster about which to retrieve information.

        :returns: :class:`ClusterDetails`
      parameters:
        - name: cluster_id
          in: query
          required: true
          schema:
            type: string
          description: The cluster about which to retrieve information.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterDetails'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/clusters/{cluster_id}/permissionLevels:
    get:
      operationId: clusters_get_permission_levels
      summary: Gets the permission levels that a user can have on an object.
      tags:
        - compute
        - clusters
      description: |-
        Gets the permission levels that a user can have on an object.

        :param cluster_id: str
          The cluster for which to get or manage permissions.

        :returns: :class:`GetClusterPermissionLevelsResponse`
      parameters:
        - name: cluster_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetClusterPermissionLevelsResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/clusters/{cluster_id}:
    get:
      operationId: clusters_get_permissions
      summary: >-
        Gets the permissions of a cluster. Clusters can inherit permissions from
        their root object.
      tags:
        - compute
        - clusters
      description: >-
        Gets the permissions of a cluster. Clusters can inherit permissions from
        their root object.


        :param cluster_id: str
          The cluster for which to get or manage permissions.

        :returns: :class:`ClusterPermissions`
      parameters:
        - name: cluster_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    put:
      operationId: clusters_set_permissions
      summary: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct
      tags:
        - compute
        - clusters
      description: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from
        their root object.


        :param cluster_id: str
          The cluster for which to get or manage permissions.
        :param access_control_list: List[:class:`ClusterAccessControlRequest`]
        (optional)


        :returns: :class:`ClusterPermissions`
      parameters:
        - name: cluster_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`ClusterPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    patch:
      operationId: clusters_update_permissions
      summary: >-
        Updates the permissions on a cluster. Clusters can inherit permissions
        from their root object.
      tags:
        - compute
        - clusters
      description: >-
        Updates the permissions on a cluster. Clusters can inherit permissions
        from their root object.


        :param cluster_id: str
          The cluster for which to get or manage permissions.
        :param access_control_list: List[:class:`ClusterAccessControlRequest`]
        (optional)


        :returns: :class:`ClusterPermissions`
      parameters:
        - name: cluster_id
          in: path
          required: true
          schema:
            type: string
          description: The cluster for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`ClusterPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClusterPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/list:
    get:
      operationId: clusters_list
      summary: >-
        Return information about all pinned and active clusters, and all
        clusters terminated within the last
      tags:
        - compute
        - clusters
      description: >-
        Return information about all pinned and active clusters, and all
        clusters terminated within the last

        30 days. Clusters terminated prior to this period are not included.


        :param filter_by: :class:`ListClustersFilterBy` (optional)
          Filters to apply to the list of clusters.
        :param page_size: int (optional)
          Use this field to specify the maximum number of results to be returned by the server. The server may
          further constrain the maximum number of results returned in a single page.
        :param page_token: str (optional)
          Use next_page_token or prev_page_token returned from the previous request to list the next or
          previous page of clusters respectively.
        :param sort_by: :class:`ListClustersSortBy` (optional)
          Sort the list of clusters by a specific criteria.

        :returns: Iterator over :class:`ClusterDetails`
      parameters:
        - name: filter_by
          in: query
          required: false
          schema:
            type: string
          description: Filters to apply to the list of clusters.
        - name: page_size
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use this field to specify the maximum number of results to be
            returned by the server. The server may further constrain the maximum
            number of results returned in a single page.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use next_page_token or prev_page_token returned from the previous
            request to list the next or previous page of clusters respectively.
        - name: sort_by
          in: query
          required: false
          schema:
            type: string
          description: Sort the list of clusters by a specific criteria.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/list-node-types:
    get:
      operationId: clusters_list_node_types
      summary: >-
        Returns a list of supported Spark node types. These node types can be
        used to launch a cluster.
      tags:
        - compute
        - clusters
      description: >-
        Returns a list of supported Spark node types. These node types can be
        used to launch a cluster.



        :returns: :class:`ListNodeTypesResponse`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListNodeTypesResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/list-zones:
    get:
      operationId: clusters_list_zones
      summary: >-
        Returns a list of availability zones where clusters can be created in
        (For example, us-west-2a). These
      tags:
        - compute
        - clusters
      description: >-
        Returns a list of availability zones where clusters can be created in
        (For example, us-west-2a). These

        zones can be used to launch a cluster.



        :returns: :class:`ListAvailableZonesResponse`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListAvailableZonesResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/permanent-delete:
    post:
      operationId: clusters_permanent_delete
      summary: >-
        Permanently deletes a Spark cluster. This cluster is terminated and
        resources are asynchronously
      tags:
        - compute
        - clusters
      description: >-
        Permanently deletes a Spark cluster. This cluster is terminated and
        resources are asynchronously

        removed.


        In addition, users will no longer see permanently deleted clusters in
        the cluster list, and API users

        can no longer perform any action on permanently deleted clusters.


        :param cluster_id: str
          The cluster to be deleted.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: The cluster to be deleted.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/pin:
    post:
      operationId: clusters_pin
      summary: >-
        Pinning a cluster ensures that the cluster will always be returned by
        the ListClusters API. Pinning a
      tags:
        - compute
        - clusters
      description: >-
        Pinning a cluster ensures that the cluster will always be returned by
        the ListClusters API. Pinning a

        cluster that is already pinned will have no effect. This API can only be
        called by workspace admins.


        :param cluster_id: str
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str
              required:
                - cluster_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/resize:
    post:
      operationId: clusters_resize
      summary: >-
        Resizes a cluster to have a desired number of workers. This will fail
        unless the cluster is in a
      tags:
        - compute
        - clusters
      description: >-
        Resizes a cluster to have a desired number of workers. This will fail
        unless the cluster is in a

        `RUNNING` state.


        :param cluster_id: str
          The cluster to be resized.
        :param autoscale: :class:`AutoScale` (optional)
          Parameters needed in order to automatically scale clusters up and down based on load. Note:
          autoscaling works best with DB runtime versions 3.0 or later.
        :param num_workers: int (optional)
          Number of worker nodes that this cluster should have. A cluster has one Spark Driver and
          `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.

          Note: When reading the properties of a cluster, this field reflects the desired number of workers
          rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10
          workers, this field will immediately be updated to reflect the target size of 10 workers, whereas
          the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are
          provisioned.

        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: The cluster to be resized.
                autoscale:
                  type: string
                  description: >-
                    Parameters needed in order to automatically scale clusters
                    up and down based on load. Note: autoscaling works best with
                    DB runtime versions 3.0 or later.
                num_workers:
                  type: string
                  description: >-
                    Number of worker nodes that this cluster should have. A
                    cluster has one Spark Driver and `num_workers` Executors for
                    a total of `num_workers` + 1 Spark nodes. Note: When reading
                    the properties of a cluster, this field reflects the desired
                    number of workers rather than the actual current number of
                    workers. For instance, if a cluster is resized from 5 to 10
                    workers, this field will immediately be updated to reflect
                    the target size of 10 workers, whereas the workers listed in
                    `spark_info` will gradually increase from 5 to 10 as the new
                    nodes are provisioned.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/restart:
    post:
      operationId: clusters_restart
      summary: >-
        Restarts a Spark cluster with the supplied ID. If the cluster is not
        currently in a `RUNNING` state,
      tags:
        - compute
        - clusters
      description: >-
        Restarts a Spark cluster with the supplied ID. If the cluster is not
        currently in a `RUNNING` state,

        nothing will happen.


        :param cluster_id: str
          The cluster to be started.
        :param restart_user: str (optional)


        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: The cluster to be started.
                restart_user:
                  type: string
                  description: >-
                    :returns: Long-running operation waiter for
                    :class:`ClusterDetails`. See
                    :method:wait_get_cluster_running for more details.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/spark-versions:
    get:
      operationId: clusters_spark_versions
      summary: >-
        Returns the list of available Spark versions. These versions can be used
        to launch a cluster.
      tags:
        - compute
        - clusters
      description: >-
        Returns the list of available Spark versions. These versions can be used
        to launch a cluster.



        :returns: :class:`GetSparkVersionsResponse`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetSparkVersionsResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/start:
    post:
      operationId: clusters_start
      summary: >-
        Starts a terminated Spark cluster with the supplied ID. This works
        similar to `createCluster` except:
      tags:
        - compute
        - clusters
      description: >-
        Starts a terminated Spark cluster with the supplied ID. This works
        similar to `createCluster` except:

        - The previous cluster id and attributes are preserved. - The cluster
        starts with the last specified

        cluster size. - If the previous cluster was an autoscaling cluster, the
        current cluster starts with

        the minimum number of nodes. - If the cluster is not currently in a
        ``TERMINATED`` state, nothing will

        happen. - Clusters launched to run a job cannot be started.


        :param cluster_id: str
          The cluster to be started.

        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: The cluster to be started.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/unpin:
    post:
      operationId: clusters_unpin
      summary: >-
        Unpinning a cluster will allow the cluster to eventually be removed from
        the ListClusters API.
      tags:
        - compute
        - clusters
      description: >-
        Unpinning a cluster will allow the cluster to eventually be removed from
        the ListClusters API.

        Unpinning a cluster that is not pinned will have no effect. This API can
        only be called by workspace

        admins.


        :param cluster_id: str
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: str
              required:
                - cluster_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.1/clusters/update:
    post:
      operationId: clusters_update
      summary: >-
        Updates the configuration of a cluster to match the partial set of
        attributes and size. Denote which
      tags:
        - compute
        - clusters
      description: >-
        Updates the configuration of a cluster to match the partial set of
        attributes and size. Denote which

        fields to update using the `update_mask` field in the request body. A
        cluster can be updated if it is

        in a `RUNNING` or `TERMINATED` state. If a cluster is updated while in a
        `RUNNING` state, it will be

        restarted so that the new attributes can take effect. If a cluster is
        updated while in a `TERMINATED`

        state, it will remain `TERMINATED`. The updated attributes will take
        effect the next time the cluster

        is started using the `clusters/start` API. Attempts to update a cluster
        in any other state will be

        rejected with an `INVALID_STATE` error code. Clusters created by the
        Databricks Jobs service cannot be

        updated.


        :param cluster_id: str
          ID of the cluster.
        :param update_mask: str
          Used to specify which cluster attributes and size fields to update. See https://google.aip.dev/161
          for more details.

          The field mask must be a single string, with multiple fields separated by commas (no spaces). The
          field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,
          `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only
          the entire collection field can be specified. Field names must exactly match the resource field
          names.

          A field mask of `*` indicates full replacement. Its recommended to always explicitly list the
          fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API
          changes in the future.
        :param cluster: :class:`UpdateClusterResource` (optional)
          The cluster to be updated.

        :returns:
          Long-running operation waiter for :class:`ClusterDetails`.
          See :method:wait_get_cluster_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: ID of the cluster.
                update_mask:
                  type: string
                  description: >-
                    Used to specify which cluster attributes and size fields to
                    update. See https://google.aip.dev/161 for more details. The
                    field mask must be a single string, with multiple fields
                    separated by commas (no spaces). The field path is relative
                    to the resource object, using a dot (`.`) to navigate
                    sub-fields (e.g., `author.given_name`). Specification of
                    elements in sequence or map fields is not allowed, as only
                    the entire collection field can be specified. Field names
                    must exactly match the resource field names. A field mask of
                    `*` indicates full replacement. Its recommended to always
                    explicitly list the fields being updated and avoid using `*`
                    wildcards, as it can lead to unintended results if the API
                    changes in the future.
                cluster:
                  type: string
                  description: The cluster to be updated.
              required:
                - cluster_id
                - update_mask
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ClusterDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/1.2/commands/cancel:
    post:
      operationId: command_execution_cancel
      summary: Cancels a currently running command within an execution context.
      tags:
        - compute
        - command_execution
      description: |-
        Cancels a currently running command within an execution context.

        The command ID is obtained from a prior successful call to __execute__.

        :param cluster_id: str (optional)
        :param command_id: str (optional)
        :param context_id: str (optional)

        :returns:
          Long-running operation waiter for :class:`CommandStatusResponse`.
          See :method:wait_command_status_command_execution_cancelled for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: ':param command_id: str (optional)'
                command_id:
                  type: string
                context_id:
                  type: string
                  description: >-
                    :returns: Long-running operation waiter for
                    :class:`CommandStatusResponse`. See
                    :method:wait_command_status_command_execution_cancelled for
                    more details.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[CommandStatusResponse]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/1.2/commands/status:
    get:
      operationId: command_execution_command_status
      summary: >-
        Gets the status of and, if available, the results from a currently
        executing command.
      tags:
        - compute
        - command_execution
      description: >-
        Gets the status of and, if available, the results from a currently
        executing command.


        The command ID is obtained from a prior successful call to __execute__.


        :param cluster_id: str

        :param context_id: str

        :param command_id: str


        :returns: :class:`CommandStatusResponse`
      parameters:
        - name: cluster_id
          in: query
          required: true
          schema:
            type: string
          description: ':param context_id: str'
        - name: context_id
          in: query
          required: true
          schema:
            type: string
        - name: command_id
          in: query
          required: true
          schema:
            type: string
          description: ':returns: :class:`CommandStatusResponse`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CommandStatusResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/1.2/contexts/status:
    get:
      operationId: command_execution_context_status
      summary: Gets the status for an execution context.
      tags:
        - compute
        - command_execution
      description: |-
        Gets the status for an execution context.

        :param cluster_id: str
        :param context_id: str

        :returns: :class:`ContextStatusResponse`
      parameters:
        - name: cluster_id
          in: query
          required: true
          schema:
            type: string
          description: ':param context_id: str'
        - name: context_id
          in: query
          required: true
          schema:
            type: string
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ContextStatusResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/1.2/contexts/create:
    post:
      operationId: command_execution_create
      summary: Creates an execution context for running cluster commands.
      tags:
        - compute
        - command_execution
      description: |-
        Creates an execution context for running cluster commands.

        If successful, this method returns the ID of the new execution context.

        :param cluster_id: str (optional)
          Running cluster id
        :param language: :class:`Language` (optional)

        :returns:
          Long-running operation waiter for :class:`ContextStatusResponse`.
          See :method:wait_context_status_command_execution_running for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: Running cluster id
                language:
                  type: string
                  description: >-
                    :returns: Long-running operation waiter for
                    :class:`ContextStatusResponse`. See
                    :method:wait_context_status_command_execution_running for
                    more details.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[ContextStatusResponse]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/1.2/contexts/destroy:
    post:
      operationId: command_execution_destroy
      summary: Deletes an execution context.
      tags:
        - compute
        - command_execution
      description: |-
        Deletes an execution context.

        :param cluster_id: str
        :param context_id: str
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: ':param context_id: str'
                context_id:
                  type: string
              required:
                - cluster_id
                - context_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/1.2/commands/execute:
    post:
      operationId: command_execution_execute
      summary: >-
        Runs a cluster command in the given execution context, using the
        provided language.
      tags:
        - compute
        - command_execution
      description: >-
        Runs a cluster command in the given execution context, using the
        provided language.


        If successful, it returns an ID for tracking the status of the command's
        execution.


        :param cluster_id: str (optional)
          Running cluster id
        :param command: str (optional)
          Executable code
        :param context_id: str (optional)
          Running context id
        :param language: :class:`Language` (optional)


        :returns:
          Long-running operation waiter for :class:`CommandStatusResponse`.
          See :method:wait_command_status_command_execution_finished_or_error for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: Running cluster id
                command:
                  type: string
                  description: Executable code
                context_id:
                  type: string
                  description: Running context id
                language:
                  type: string
                  description: >-
                    :returns: Long-running operation waiter for
                    :class:`CommandStatusResponse`. See
                    :method:wait_command_status_command_execution_finished_or_error
                    for more details.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[CommandStatusResponse]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/global-init-scripts:
    post:
      operationId: global_init_scripts_create
      summary: Creates a new global init script in this workspace.
      tags:
        - compute
        - global_init_scripts
      description: |-
        Creates a new global init script in this workspace.

        :param name: str
          The name of the script
        :param script: str
          The Base64-encoded content of the script.
        :param enabled: bool (optional)
          Specifies whether the script is enabled. The script runs only if enabled.
        :param position: int (optional)
          The position of a global init script, where 0 represents the first script to run, 1 is the second
          script to run, in ascending order.

          If you omit the numeric position for a new global init script, it defaults to last position. It will
          run after all current scripts. Setting any value greater than the position of the last script is
          equivalent to the last position. Example: Take three existing scripts with positions 0, 1, and 2.
          Any position of (3) or greater puts the script in the last position. If an explicit position value
          conflicts with an existing script value, your request succeeds, but the original script at that
          position and all later scripts have their positions incremented by 1.

        :returns: :class:`CreateResponse`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                name:
                  type: string
                  description: The name of the script
                script:
                  type: string
                  description: The Base64-encoded content of the script.
                enabled:
                  type: string
                  description: >-
                    Specifies whether the script is enabled. The script runs
                    only if enabled.
                position:
                  type: string
                  description: >-
                    The position of a global init script, where 0 represents the
                    first script to run, 1 is the second script to run, in
                    ascending order. If you omit the numeric position for a new
                    global init script, it defaults to last position. It will
                    run after all current scripts. Setting any value greater
                    than the position of the last script is equivalent to the
                    last position. Example: Take three existing scripts with
                    positions 0, 1, and 2. Any position of (3) or greater puts
                    the script in the last position. If an explicit position
                    value conflicts with an existing script value, your request
                    succeeds, but the original script at that position and all
                    later scripts have their positions incremented by 1.
              required:
                - name
                - script
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    get:
      operationId: global_init_scripts_list
      summary: >-
        Get a list of all global init scripts for this workspace. This returns
        all properties for each script
      tags:
        - compute
        - global_init_scripts
      description: >-
        Get a list of all global init scripts for this workspace. This returns
        all properties for each script

        but **not** the script contents. To retrieve the contents of a script,
        use the [get a global init

        script](:method:globalinitscripts/get) operation.



        :returns: Iterator over :class:`GlobalInitScriptDetails`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[GlobalInitScriptDetails]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/global-init-scripts/{script_id}:
    delete:
      operationId: global_init_scripts_delete
      summary: Deletes a global init script.
      tags:
        - compute
        - global_init_scripts
      description: |-
        Deletes a global init script.

        :param script_id: str
          The ID of the global init script.
      parameters:
        - name: script_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the global init script.
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    get:
      operationId: global_init_scripts_get
      summary: Gets all the details of a script, including its Base64-encoded contents.
      tags:
        - compute
        - global_init_scripts
      description: |-
        Gets all the details of a script, including its Base64-encoded contents.

        :param script_id: str
          The ID of the global init script.

        :returns: :class:`GlobalInitScriptDetailsWithContent`
      parameters:
        - name: script_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the global init script.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GlobalInitScriptDetailsWithContent'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    patch:
      operationId: global_init_scripts_update
      summary: >-
        Updates a global init script, specifying only the fields to change. All
        fields are optional.
      tags:
        - compute
        - global_init_scripts
      description: >-
        Updates a global init script, specifying only the fields to change. All
        fields are optional.

        Unspecified fields retain their current value.


        :param script_id: str
          The ID of the global init script.
        :param name: str
          The name of the script
        :param script: str
          The Base64-encoded content of the script.
        :param enabled: bool (optional)
          Specifies whether the script is enabled. The script runs only if enabled.
        :param position: int (optional)
          The position of a script, where 0 represents the first script to run, 1 is the second script to run,
          in ascending order. To move the script to run first, set its position to 0.

          To move the script to the end, set its position to any value greater or equal to the position of the
          last script. Example, three existing scripts with positions 0, 1, and 2. Any position value of 2 or
          greater puts the script in the last position (2).

          If an explicit position value conflicts with an existing script, your request succeeds, but the
          original script at that position and all later scripts have their positions incremented by 1.
      parameters:
        - name: script_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the global init script.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                name:
                  type: string
                  description: The name of the script
                script:
                  type: string
                  description: The Base64-encoded content of the script.
                enabled:
                  type: string
                  description: >-
                    Specifies whether the script is enabled. The script runs
                    only if enabled.
                position:
                  type: string
                  description: >-
                    The position of a script, where 0 represents the first
                    script to run, 1 is the second script to run, in ascending
                    order. To move the script to run first, set its position to
                    0. To move the script to the end, set its position to any
                    value greater or equal to the position of the last script.
                    Example, three existing scripts with positions 0, 1, and 2.
                    Any position value of 2 or greater puts the script in the
                    last position (2). If an explicit position value conflicts
                    with an existing script, your request succeeds, but the
                    original script at that position and all later scripts have
                    their positions incremented by 1.
              required:
                - name
                - script
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-pools/create:
    post:
      operationId: instance_pools_create
      summary: Creates a new instance pool using idle and ready-to-use cloud instances.
      tags:
        - compute
        - instance_pools
      description: |-
        Creates a new instance pool using idle and ready-to-use cloud instances.

        :param instance_pool_name: str
          Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100
          characters.
        :param node_type_id: str
          This field encodes, through a single value, the resources available to each of the Spark nodes in
          this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute
          intensive workloads. A list of available node types can be retrieved by using the
          :method:clusters/listNodeTypes API call.
        :param aws_attributes: :class:`InstancePoolAwsAttributes` (optional)
          Attributes related to instance pools running on Amazon Web Services. If not specified at pool
          creation, a set of default values will be used.
        :param azure_attributes: :class:`InstancePoolAzureAttributes` (optional)
          Attributes related to instance pools running on Azure. If not specified at pool creation, a set of
          default values will be used.
        :param custom_tags: Dict[str,str] (optional)
          Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and
          EBS volumes) with these tags in addition to `default_tags`. Notes:

          - Currently, Databricks allows at most 45 custom tags
        :param disk_spec: :class:`DiskSpec` (optional)
          Defines the specification of the disks that will be attached to all spark containers.
        :param enable_elastic_disk: bool (optional)
          Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire
          additional disk space when its Spark workers are running low on disk space. In AWS, this feature
          requires specific AWS permissions to function correctly - refer to the User Guide for more details.
        :param gcp_attributes: :class:`InstancePoolGcpAttributes` (optional)
          Attributes related to instance pools running on Google Cloud Platform. If not specified at pool
          creation, a set of default values will be used.
        :param idle_instance_autotermination_minutes: int (optional)
          Automatically terminates the extra instances in the pool cache after they are inactive for this time
          in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances
          will be automatically terminated after a default timeout. If specified, the threshold must be
          between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances
          from the cache if min cache size could still hold.
        :param max_capacity: int (optional)
          Maximum number of outstanding instances to keep in the pool, including both instances used by
          clusters and idle instances. Clusters that require further instance provisioning will fail during
          upsize requests.
        :param min_idle_instances: int (optional)
          Minimum number of idle instances to keep in the instance pool
        :param node_type_flexibility: :class:`NodeTypeFlexibility` (optional)
          Flexible node type configuration for the pool.
        :param preloaded_docker_images: List[:class:`DockerImage`] (optional)
          Custom Docker Image BYOC
        :param preloaded_spark_versions: List[str] (optional)
          A list containing at most one preloaded Spark image version for the pool. Pool-backed clusters
          started with the preloaded Spark version will start faster. A list of available Spark versions can
          be retrieved by using the :method:clusters/sparkVersions API call.
        :param remote_disk_throughput: int (optional)
          If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported
          for GCP HYPERDISK_BALANCED types.
        :param total_initial_remote_disk_size: int (optional)
          If set, what the total initial volume size (in GB) of the remote disks should be. Currently only
          supported for GCP HYPERDISK_BALANCED types.

        :returns: :class:`CreateInstancePoolResponse`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_pool_name:
                  type: string
                  description: >-
                    Pool name requested by the user. Pool name must be unique.
                    Length must be between 1 and 100 characters.
                node_type_id:
                  type: string
                  description: >-
                    This field encodes, through a single value, the resources
                    available to each of the Spark nodes in this cluster. For
                    example, the Spark nodes can be provisioned and optimized
                    for memory or compute intensive workloads. A list of
                    available node types can be retrieved by using the
                    :method:clusters/listNodeTypes API call.
                aws_attributes:
                  type: string
                  description: >-
                    Attributes related to instance pools running on Amazon Web
                    Services. If not specified at pool creation, a set of
                    default values will be used.
                azure_attributes:
                  type: string
                  description: >-
                    Attributes related to instance pools running on Azure. If
                    not specified at pool creation, a set of default values will
                    be used.
                custom_tags:
                  type: string
                  description: >-
                    Additional tags for pool resources. Databricks will tag all
                    pool resources (e.g., AWS instances and EBS volumes) with
                    these tags in addition to `default_tags`. Notes: -
                    Currently, Databricks allows at most 45 custom tags
                disk_spec:
                  type: string
                  description: >-
                    Defines the specification of the disks that will be attached
                    to all spark containers.
                enable_elastic_disk:
                  type: string
                  description: >-
                    Autoscaling Local Storage: when enabled, this instances in
                    this pool will dynamically acquire additional disk space
                    when its Spark workers are running low on disk space. In
                    AWS, this feature requires specific AWS permissions to
                    function correctly - refer to the User Guide for more
                    details.
                gcp_attributes:
                  type: string
                  description: >-
                    Attributes related to instance pools running on Google Cloud
                    Platform. If not specified at pool creation, a set of
                    default values will be used.
                idle_instance_autotermination_minutes:
                  type: string
                  description: >-
                    Automatically terminates the extra instances in the pool
                    cache after they are inactive for this time in minutes if
                    min_idle_instances requirement is already met. If not set,
                    the extra pool instances will be automatically terminated
                    after a default timeout. If specified, the threshold must be
                    between 0 and 10000 minutes. Users can also set this value
                    to 0 to instantly remove idle instances from the cache if
                    min cache size could still hold.
                max_capacity:
                  type: string
                  description: >-
                    Maximum number of outstanding instances to keep in the pool,
                    including both instances used by clusters and idle
                    instances. Clusters that require further instance
                    provisioning will fail during upsize requests.
                min_idle_instances:
                  type: string
                  description: >-
                    Minimum number of idle instances to keep in the instance
                    pool
                node_type_flexibility:
                  type: string
                  description: Flexible node type configuration for the pool.
                preloaded_docker_images:
                  type: string
                  description: Custom Docker Image BYOC
                preloaded_spark_versions:
                  type: string
                  description: >-
                    A list containing at most one preloaded Spark image version
                    for the pool. Pool-backed clusters started with the
                    preloaded Spark version will start faster. A list of
                    available Spark versions can be retrieved by using the
                    :method:clusters/sparkVersions API call.
                remote_disk_throughput:
                  type: string
                  description: >-
                    If set, what the configurable throughput (in Mb/s) for the
                    remote disk is. Currently only supported for GCP
                    HYPERDISK_BALANCED types.
                total_initial_remote_disk_size:
                  type: string
                  description: >-
                    If set, what the total initial volume size (in GB) of the
                    remote disks should be. Currently only supported for GCP
                    HYPERDISK_BALANCED types.
              required:
                - instance_pool_name
                - node_type_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateInstancePoolResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-pools/delete:
    post:
      operationId: instance_pools_delete
      summary: >-
        Deletes the instance pool permanently. The idle instances in the pool
        are terminated asynchronously.
      tags:
        - compute
        - instance_pools
      description: >-
        Deletes the instance pool permanently. The idle instances in the pool
        are terminated asynchronously.


        :param instance_pool_id: str
          The instance pool to be terminated.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_pool_id:
                  type: string
                  description: The instance pool to be terminated.
              required:
                - instance_pool_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-pools/edit:
    post:
      operationId: instance_pools_edit
      summary: Modifies the configuration of an existing instance pool.
      tags:
        - compute
        - instance_pools
      description: |-
        Modifies the configuration of an existing instance pool.

        :param instance_pool_id: str
          Instance pool ID
        :param instance_pool_name: str
          Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100
          characters.
        :param node_type_id: str
          This field encodes, through a single value, the resources available to each of the Spark nodes in
          this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute
          intensive workloads. A list of available node types can be retrieved by using the
          :method:clusters/listNodeTypes API call.
        :param custom_tags: Dict[str,str] (optional)
          Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and
          EBS volumes) with these tags in addition to `default_tags`. Notes:

          - Currently, Databricks allows at most 45 custom tags
        :param idle_instance_autotermination_minutes: int (optional)
          Automatically terminates the extra instances in the pool cache after they are inactive for this time
          in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances
          will be automatically terminated after a default timeout. If specified, the threshold must be
          between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances
          from the cache if min cache size could still hold.
        :param max_capacity: int (optional)
          Maximum number of outstanding instances to keep in the pool, including both instances used by
          clusters and idle instances. Clusters that require further instance provisioning will fail during
          upsize requests.
        :param min_idle_instances: int (optional)
          Minimum number of idle instances to keep in the instance pool
        :param node_type_flexibility: :class:`NodeTypeFlexibility` (optional)
          Flexible node type configuration for the pool.
        :param remote_disk_throughput: int (optional)
          If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported
          for GCP HYPERDISK_BALANCED types.
        :param total_initial_remote_disk_size: int (optional)
          If set, what the total initial volume size (in GB) of the remote disks should be. Currently only
          supported for GCP HYPERDISK_BALANCED types.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_pool_id:
                  type: string
                  description: Instance pool ID
                instance_pool_name:
                  type: string
                  description: >-
                    Pool name requested by the user. Pool name must be unique.
                    Length must be between 1 and 100 characters.
                node_type_id:
                  type: string
                  description: >-
                    This field encodes, through a single value, the resources
                    available to each of the Spark nodes in this cluster. For
                    example, the Spark nodes can be provisioned and optimized
                    for memory or compute intensive workloads. A list of
                    available node types can be retrieved by using the
                    :method:clusters/listNodeTypes API call.
                custom_tags:
                  type: string
                  description: >-
                    Additional tags for pool resources. Databricks will tag all
                    pool resources (e.g., AWS instances and EBS volumes) with
                    these tags in addition to `default_tags`. Notes: -
                    Currently, Databricks allows at most 45 custom tags
                idle_instance_autotermination_minutes:
                  type: string
                  description: >-
                    Automatically terminates the extra instances in the pool
                    cache after they are inactive for this time in minutes if
                    min_idle_instances requirement is already met. If not set,
                    the extra pool instances will be automatically terminated
                    after a default timeout. If specified, the threshold must be
                    between 0 and 10000 minutes. Users can also set this value
                    to 0 to instantly remove idle instances from the cache if
                    min cache size could still hold.
                max_capacity:
                  type: string
                  description: >-
                    Maximum number of outstanding instances to keep in the pool,
                    including both instances used by clusters and idle
                    instances. Clusters that require further instance
                    provisioning will fail during upsize requests.
                min_idle_instances:
                  type: string
                  description: >-
                    Minimum number of idle instances to keep in the instance
                    pool
                node_type_flexibility:
                  type: string
                  description: Flexible node type configuration for the pool.
                remote_disk_throughput:
                  type: string
                  description: >-
                    If set, what the configurable throughput (in Mb/s) for the
                    remote disk is. Currently only supported for GCP
                    HYPERDISK_BALANCED types.
                total_initial_remote_disk_size:
                  type: string
                  description: >-
                    If set, what the total initial volume size (in GB) of the
                    remote disks should be. Currently only supported for GCP
                    HYPERDISK_BALANCED types.
              required:
                - instance_pool_id
                - instance_pool_name
                - node_type_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-pools/get:
    get:
      operationId: instance_pools_get
      summary: Retrieve the information for an instance pool based on its identifier.
      tags:
        - compute
        - instance_pools
      description: |-
        Retrieve the information for an instance pool based on its identifier.

        :param instance_pool_id: str
          The canonical unique identifier for the instance pool.

        :returns: :class:`GetInstancePool`
      parameters:
        - name: instance_pool_id
          in: query
          required: true
          schema:
            type: string
          description: The canonical unique identifier for the instance pool.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetInstancePool'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/instance-pools/{instance_pool_id}/permissionLevels:
    get:
      operationId: instance_pools_get_permission_levels
      summary: Gets the permission levels that a user can have on an object.
      tags:
        - compute
        - instance_pools
      description: |-
        Gets the permission levels that a user can have on an object.

        :param instance_pool_id: str
          The instance pool for which to get or manage permissions.

        :returns: :class:`GetInstancePoolPermissionLevelsResponse`
      parameters:
        - name: instance_pool_id
          in: path
          required: true
          schema:
            type: string
          description: The instance pool for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetInstancePoolPermissionLevelsResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/instance-pools/{instance_pool_id}:
    get:
      operationId: instance_pools_get_permissions
      summary: >-
        Gets the permissions of an instance pool. Instance pools can inherit
        permissions from their root
      tags:
        - compute
        - instance_pools
      description: >-
        Gets the permissions of an instance pool. Instance pools can inherit
        permissions from their root

        object.


        :param instance_pool_id: str
          The instance pool for which to get or manage permissions.

        :returns: :class:`InstancePoolPermissions`
      parameters:
        - name: instance_pool_id
          in: path
          required: true
          schema:
            type: string
          description: The instance pool for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstancePoolPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    put:
      operationId: instance_pools_set_permissions
      summary: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct
      tags:
        - compute
        - instance_pools
      description: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from
        their root object.


        :param instance_pool_id: str
          The instance pool for which to get or manage permissions.
        :param access_control_list:
        List[:class:`InstancePoolAccessControlRequest`] (optional)


        :returns: :class:`InstancePoolPermissions`
      parameters:
        - name: instance_pool_id
          in: path
          required: true
          schema:
            type: string
          description: The instance pool for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`InstancePoolPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstancePoolPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    patch:
      operationId: instance_pools_update_permissions
      summary: >-
        Updates the permissions on an instance pool. Instance pools can inherit
        permissions from their root
      tags:
        - compute
        - instance_pools
      description: >-
        Updates the permissions on an instance pool. Instance pools can inherit
        permissions from their root

        object.


        :param instance_pool_id: str
          The instance pool for which to get or manage permissions.
        :param access_control_list:
        List[:class:`InstancePoolAccessControlRequest`] (optional)


        :returns: :class:`InstancePoolPermissions`
      parameters:
        - name: instance_pool_id
          in: path
          required: true
          schema:
            type: string
          description: The instance pool for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`InstancePoolPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InstancePoolPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-pools/list:
    get:
      operationId: instance_pools_list
      summary: Gets a list of instance pools with their statistics.
      tags:
        - compute
        - instance_pools
      description: |-
        Gets a list of instance pools with their statistics.


        :returns: Iterator over :class:`InstancePoolAndStats`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[InstancePoolAndStats]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-profiles/add:
    post:
      operationId: instance_profiles_add
      summary: >-
        Registers an instance profile in Databricks. In the UI, you can then
        give users the permission to use
      tags:
        - compute
        - instance_profiles
      description: >-
        Registers an instance profile in Databricks. In the UI, you can then
        give users the permission to use

        this instance profile when launching clusters.


        This API is only available to admin users.


        :param instance_profile_arn: str
          The AWS ARN of the instance profile to register with Databricks. This field is required.
        :param iam_role_arn: str (optional)
          The AWS IAM role ARN of the role associated with the instance profile. This field is required if
          your role name and instance profile name do not match and you want to use the instance profile with
          [Databricks SQL Serverless].

          Otherwise, this field is optional.

          [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html
        :param is_meta_instance_profile: bool (optional)
          Boolean flag indicating whether the instance profile should only be used in credential passthrough
          scenarios. If true, it means the instance profile contains an meta IAM role which could assume a
          wide range of roles. Therefore it should always be used with authorization. This field is optional,
          the default value is `false`.
        :param skip_validation: bool (optional)
          By default, Databricks validates that it has sufficient permissions to launch instances with the
          instance profile. This validation uses AWS dry-run mode for the RunInstances API. If validation
          fails with an error message that does not indicate an IAM related permission issue, (e.g. Your
          requested instance type is not supported in your requested availability zone), you can pass this
          flag to skip the validation and forcibly add the instance profile.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_profile_arn:
                  type: string
                  description: >-
                    The AWS ARN of the instance profile to register with
                    Databricks. This field is required.
                iam_role_arn:
                  type: string
                  description: >-
                    The AWS IAM role ARN of the role associated with the
                    instance profile. This field is required if your role name
                    and instance profile name do not match and you want to use
                    the instance profile with [Databricks SQL Serverless].
                    Otherwise, this field is optional. [Databricks SQL
                    Serverless]:
                    https://docs.databricks.com/sql/admin/serverless.html
                is_meta_instance_profile:
                  type: string
                  description: >-
                    Boolean flag indicating whether the instance profile should
                    only be used in credential passthrough scenarios. If true,
                    it means the instance profile contains an meta IAM role
                    which could assume a wide range of roles. Therefore it
                    should always be used with authorization. This field is
                    optional, the default value is `false`.
                skip_validation:
                  type: string
                  description: >-
                    By default, Databricks validates that it has sufficient
                    permissions to launch instances with the instance profile.
                    This validation uses AWS dry-run mode for the RunInstances
                    API. If validation fails with an error message that does not
                    indicate an IAM related permission issue, (e.g. Your
                    requested instance type is not supported in your requested
                    availability zone), you can pass this flag to skip the
                    validation and forcibly add the instance profile.
              required:
                - instance_profile_arn
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-profiles/edit:
    post:
      operationId: instance_profiles_edit
      summary: >-
        The only supported field to change is the optional IAM role ARN
        associated with the instance profile.
      tags:
        - compute
        - instance_profiles
      description: >-
        The only supported field to change is the optional IAM role ARN
        associated with the instance profile.

        It is required to specify the IAM role ARN if both of the following are
        true:


        * Your role name and instance profile name do not match. The name is the
        part after the last slash in

        each ARN. * You want to use the instance profile with [Databricks SQL
        Serverless].


        To understand where these fields are in the AWS console, see [Enable
        serverless SQL warehouses].


        This API is only available to admin users.


        [Databricks SQL Serverless]:
        https://docs.databricks.com/sql/admin/serverless.html

        [Enable serverless SQL warehouses]:
        https://docs.databricks.com/sql/admin/serverless.html


        :param instance_profile_arn: str
          The AWS ARN of the instance profile to register with Databricks. This field is required.
        :param iam_role_arn: str (optional)
          The AWS IAM role ARN of the role associated with the instance profile. This field is required if
          your role name and instance profile name do not match and you want to use the instance profile with
          [Databricks SQL Serverless].

          Otherwise, this field is optional.

          [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html
        :param is_meta_instance_profile: bool (optional)
          Boolean flag indicating whether the instance profile should only be used in credential passthrough
          scenarios. If true, it means the instance profile contains an meta IAM role which could assume a
          wide range of roles. Therefore it should always be used with authorization. This field is optional,
          the default value is `false`.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_profile_arn:
                  type: string
                  description: >-
                    The AWS ARN of the instance profile to register with
                    Databricks. This field is required.
                iam_role_arn:
                  type: string
                  description: >-
                    The AWS IAM role ARN of the role associated with the
                    instance profile. This field is required if your role name
                    and instance profile name do not match and you want to use
                    the instance profile with [Databricks SQL Serverless].
                    Otherwise, this field is optional. [Databricks SQL
                    Serverless]:
                    https://docs.databricks.com/sql/admin/serverless.html
                is_meta_instance_profile:
                  type: string
                  description: >-
                    Boolean flag indicating whether the instance profile should
                    only be used in credential passthrough scenarios. If true,
                    it means the instance profile contains an meta IAM role
                    which could assume a wide range of roles. Therefore it
                    should always be used with authorization. This field is
                    optional, the default value is `false`.
              required:
                - instance_profile_arn
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-profiles/list:
    get:
      operationId: instance_profiles_list
      summary: >-
        List the instance profiles that the calling user can use to launch a
        cluster.
      tags:
        - compute
        - instance_profiles
      description: >-
        List the instance profiles that the calling user can use to launch a
        cluster.


        This API is available to all users.



        :returns: Iterator over :class:`InstanceProfile`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[InstanceProfile]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/instance-profiles/remove:
    post:
      operationId: instance_profiles_remove
      summary: >-
        Remove the instance profile with the provided ARN. Existing clusters
        with this instance profile will
      tags:
        - compute
        - instance_profiles
      description: >-
        Remove the instance profile with the provided ARN. Existing clusters
        with this instance profile will

        continue to function.


        This API is only accessible to admin users.


        :param instance_profile_arn: str
          The ARN of the instance profile to remove. This field is required.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                instance_profile_arn:
                  type: string
                  description: >-
                    The ARN of the instance profile to remove. This field is
                    required.
              required:
                - instance_profile_arn
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/libraries/all-cluster-statuses:
    get:
      operationId: libraries_all_cluster_statuses
      summary: >-
        Get the status of all libraries on all clusters. A status is returned
        for all libraries installed on
      tags:
        - compute
        - libraries
      description: >-
        Get the status of all libraries on all clusters. A status is returned
        for all libraries installed on

        this cluster via the API or the libraries UI.



        :returns: Iterator over :class:`ClusterLibraryStatuses`
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[ClusterLibraryStatuses]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/libraries/cluster-status:
    get:
      operationId: libraries_cluster_status
      summary: >-
        Get the status of libraries on a cluster. A status is returned for all
        libraries installed on this
      tags:
        - compute
        - libraries
      description: >-
        Get the status of libraries on a cluster. A status is returned for all
        libraries installed on this

        cluster via the API or the libraries UI. The order of returned libraries
        is as follows: 1. Libraries

        set to be installed on this cluster, in the order that the libraries
        were added to the cluster, are

        returned first. 2. Libraries that were previously requested to be
        installed on this cluster or, but

        are now marked for removal, in no particular order, are returned last.


        :param cluster_id: str
          Unique identifier of the cluster whose status should be retrieved.

        :returns: Iterator over :class:`LibraryFullStatus`
      parameters:
        - name: cluster_id
          in: query
          required: true
          schema:
            type: string
          description: Unique identifier of the cluster whose status should be retrieved.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[LibraryFullStatus]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/libraries/install:
    post:
      operationId: libraries_install
      summary: >-
        Add libraries to install on a cluster. The installation is asynchronous;
        it happens in the background
      tags:
        - compute
        - libraries
      description: >-
        Add libraries to install on a cluster. The installation is asynchronous;
        it happens in the background

        after the completion of this request.


        :param cluster_id: str
          Unique identifier for the cluster on which to install these libraries.
        :param libraries: List[:class:`Library`]
          The libraries to install.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: >-
                    Unique identifier for the cluster on which to install these
                    libraries.
                libraries:
                  type: string
                  description: The libraries to install.
              required:
                - cluster_id
                - libraries
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/libraries/uninstall:
    post:
      operationId: libraries_uninstall
      summary: >-
        Set libraries to uninstall from a cluster. The libraries won't be
        uninstalled until the cluster is
      tags:
        - compute
        - libraries
      description: >-
        Set libraries to uninstall from a cluster. The libraries won't be
        uninstalled until the cluster is

        restarted. A request to uninstall a library that is not currently
        installed is ignored.


        :param cluster_id: str
          Unique identifier for the cluster on which to uninstall these libraries.
        :param libraries: List[:class:`Library`]
          The libraries to uninstall.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: >-
                    Unique identifier for the cluster on which to uninstall
                    these libraries.
                libraries:
                  type: string
                  description: The libraries to uninstall.
              required:
                - cluster_id
                - libraries
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/enforce-compliance:
    post:
      operationId: policy_compliance_for_clusters_enforce_compliance
      summary: >-
        Updates a cluster to be compliant with the current version of its
        policy. A cluster can be updated if
      tags:
        - compute
        - policy_compliance_for_clusters
      description: >-
        Updates a cluster to be compliant with the current version of its
        policy. A cluster can be updated if

        it is in a `RUNNING` or `TERMINATED` state.


        If a cluster is updated while in a `RUNNING` state, it will be restarted
        so that the new attributes

        can take effect.


        If a cluster is updated while in a `TERMINATED` state, it will remain
        `TERMINATED`. The next time the

        cluster is started, the new attributes will take effect.


        Clusters created by the Databricks Jobs, DLT, or Models services cannot
        be enforced by this API.

        Instead, use the "Enforce job policy compliance" API to enforce policy
        compliance on jobs.


        :param cluster_id: str
          The ID of the cluster you want to enforce policy compliance on.
        :param validate_only: bool (optional)
          If set, previews the changes that would be made to a cluster to enforce compliance but does not
          update the cluster.

        :returns: :class:`EnforceClusterComplianceResponse`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                cluster_id:
                  type: string
                  description: >-
                    The ID of the cluster you want to enforce policy compliance
                    on.
                validate_only:
                  type: string
                  description: >-
                    If set, previews the changes that would be made to a cluster
                    to enforce compliance but does not update the cluster.
              required:
                - cluster_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/EnforceClusterComplianceResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/get-compliance:
    get:
      operationId: policy_compliance_for_clusters_get_compliance
      summary: >-
        Returns the policy compliance status of a cluster. Clusters could be out
        of compliance if their policy
      tags:
        - compute
        - policy_compliance_for_clusters
      description: >-
        Returns the policy compliance status of a cluster. Clusters could be out
        of compliance if their policy

        was updated after the cluster was last edited.


        :param cluster_id: str
          The ID of the cluster to get the compliance status

        :returns: :class:`GetClusterComplianceResponse`
      parameters:
        - name: cluster_id
          in: query
          required: true
          schema:
            type: string
          description: The ID of the cluster to get the compliance status
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetClusterComplianceResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/clusters/list-compliance:
    get:
      operationId: policy_compliance_for_clusters_list_compliance
      summary: >-
        Returns the policy compliance status of all clusters that use a given
        policy. Clusters could be out of
      tags:
        - compute
        - policy_compliance_for_clusters
      description: >-
        Returns the policy compliance status of all clusters that use a given
        policy. Clusters could be out of

        compliance if their policy was updated after the cluster was last
        edited.


        :param policy_id: str
          Canonical unique identifier for the cluster policy.
        :param page_size: int (optional)
          Use this field to specify the maximum number of results to be returned by the server. The server may
          further constrain the maximum number of results returned in a single page.
        :param page_token: str (optional)
          A page token that can be used to navigate to the next page or previous page as returned by
          `next_page_token` or `prev_page_token`.

        :returns: Iterator over :class:`ClusterCompliance`
      parameters:
        - name: policy_id
          in: query
          required: true
          schema:
            type: string
          description: Canonical unique identifier for the cluster policy.
        - name: page_size
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use this field to specify the maximum number of results to be
            returned by the server. The server may further constrain the maximum
            number of results returned in a single page.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            A page token that can be used to navigate to the next page or
            previous page as returned by `next_page_token` or `prev_page_token`.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[ClusterCompliance]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policy-families/{policy_family_id}:
    get:
      operationId: policy_families_get
      summary: >-
        Retrieve the information for an policy family based on its identifier
        and version
      tags:
        - compute
        - policy_families
      description: >-
        Retrieve the information for an policy family based on its identifier
        and version


        :param policy_family_id: str
          The family ID about which to retrieve information.
        :param version: int (optional)
          The version number for the family to fetch. Defaults to the latest version.

        :returns: :class:`PolicyFamily`
      parameters:
        - name: policy_family_id
          in: path
          required: true
          schema:
            type: string
          description: The family ID about which to retrieve information.
        - name: version
          in: query
          required: false
          schema:
            type: string
          description: >-
            The version number for the family to fetch. Defaults to the latest
            version.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PolicyFamily'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policy-families:
    get:
      operationId: policy_families_list
      summary: >-
        Returns the list of policy definition types available to use at their
        latest version. This API is
      tags:
        - compute
        - policy_families
      description: >-
        Returns the list of policy definition types available to use at their
        latest version. This API is

        paginated.


        :param max_results: int (optional)
          Maximum number of policy families to return.
        :param page_token: str (optional)
          A token that can be used to get the next page of results.

        :returns: Iterator over :class:`PolicyFamily`
      parameters:
        - name: max_results
          in: query
          required: false
          schema:
            type: string
          description: Maximum number of policy families to return.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: A token that can be used to get the next page of results.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[PolicyFamily]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
components:
  schemas:
    AddResponse:
      type: object
      properties: {}
    Adlsgen2Info:
      type: object
      properties:
        destination:
          type: string
          description: >-
            abfss destination, e.g.
            `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`.
      required:
        - destination
      description: A storage location in Adls Gen2
    AutoScale:
      type: object
      properties:
        max_workers:
          type: integer
        min_workers:
          type: integer
          description: >-
            The minimum number of workers to which the cluster can scale down
            when underutilized. It is also the initial number of workers the
            cluster will have after creation.
    AwsAttributes:
      type: object
      properties:
        availability:
          $ref: '#/components/schemas/AwsAvailability'
        ebs_volume_count:
          type: integer
          description: >-
            The number of volumes launched for each instance. Users can choose
            up to 10 volumes. This feature is only enabled for supported node
            types. Legacy node types cannot specify custom EBS volumes. For node
            types with no instance store, at least one EBS volume needs to be
            specified; otherwise, cluster creation will fail. These EBS volumes
            will be mounted at `/ebs0`, `/ebs1`, and etc. Instance store volumes
            will be mounted at `/local_disk0`, `/local_disk1`, and etc. If EBS
            volumes are attached, Databricks will configure Spark to use only
            the EBS volumes for scratch storage because heterogenously sized
            scratch devices can lead to inefficient disk utilization. If no EBS
            volumes are attached, Databricks will configure Spark to use
            instance store volumes. Please note that if EBS volumes are
            specified, then the Spark configuration `spark.local.dir` will be
            overridden.
        ebs_volume_iops:
          type: integer
          description: >-
            If using gp3 volumes, what IOPS to use for the disk. If this is not
            set, the maximum performance of a gp2 volume with the same volume
            size will be used.
        ebs_volume_size:
          type: integer
          description: >-
            The size of each EBS volume (in GiB) launched for each instance. For
            general purpose SSD, this value must be within the range 100 - 4096.
            For throughput optimized HDD, this value must be within the range
            500 - 4096.
        ebs_volume_throughput:
          type: integer
          description: >-
            If using gp3 volumes, what throughput to use for the disk. If this
            is not set, the maximum performance of a gp2 volume with the same
            volume size will be used.
        ebs_volume_type:
          $ref: '#/components/schemas/EbsVolumeType'
          description: The type of EBS volumes that will be launched with this cluster.
        first_on_demand:
          type: integer
          description: >-
            The first `first_on_demand` nodes of the cluster will be placed on
            on-demand instances. If this value is greater than 0, the cluster
            driver node in particular will be placed on an on-demand instance.
            If this value is greater than or equal to the current cluster size,
            all nodes will be placed on on-demand instances. If this value is
            less than the current cluster size, `first_on_demand` nodes will be
            placed on on-demand instances and the remainder will be placed on
            `availability` instances. Note that this value does not affect
            cluster size and cannot currently be mutated over the lifetime of a
            cluster.
        instance_profile_arn:
          type: string
          description: >-
            Nodes for this cluster will only be placed on AWS instances with
            this instance profile. If ommitted, nodes will be placed on
            instances without an IAM instance profile. The instance profile must
            have previously been added to the Databricks environment by an
            account administrator. This feature may only be available to certain
            customer plans.
        spot_bid_price_percent:
          type: integer
          description: >-
            The bid price for AWS spot instances, as a percentage of the
            corresponding instance type's on-demand price. For example, if this
            field is set to 50, and the cluster needs a new `r3.xlarge` spot
            instance, then the bid price is half of the price of on-demand
            `r3.xlarge` instances. Similarly, if this field is set to 200, the
            bid price is twice the price of on-demand `r3.xlarge` instances. If
            not specified, the default value is 100. When spot instances are
            requested for this cluster, only spot instances whose bid price
            percentage matches this field will be considered. Note that, for
            safety, we enforce this field to be no more than 10000.
        zone_id:
          type: string
          description: >-
            Identifier for the availability zone/datacenter in which the cluster
            resides. This string will be of a form like "us-west-2a". The
            provided availability zone must be in the same region as the
            Databricks deployment. For example, "us-west-2a" is not a valid zone
            id if the Databricks deployment resides in the "us-east-1" region.
            This is an optional field at cluster creation, and if not specified,
            the zone "auto" will be used. If the zone specified is "auto", will
            try to place cluster in a zone with high availability, and will
            retry placement in a different AZ if there is not enough capacity.
            The list of available zones as well as the default value can be
            found by using the `List Zones` method.
      description: >-
        Attributes set during cluster creation which are related to Amazon Web
        Services.
    AzureAttributes:
      type: object
      properties:
        availability:
          $ref: '#/components/schemas/AzureAvailability'
          description: >-
            Availability type used for all subsequent nodes past the
            `first_on_demand` ones. Note: If `first_on_demand` is zero, this
            availability type will be used for the entire cluster.
        first_on_demand:
          type: integer
          description: >-
            The first `first_on_demand` nodes of the cluster will be placed on
            on-demand instances. This value should be greater than 0, to make
            sure the cluster driver node is placed on an on-demand instance. If
            this value is greater than or equal to the current cluster size, all
            nodes will be placed on on-demand instances. If this value is less
            than the current cluster size, `first_on_demand` nodes will be
            placed on on-demand instances and the remainder will be placed on
            `availability` instances. Note that this value does not affect
            cluster size and cannot currently be mutated over the lifetime of a
            cluster.
        log_analytics_info:
          $ref: '#/components/schemas/LogAnalyticsInfo'
          description: >-
            Defines values necessary to configure and run Azure Log Analytics
            agent
        spot_bid_max_price:
          type: number
          description: >-
            The max bid price to be used for Azure spot instances. The Max price
            for the bid cannot be higher than the on-demand price of the
            instance. If not specified, the default value is -1, which specifies
            that the instance cannot be evicted on the basis of price, and only
            on the basis of availability. Further, the value should > 0 or -1.
      description: >-
        Attributes set during cluster creation which are related to Microsoft
        Azure.
    CancelResponse:
      type: object
      properties: {}
    ChangeClusterOwnerResponse:
      type: object
      properties: {}
    ClientsTypes:
      type: object
      properties:
        jobs:
          type: boolean
        notebooks:
          type: boolean
          description: With notebooks set, this cluster can be used for notebooks
    CloneCluster:
      type: object
      properties:
        source_cluster_id:
          type: string
      required:
        - source_cluster_id
    CloudProviderNodeInfo:
      type: object
      properties:
        status:
          type: array
          items:
            $ref: '#/components/schemas/CloudProviderNodeStatus'
    ClusterAccessControlRequest:
      type: object
      properties:
        group_name:
          type: string
        permission_level:
          $ref: '#/components/schemas/ClusterPermissionLevel'
        service_principal_name:
          type: string
          description: application ID of a service principal
        user_name:
          type: string
          description: name of the user
    ClusterAccessControlResponse:
      type: object
      properties:
        all_permissions:
          type: array
          items:
            $ref: '#/components/schemas/ClusterPermission'
        display_name:
          type: string
          description: Display name of the user or service principal.
        group_name:
          type: string
          description: name of the group
        service_principal_name:
          type: string
          description: Name of the service principal.
        user_name:
          type: string
          description: name of the user
    ClusterAttributes:
      type: object
      properties:
        spark_version:
          type: string
          description: >-
            The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of
            available Spark versions can be retrieved by using the
            :method:clusters/sparkVersions API call.
        autotermination_minutes:
          type: integer
          description: >-
            Automatically terminates the cluster after it is inactive for this
            time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000
            minutes. Users can also set this value to 0 to explicitly disable
            automatic termination.
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
          description: >-
            Attributes related to clusters running on Amazon Web Services. If
            not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          $ref: '#/components/schemas/AzureAttributes'
          description: >-
            Attributes related to clusters running on Microsoft Azure. If not
            specified at cluster creation, a set of default values will be used.
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
          description: >-
            The configuration for delivering spark logs to a long-term storage
            destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for
            one cluster. If the conf is given, the logs will be delivered to the
            destination every `5 mins`. The destination of driver logs is
            `$destination/$clusterId/driver`, while the destination of executor
            logs is `$destination/$clusterId/executor`.
        cluster_name:
          type: string
          description: >-
            Cluster name requested by the user. This doesn't have to be unique.
            If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set
            based on the job and job run IDs.
        custom_tags:
          type: object
          description: >-
            Additional tags for cluster resources. Databricks will tag all
            cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes: - Currently, Databricks
            allows at most 45 custom tags - Clusters can only reuse cloud
            resources if the resources' tags are a subset of the cluster tags
        data_security_mode:
          $ref: '#/components/schemas/DataSecurityMode'
        docker_image:
          $ref: '#/components/schemas/DockerImage'
          description: Custom docker image BYOC
        driver_instance_pool_id:
          type: string
          description: >-
            The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id
            (instance_pool_id) if the driver pool is not assigned.
        driver_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for the driver node.
        driver_node_type_id:
          type: string
          description: >-
            The node type of the Spark driver. Note that this field is optional;
            if unset, the driver node type will be set as the same value as
            `node_type_id` defined above. This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both
            driver_node_type_id, node_type_id, and virtual_cluster_size are
            specified, driver_node_type_id and node_type_id take precedence.
        enable_elastic_disk:
          type: boolean
          description: >-
            Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are
            running low on disk space.
        enable_local_disk_encryption:
          type: boolean
          description: Whether to enable LUKS on cluster VMs' local disks
        gcp_attributes:
          $ref: '#/components/schemas/GcpAttributes'
          description: >-
            Attributes related to clusters running on Google Cloud Platform. If
            not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: array
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          description: >-
            The configuration for storing init scripts. Any number of
            destinations can be specified. The scripts are executed sequentially
            in the order provided. If `cluster_log_conf` is specified, init
            script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool to which the cluster belongs.
        is_single_node:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`. When set
            to true, Databricks will automatically set single node related
            `custom_tags`, `spark_conf`, and `num_workers`
        kind:
          $ref: '#/components/schemas/Kind'
        node_type_id:
          type: string
          description: >-
            This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark
            nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved
            by using the :method:clusters/listNodeTypes API call.
        policy_id:
          type: string
          description: >-
            The ID of the cluster policy used to create the cluster if
            applicable.
        remote_disk_throughput:
          type: integer
          description: >-
            If set, what the configurable throughput (in Mb/s) for the remote
            disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          $ref: '#/components/schemas/RuntimeEngine'
          description: >-
            Determines the cluster's runtime engine, either standard or Photon.
            This field is not compatible with legacy `spark_version` values that
            contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`. If left unspecified, the runtime
            engine defaults to standard unless the spark_version contains
            -photon-, in which case Photon will be used.
        single_user_name:
          type: string
          description: Single user name if data_security_mode is `SINGLE_USER`
        spark_conf:
          type: object
          description: >-
            An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of
            extra JVM options to the driver and the executors via
            `spark.driver.extraJavaOptions` and
            `spark.executor.extraJavaOptions` respectively.
        spark_env_vars:
          type: object
          description: >-
            An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the
            form (X,Y) will be exported as is (i.e., `export X='Y'`) while
            launching the driver and workers. In order to specify an additional
            set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
            `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This
            ensures that all default databricks managed environmental variables
            are included as well. Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS":
            "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS":
            "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`
        ssh_public_keys:
          type: array
          items:
            type: string
          description: >-
            SSH public key contents that will be added to each Spark node in
            this cluster. The corresponding private keys can be used to login
            with the user name `ubuntu` on port `2200`. Up to 10 keys can be
            specified.
        total_initial_remote_disk_size:
          type: integer
          description: >-
            If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            disks.
        use_ml_runtime:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`.
            `effective_spark_version` is determined by `spark_version` (DBR
            release), this field `use_ml_runtime`, and whether `node_type_id` is
            gpu node or not.
        worker_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for worker nodes.
        workload_type:
          $ref: '#/components/schemas/WorkloadType'
      required:
        - spark_version
      description: >-
        Common set of attributes set during cluster creation. These attributes
        cannot be changed over
            the lifetime of a cluster.
    ClusterCompliance:
      type: object
      properties:
        cluster_id:
          type: string
        is_compliant:
          type: boolean
          description: >-
            Whether this cluster is in compliance with the latest version of its
            policy.
        violations:
          type: object
          description: >-
            An object containing key-value mappings representing the first 200
            policy validation errors. The keys indicate the path where the
            policy validation error is occurring. The values indicate an error
            message describing the policy validation error.
      required:
        - cluster_id
    ClusterDetails:
      type: object
      properties:
        autoscale:
          $ref: '#/components/schemas/AutoScale'
          description: >-
            Parameters needed in order to automatically scale clusters up and
            down based on load. Note: autoscaling works best with DB runtime
            versions 3.0 or later.
        autotermination_minutes:
          type: integer
          description: >-
            Automatically terminates the cluster after it is inactive for this
            time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000
            minutes. Users can also set this value to 0 to explicitly disable
            automatic termination.
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
          description: >-
            Attributes related to clusters running on Amazon Web Services. If
            not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          $ref: '#/components/schemas/AzureAttributes'
          description: >-
            Attributes related to clusters running on Microsoft Azure. If not
            specified at cluster creation, a set of default values will be used.
        cluster_cores:
          type: number
          description: >-
            Number of CPU cores available for this cluster. Note that this can
            be fractional, e.g. 7.5 cores, since certain node types are
            configured to share cores between Spark nodes on the same instance.
        cluster_id:
          type: string
          description: >-
            Canonical identifier for the cluster. This id is retained during
            cluster restarts and resizes, while each new cluster has a globally
            unique id.
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
          description: >-
            The configuration for delivering spark logs to a long-term storage
            destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for
            one cluster. If the conf is given, the logs will be delivered to the
            destination every `5 mins`. The destination of driver logs is
            `$destination/$clusterId/driver`, while the destination of executor
            logs is `$destination/$clusterId/executor`.
        cluster_log_status:
          $ref: '#/components/schemas/LogSyncStatus'
          description: Cluster log delivery status.
        cluster_memory_mb:
          type: integer
          description: Total amount of cluster memory, in megabytes
        cluster_name:
          type: string
          description: >-
            Cluster name requested by the user. This doesn't have to be unique.
            If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set
            based on the job and job run IDs.
        cluster_source:
          $ref: '#/components/schemas/ClusterSource'
          description: >-
            Determines whether the cluster was created by a user through the UI,
            created by the Databricks Jobs Scheduler, or through an API request.
        creator_user_name:
          type: string
          description: >-
            Creator user name. The field won't be included in the response if
            the user has already been deleted.
        custom_tags:
          type: object
          description: >-
            Additional tags for cluster resources. Databricks will tag all
            cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes: - Currently, Databricks
            allows at most 45 custom tags - Clusters can only reuse cloud
            resources if the resources' tags are a subset of the cluster tags
        data_security_mode:
          $ref: '#/components/schemas/DataSecurityMode'
        default_tags:
          type: object
          description: >-
            Tags that are added by Databricks regardless of any `custom_tags`,
            including: - Vendor: Databricks - Creator: <username_of_creator> -
            ClusterName: <name_of_cluster> - ClusterId: <id_of_cluster> - Name:
            <Databricks internal use>
        docker_image:
          $ref: '#/components/schemas/DockerImage'
          description: Custom docker image BYOC
        driver:
          $ref: '#/components/schemas/SparkNode'
          description: >-
            Node on which the Spark driver resides. The driver node contains the
            Spark master and the Databricks application that manages the
            per-notebook Spark REPLs.
        driver_instance_pool_id:
          type: string
          description: >-
            The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id
            (instance_pool_id) if the driver pool is not assigned.
        driver_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for the driver node.
        driver_node_type_id:
          type: string
          description: >-
            The node type of the Spark driver. Note that this field is optional;
            if unset, the driver node type will be set as the same value as
            `node_type_id` defined above. This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both
            driver_node_type_id, node_type_id, and virtual_cluster_size are
            specified, driver_node_type_id and node_type_id take precedence.
        enable_elastic_disk:
          type: boolean
          description: >-
            Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are
            running low on disk space.
        enable_local_disk_encryption:
          type: boolean
          description: Whether to enable LUKS on cluster VMs' local disks
        executors:
          type: array
          items:
            $ref: '#/components/schemas/SparkNode'
          description: Nodes on which the Spark executors reside.
        gcp_attributes:
          $ref: '#/components/schemas/GcpAttributes'
          description: >-
            Attributes related to clusters running on Google Cloud Platform. If
            not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: array
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          description: >-
            The configuration for storing init scripts. Any number of
            destinations can be specified. The scripts are executed sequentially
            in the order provided. If `cluster_log_conf` is specified, init
            script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool to which the cluster belongs.
        is_single_node:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`. When set
            to true, Databricks will automatically set single node related
            `custom_tags`, `spark_conf`, and `num_workers`
        jdbc_port:
          type: integer
          description: >-
            Port on which Spark JDBC server is listening, in the driver nod. No
            service will be listeningon on this port in executor nodes.
        kind:
          $ref: '#/components/schemas/Kind'
        last_restarted_time:
          type: integer
          description: the timestamp that the cluster was started/restarted
        last_state_loss_time:
          type: integer
          description: >-
            Time when the cluster driver last lost its state (due to a restart
            or driver failure).
        node_type_id:
          type: string
          description: >-
            This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark
            nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved
            by using the :method:clusters/listNodeTypes API call.
        num_workers:
          type: integer
          description: >-
            Number of worker nodes that this cluster should have. A cluster has
            one Spark Driver and `num_workers` Executors for a total of
            `num_workers` + 1 Spark nodes. Note: When reading the properties of
            a cluster, this field reflects the desired number of workers rather
            than the actual current number of workers. For instance, if a
            cluster is resized from 5 to 10 workers, this field will immediately
            be updated to reflect the target size of 10 workers, whereas the
            workers listed in `spark_info` will gradually increase from 5 to 10
            as the new nodes are provisioned.
        policy_id:
          type: string
          description: >-
            The ID of the cluster policy used to create the cluster if
            applicable.
        remote_disk_throughput:
          type: integer
          description: >-
            If set, what the configurable throughput (in Mb/s) for the remote
            disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          $ref: '#/components/schemas/RuntimeEngine'
          description: >-
            Determines the cluster's runtime engine, either standard or Photon.
            This field is not compatible with legacy `spark_version` values that
            contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`. If left unspecified, the runtime
            engine defaults to standard unless the spark_version contains
            -photon-, in which case Photon will be used.
        single_user_name:
          type: string
          description: Single user name if data_security_mode is `SINGLE_USER`
        spark_conf:
          type: object
          description: >-
            An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of
            extra JVM options to the driver and the executors via
            `spark.driver.extraJavaOptions` and
            `spark.executor.extraJavaOptions` respectively.
        spark_context_id:
          type: integer
          description: >-
            A canonical SparkContext identifier. This value *does* change when
            the Spark driver restarts. The pair `(cluster_id, spark_context_id)`
            is a globally unique identifier over all Spark contexts.
        spark_env_vars:
          type: object
          description: >-
            An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the
            form (X,Y) will be exported as is (i.e., `export X='Y'`) while
            launching the driver and workers. In order to specify an additional
            set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
            `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This
            ensures that all default databricks managed environmental variables
            are included as well. Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS":
            "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS":
            "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`
        spark_version:
          type: string
          description: >-
            The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of
            available Spark versions can be retrieved by using the
            :method:clusters/sparkVersions API call.
        spec:
          $ref: '#/components/schemas/ClusterSpec'
          description: >-
            The spec contains a snapshot of the latest user specified settings
            that were used to create/edit the cluster. Note: not included in the
            response of the ListClusters API.
        ssh_public_keys:
          type: array
          items:
            type: string
          description: >-
            SSH public key contents that will be added to each Spark node in
            this cluster. The corresponding private keys can be used to login
            with the user name `ubuntu` on port `2200`. Up to 10 keys can be
            specified.
        start_time:
          type: integer
          description: >-
            Time (in epoch milliseconds) when the cluster creation request was
            received (when the cluster entered a `PENDING` state).
        state:
          $ref: '#/components/schemas/State'
          description: Current state of the cluster.
        state_message:
          type: string
          description: >-
            A message associated with the most recent state transition (e.g.,
            the reason why the cluster entered a `TERMINATED` state).
        terminated_time:
          type: integer
          description: >-
            Time (in epoch milliseconds) when the cluster was terminated, if
            applicable.
        termination_reason:
          $ref: '#/components/schemas/TerminationReason'
          description: >-
            Information about why the cluster was terminated. This field only
            appears when the cluster is in a `TERMINATING` or `TERMINATED`
            state.
        total_initial_remote_disk_size:
          type: integer
          description: >-
            If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            disks.
        use_ml_runtime:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`.
            `effective_spark_version` is determined by `spark_version` (DBR
            release), this field `use_ml_runtime`, and whether `node_type_id` is
            gpu node or not.
        worker_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for worker nodes.
        workload_type:
          $ref: '#/components/schemas/WorkloadType'
      description: >-
        Describes all of the metadata about a single Spark cluster in
        Databricks.
    ClusterEvent:
      type: object
      properties:
        cluster_id:
          type: string
        data_plane_event_details:
          $ref: '#/components/schemas/DataPlaneEventDetails'
        details:
          $ref: '#/components/schemas/EventDetails'
        timestamp:
          type: integer
          description: >-
            The timestamp when the event occurred, stored as the number of
            milliseconds since the Unix epoch. If not provided, this will be
            assigned by the Timeline service.
        type:
          $ref: '#/components/schemas/EventType'
      required:
        - cluster_id
    ClusterLibraryStatuses:
      type: object
      properties:
        cluster_id:
          type: string
        library_statuses:
          type: array
          items:
            $ref: '#/components/schemas/LibraryFullStatus'
          description: Status of all libraries on the cluster.
    ClusterLogConf:
      type: object
      properties:
        dbfs:
          $ref: '#/components/schemas/DbfsStorageInfo'
          description: >-
            destination needs to be provided. e.g. `{ "dbfs" : { "destination" :
            "dbfs:/home/cluster_log" } }`
        s3:
          $ref: '#/components/schemas/S3StorageInfo'
          description: >-
            destination and either the region or endpoint need to be provided.
            e.g. `{ "s3": { "destination" : "s3://cluster_log_bucket/prefix",
            "region" : "us-west-2" } }` Cluster iam role is used to access s3,
            please make sure the cluster iam role in `instance_profile_arn` has
            permission to write data to the s3 destination.
        volumes:
          $ref: '#/components/schemas/VolumesStorageInfo'
          description: >-
            destination needs to be provided, e.g. `{ "volumes": {
            "destination": "/Volumes/catalog/schema/volume/cluster_log" } }`
      description: Cluster log delivery config
    ClusterPermission:
      type: object
      properties:
        inherited:
          type: boolean
        inherited_from_object:
          type: array
          items:
            type: string
        permission_level:
          $ref: '#/components/schemas/ClusterPermissionLevel'
    ClusterPermissions:
      type: object
      properties:
        access_control_list:
          type: array
          items:
            $ref: '#/components/schemas/ClusterAccessControlResponse'
        object_id:
          type: string
        object_type:
          type: string
    ClusterPermissionsDescription:
      type: object
      properties:
        description:
          type: string
        permission_level:
          $ref: '#/components/schemas/ClusterPermissionLevel'
    ClusterPolicyAccessControlRequest:
      type: object
      properties:
        group_name:
          type: string
        permission_level:
          $ref: '#/components/schemas/ClusterPolicyPermissionLevel'
        service_principal_name:
          type: string
          description: application ID of a service principal
        user_name:
          type: string
          description: name of the user
    ClusterPolicyAccessControlResponse:
      type: object
      properties:
        all_permissions:
          type: array
          items:
            $ref: '#/components/schemas/ClusterPolicyPermission'
        display_name:
          type: string
          description: Display name of the user or service principal.
        group_name:
          type: string
          description: name of the group
        service_principal_name:
          type: string
          description: Name of the service principal.
        user_name:
          type: string
          description: name of the user
    ClusterPolicyPermission:
      type: object
      properties:
        inherited:
          type: boolean
        inherited_from_object:
          type: array
          items:
            type: string
        permission_level:
          $ref: '#/components/schemas/ClusterPolicyPermissionLevel'
    ClusterPolicyPermissions:
      type: object
      properties:
        access_control_list:
          type: array
          items:
            $ref: '#/components/schemas/ClusterPolicyAccessControlResponse'
        object_id:
          type: string
        object_type:
          type: string
    ClusterPolicyPermissionsDescription:
      type: object
      properties:
        description:
          type: string
        permission_level:
          $ref: '#/components/schemas/ClusterPolicyPermissionLevel'
    ClusterSettingsChange:
      type: object
      properties:
        field:
          type: string
          description: The field where this change would be made.
        new_value:
          type: string
          description: >-
            The new value of this field after enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string.
            This is intended to be read by a human. The typed new value of this
            field can be retrieved by reading the settings field in the API
            response.
        previous_value:
          type: string
          description: >-
            The previous value of this field before enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string.
            This is intended to be read by a human. The type of the field can be
            retrieved by reading the settings field in the API response.
      description: >-
        Represents a change to the cluster settings required for the cluster to
        become compliant with
            its policy.
    ClusterSize:
      type: object
      properties:
        autoscale:
          $ref: '#/components/schemas/AutoScale'
        num_workers:
          type: integer
          description: >-
            Number of worker nodes that this cluster should have. A cluster has
            one Spark Driver and `num_workers` Executors for a total of
            `num_workers` + 1 Spark nodes. Note: When reading the properties of
            a cluster, this field reflects the desired number of workers rather
            than the actual current number of workers. For instance, if a
            cluster is resized from 5 to 10 workers, this field will immediately
            be updated to reflect the target size of 10 workers, whereas the
            workers listed in `spark_info` will gradually increase from 5 to 10
            as the new nodes are provisioned.
    ClusterSpec:
      type: object
      properties:
        apply_policy_default_values:
          type: boolean
          description: >-
            When set to true, fixed and default values from the policy will be
            used for fields that are omitted. When set to false, only fixed
            values from the policy will be applied.
        autoscale:
          $ref: '#/components/schemas/AutoScale'
          description: >-
            Parameters needed in order to automatically scale clusters up and
            down based on load. Note: autoscaling works best with DB runtime
            versions 3.0 or later.
        autotermination_minutes:
          type: integer
          description: >-
            Automatically terminates the cluster after it is inactive for this
            time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000
            minutes. Users can also set this value to 0 to explicitly disable
            automatic termination.
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
          description: >-
            Attributes related to clusters running on Amazon Web Services. If
            not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          $ref: '#/components/schemas/AzureAttributes'
          description: >-
            Attributes related to clusters running on Microsoft Azure. If not
            specified at cluster creation, a set of default values will be used.
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
          description: >-
            The configuration for delivering spark logs to a long-term storage
            destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for
            one cluster. If the conf is given, the logs will be delivered to the
            destination every `5 mins`. The destination of driver logs is
            `$destination/$clusterId/driver`, while the destination of executor
            logs is `$destination/$clusterId/executor`.
        cluster_name:
          type: string
          description: >-
            Cluster name requested by the user. This doesn't have to be unique.
            If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set
            based on the job and job run IDs.
        custom_tags:
          type: object
          description: >-
            Additional tags for cluster resources. Databricks will tag all
            cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes: - Currently, Databricks
            allows at most 45 custom tags - Clusters can only reuse cloud
            resources if the resources' tags are a subset of the cluster tags
        data_security_mode:
          $ref: '#/components/schemas/DataSecurityMode'
        docker_image:
          $ref: '#/components/schemas/DockerImage'
          description: Custom docker image BYOC
        driver_instance_pool_id:
          type: string
          description: >-
            The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id
            (instance_pool_id) if the driver pool is not assigned.
        driver_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for the driver node.
        driver_node_type_id:
          type: string
          description: >-
            The node type of the Spark driver. Note that this field is optional;
            if unset, the driver node type will be set as the same value as
            `node_type_id` defined above. This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both
            driver_node_type_id, node_type_id, and virtual_cluster_size are
            specified, driver_node_type_id and node_type_id take precedence.
        enable_elastic_disk:
          type: boolean
          description: >-
            Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are
            running low on disk space.
        enable_local_disk_encryption:
          type: boolean
          description: Whether to enable LUKS on cluster VMs' local disks
        gcp_attributes:
          $ref: '#/components/schemas/GcpAttributes'
          description: >-
            Attributes related to clusters running on Google Cloud Platform. If
            not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: array
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          description: >-
            The configuration for storing init scripts. Any number of
            destinations can be specified. The scripts are executed sequentially
            in the order provided. If `cluster_log_conf` is specified, init
            script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool to which the cluster belongs.
        is_single_node:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`. When set
            to true, Databricks will automatically set single node related
            `custom_tags`, `spark_conf`, and `num_workers`
        kind:
          $ref: '#/components/schemas/Kind'
        node_type_id:
          type: string
          description: >-
            This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark
            nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved
            by using the :method:clusters/listNodeTypes API call.
        num_workers:
          type: integer
          description: >-
            Number of worker nodes that this cluster should have. A cluster has
            one Spark Driver and `num_workers` Executors for a total of
            `num_workers` + 1 Spark nodes. Note: When reading the properties of
            a cluster, this field reflects the desired number of workers rather
            than the actual current number of workers. For instance, if a
            cluster is resized from 5 to 10 workers, this field will immediately
            be updated to reflect the target size of 10 workers, whereas the
            workers listed in `spark_info` will gradually increase from 5 to 10
            as the new nodes are provisioned.
        policy_id:
          type: string
          description: >-
            The ID of the cluster policy used to create the cluster if
            applicable.
        remote_disk_throughput:
          type: integer
          description: >-
            If set, what the configurable throughput (in Mb/s) for the remote
            disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          $ref: '#/components/schemas/RuntimeEngine'
          description: >-
            Determines the cluster's runtime engine, either standard or Photon.
            This field is not compatible with legacy `spark_version` values that
            contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`. If left unspecified, the runtime
            engine defaults to standard unless the spark_version contains
            -photon-, in which case Photon will be used.
        single_user_name:
          type: string
          description: Single user name if data_security_mode is `SINGLE_USER`
        spark_conf:
          type: object
          description: >-
            An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of
            extra JVM options to the driver and the executors via
            `spark.driver.extraJavaOptions` and
            `spark.executor.extraJavaOptions` respectively.
        spark_env_vars:
          type: object
          description: >-
            An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the
            form (X,Y) will be exported as is (i.e., `export X='Y'`) while
            launching the driver and workers. In order to specify an additional
            set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
            `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This
            ensures that all default databricks managed environmental variables
            are included as well. Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS":
            "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS":
            "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`
        spark_version:
          type: string
          description: >-
            The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of
            available Spark versions can be retrieved by using the
            :method:clusters/sparkVersions API call.
        ssh_public_keys:
          type: array
          items:
            type: string
          description: >-
            SSH public key contents that will be added to each Spark node in
            this cluster. The corresponding private keys can be used to login
            with the user name `ubuntu` on port `2200`. Up to 10 keys can be
            specified.
        total_initial_remote_disk_size:
          type: integer
          description: >-
            If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            disks.
        use_ml_runtime:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`.
            `effective_spark_version` is determined by `spark_version` (DBR
            release), this field `use_ml_runtime`, and whether `node_type_id` is
            gpu node or not.
        worker_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for worker nodes.
        workload_type:
          $ref: '#/components/schemas/WorkloadType'
      description: >-
        Contains a snapshot of the latest user specified settings that were used
        to create/edit the
            cluster.
    CommandStatusResponse:
      type: object
      properties:
        id:
          type: string
        results:
          $ref: '#/components/schemas/Results'
        status:
          $ref: '#/components/schemas/CommandStatus'
    ContextStatusResponse:
      type: object
      properties:
        id:
          type: string
        status:
          $ref: '#/components/schemas/ContextStatus'
    CreateClusterResponse:
      type: object
      properties:
        cluster_id:
          type: string
    CreateInstancePoolResponse:
      type: object
      properties:
        instance_pool_id:
          type: string
    CreatePolicyResponse:
      type: object
      properties:
        policy_id:
          type: string
    CreateResponse:
      type: object
      properties:
        script_id:
          type: string
    Created:
      type: object
      properties:
        id:
          type: string
    CustomPolicyTag:
      type: object
      properties:
        key:
          type: string
        value:
          type: string
          description: The value of the tag.
      required:
        - key
    DataPlaneEventDetails:
      type: object
      properties:
        event_type:
          $ref: '#/components/schemas/DataPlaneEventDetailsEventType'
        executor_failures:
          type: integer
        host_id:
          type: string
        timestamp:
          type: integer
    DbfsStorageInfo:
      type: object
      properties:
        destination:
          type: string
          description: dbfs destination, e.g. `dbfs:/my/path`
      required:
        - destination
      description: A storage location in DBFS
    DeleteClusterResponse:
      type: object
      properties: {}
    DeleteInstancePoolResponse:
      type: object
      properties: {}
    DeletePolicyResponse:
      type: object
      properties: {}
    DeleteResponse:
      type: object
      properties: {}
    DestroyResponse:
      type: object
      properties: {}
    DiskSpec:
      type: object
      properties:
        disk_count:
          type: integer
          description: >-
            The number of disks launched for each instance: - This feature is
            only enabled for supported node types. - Users can choose up to the
            limit of the disks supported by the node type. - For node types with
            no OS disk, at least one disk must be specified; otherwise, cluster
            creation will fail. If disks are attached, Databricks will configure
            Spark to use only the disks for scratch storage, because
            heterogenously sized scratch devices can lead to inefficient disk
            utilization. If no disks are attached, Databricks will configure
            Spark to use instance store disks. Note: If disks are specified,
            then the Spark configuration `spark.local.dir` will be overridden.
            Disks will be mounted at: - For AWS: `/ebs0`, `/ebs1`, and etc. -
            For Azure: `/remote_volume0`, `/remote_volume1`, and etc.
        disk_iops:
          type: integer
        disk_size:
          type: integer
          description: >-
            The size of each disk (in GiB) launched for each instance. Values
            must fall into the supported range for a particular instance type.
            For AWS: - General Purpose SSD: 100 - 4096 GiB - Throughput
            Optimized HDD: 500 - 4096 GiB For Azure: - Premium LRS (SSD): 1 -
            1023 GiB - Standard LRS (HDD): 1- 1023 GiB
        disk_throughput:
          type: integer
        disk_type:
          $ref: '#/components/schemas/DiskType'
          description: The type of disks that will be launched with this cluster.
      description: >-
        Describes the disks that are launched for each instance in the spark
        cluster. For example, if
            the cluster has 3 instances, each instance is configured to launch 2 disks, 100 GiB each, then
            Databricks will launch a total of 6 disks, 100 GiB each, for this cluster.
    DiskType:
      type: object
      properties:
        azure_disk_volume_type:
          $ref: '#/components/schemas/DiskTypeAzureDiskVolumeType'
        ebs_volume_type:
          $ref: '#/components/schemas/DiskTypeEbsVolumeType'
      description: Describes the disk type.
    DockerBasicAuth:
      type: object
      properties:
        password:
          type: string
        username:
          type: string
          description: Name of the user
    DockerImage:
      type: object
      properties:
        basic_auth:
          $ref: '#/components/schemas/DockerBasicAuth'
        url:
          type: string
          description: URL of the docker image.
    EditClusterResponse:
      type: object
      properties: {}
    EditInstancePoolResponse:
      type: object
      properties: {}
    EditPolicyResponse:
      type: object
      properties: {}
    EditResponse:
      type: object
      properties: {}
    EnforceClusterComplianceResponse:
      type: object
      properties:
        changes:
          type: array
          items:
            $ref: '#/components/schemas/ClusterSettingsChange'
        has_changes:
          type: boolean
          description: >-
            Whether any changes have been made to the cluster settings for the
            cluster to become compliant with its policy.
    Environment:
      type: object
      properties:
        base_environment:
          type: string
          description: >-
            The `base_environment` key refers to an `env.yaml` file that
            specifies an environment version and a collection of dependencies
            required for the environment setup. This `env.yaml` file may itself
            include a `base_environment` reference pointing to another
            `env_1.yaml` file. However, when used as a base environment,
            `env_1.yaml` (or further nested references) will not be processed or
            included in the final environment, meaning that the resolution of
            `base_environment` references is not recursive.
        client:
          type: string
          description: Use `environment_version` instead.
        dependencies:
          type: array
          items:
            type: string
          description: >-
            List of pip dependencies, as supported by the version of pip in this
            environment. Each dependency is a valid pip requirements file line
            per
            https://pip.pypa.io/en/stable/reference/requirements-file-format/.
            Allowed dependencies include a requirement specifier, an archive
            URL, a local project path (such as WSFS or UC Volumes in
            Databricks), or a VCS project URL.
        environment_version:
          type: string
          description: >-
            Required. Environment version used by the environment. Each version
            comes with a specific Python version and a set of Python packages.
            The version is a string, consisting of an integer.
        java_dependencies:
          type: array
          items:
            type: string
          description: >-
            List of java dependencies. Each dependency is a string representing
            a java library path. For example: `/Volumes/path/to/test.jar`.
      description: >-
        The environment entity used to preserve serverless environment side
        panel, jobs' environment for
            non-notebook task, and DLT's environment for classic and serverless pipelines. In this minimal
            environment spec, only pip dependencies are supported.
    EventDetails:
      type: object
      properties:
        attributes:
          $ref: '#/components/schemas/ClusterAttributes'
        cause:
          $ref: '#/components/schemas/EventDetailsCause'
          description: The cause of a change in target size.
        cluster_size:
          $ref: '#/components/schemas/ClusterSize'
          description: >-
            The actual cluster size that was set in the cluster creation or
            edit.
        current_num_vcpus:
          type: integer
          description: The current number of vCPUs in the cluster.
        current_num_workers:
          type: integer
          description: The current number of nodes in the cluster.
        did_not_expand_reason:
          type: string
        disk_size:
          type: integer
          description: Current disk size in bytes
        driver_state_message:
          type: string
          description: More details about the change in driver's state
        enable_termination_for_node_blocklisted:
          type: boolean
          description: >-
            Whether or not a blocklisted node should be terminated. For
            ClusterEventType NODE_BLACKLISTED.
        free_space:
          type: integer
        init_scripts:
          $ref: '#/components/schemas/InitScriptEventDetails'
          description: >-
            List of global and cluster init scripts associated with this cluster
            event.
        instance_id:
          type: string
          description: Instance Id where the event originated from
        job_run_name:
          type: string
          description: >-
            Unique identifier of the specific job run associated with this
            cluster event * For clusters created for jobs, this will be the same
            as the cluster name
        previous_attributes:
          $ref: '#/components/schemas/ClusterAttributes'
          description: The cluster attributes before a cluster was edited.
        previous_cluster_size:
          $ref: '#/components/schemas/ClusterSize'
          description: The size of the cluster before an edit or resize.
        previous_disk_size:
          type: integer
          description: Previous disk size in bytes
        reason:
          $ref: '#/components/schemas/TerminationReason'
          description: >-
            A termination reason: * On a TERMINATED event, this is the reason of
            the termination. * On a RESIZE_COMPLETE event, this indicates the
            reason that we failed to acquire some nodes.
        target_num_vcpus:
          type: integer
          description: The targeted number of vCPUs in the cluster.
        target_num_workers:
          type: integer
          description: The targeted number of nodes in the cluster.
        user:
          type: string
          description: >-
            The user that caused the event to occur. (Empty if it was done by
            the control plane.)
    GcpAttributes:
      type: object
      properties:
        availability:
          $ref: '#/components/schemas/GcpAvailability'
          description: >-
            This field determines whether the spark executors will be scheduled
            to run on preemptible VMs, on-demand VMs, or preemptible VMs with a
            fallback to on-demand VMs if the former is unavailable.
        boot_disk_size:
          type: integer
          description: Boot disk size in GB
        first_on_demand:
          type: integer
          description: >-
            The first `first_on_demand` nodes of the cluster will be placed on
            on-demand instances. This value should be greater than 0, to make
            sure the cluster driver node is placed on an on-demand instance. If
            this value is greater than or equal to the current cluster size, all
            nodes will be placed on on-demand instances. If this value is less
            than the current cluster size, `first_on_demand` nodes will be
            placed on on-demand instances and the remainder will be placed on
            `availability` instances. Note that this value does not affect
            cluster size and cannot currently be mutated over the lifetime of a
            cluster.
        google_service_account:
          type: string
          description: >-
            If provided, the cluster will impersonate the google service account
            when accessing gcloud services (like GCS). The google service
            account must have previously been added to the Databricks
            environment by an account administrator.
        local_ssd_count:
          type: integer
          description: >-
            If provided, each node (workers and driver) in the cluster will have
            this number of local SSDs attached. Each local SSD is 375GB in size.
            Refer to [GCP documentation] for the supported number of local SSDs
            for each instance type. [GCP documentation]:
            https://cloud.google.com/compute/docs/disks/local-ssd#choose_number_local_ssds
        use_preemptible_executors:
          type: boolean
          description: >-
            This field determines whether the spark executors will be scheduled
            to run on preemptible VMs (when set to true) versus standard compute
            engine VMs (when set to false; default). Note: Soon to be
            deprecated, use the 'availability' field instead.
        zone_id:
          type: string
          description: >-
            Identifier for the availability zone in which the cluster resides.
            This can be one of the following: - "HA" => High availability,
            spread nodes across availability zones for a Databricks deployment
            region [default]. - "AUTO" => Databricks picks an availability zone
            to schedule the cluster on. - A GCP availability zone => Pick One of
            the available zones for (machine type + region) from
            https://cloud.google.com/compute/docs/regions-zones.
      description: Attributes set during cluster creation which are related to GCP.
    GcsStorageInfo:
      type: object
      properties:
        destination:
          type: string
          description: GCS destination/URI, e.g. `gs://my-bucket/some-prefix`
      required:
        - destination
      description: A storage location in Google Cloud Platform's GCS
    GetClusterComplianceResponse:
      type: object
      properties:
        is_compliant:
          type: boolean
        violations:
          type: object
          description: >-
            An object containing key-value mappings representing the first 200
            policy validation errors. The keys indicate the path where the
            policy validation error is occurring. The values indicate an error
            message describing the policy validation error.
    GetClusterPermissionLevelsResponse:
      type: object
      properties:
        permission_levels:
          type: array
          items:
            $ref: '#/components/schemas/ClusterPermissionsDescription'
    GetClusterPolicyPermissionLevelsResponse:
      type: object
      properties:
        permission_levels:
          type: array
          items:
            $ref: '#/components/schemas/ClusterPolicyPermissionsDescription'
    GetEvents:
      type: object
      properties:
        cluster_id:
          type: string
        end_time:
          type: integer
          description: >-
            The end time in epoch milliseconds. If empty, returns events up to
            the current time.
        event_types:
          type: array
          items:
            $ref: '#/components/schemas/EventType'
          description: >-
            An optional set of event types to filter on. If empty, all event
            types are returned.
        limit:
          type: integer
          description: >-
            Deprecated: use page_token in combination with page_size instead.
            The maximum number of events to include in a page of events.
            Defaults to 50, and maximum allowed value is 500.
        offset:
          type: integer
          description: >-
            Deprecated: use page_token in combination with page_size instead.
            The offset in the result set. Defaults to 0 (no offset). When an
            offset is specified and the results are requested in descending
            order, the end_time field is required.
        order:
          $ref: '#/components/schemas/GetEventsOrder'
          description: >-
            The order to list events in; either "ASC" or "DESC". Defaults to
            "DESC".
        page_size:
          type: integer
          description: >-
            The maximum number of events to include in a page of events. The
            server may further constrain the maximum number of results returned
            in a single page. If the page_size is empty or 0, the server will
            decide the number of results to be returned. The field has to be in
            the range [0,500]. If the value is outside the range, the server
            enforces 0 or 500.
        page_token:
          type: string
          description: >-
            Use next_page_token or prev_page_token returned from the previous
            request to list the next or previous page of events respectively. If
            page_token is empty, the first page is returned.
        start_time:
          type: integer
          description: >-
            The start time in epoch milliseconds. If empty, returns events
            starting from the beginning of time.
      required:
        - cluster_id
    GetEventsResponse:
      type: object
      properties:
        events:
          type: array
          items:
            $ref: '#/components/schemas/ClusterEvent'
        next_page:
          $ref: '#/components/schemas/GetEvents'
          description: >-
            Deprecated: use next_page_token or prev_page_token instead. The
            parameters required to retrieve the next page of events. Omitted if
            there are no more events to read.
        next_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the next page
            of results. If the value is "", it means no further results for the
            request.
        prev_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the previous
            page of results. If the value is "", it means no further results for
            the request.
        total_count:
          type: integer
          description: >-
            Deprecated: Returns 0 when request uses page_token. Will start
            returning zero when request uses offset/limit soon. The total number
            of events filtered by the start_time, end_time, and event_types.
    GetInstancePool:
      type: object
      properties:
        instance_pool_id:
          type: string
        aws_attributes:
          $ref: '#/components/schemas/InstancePoolAwsAttributes'
          description: >-
            Attributes related to instance pools running on Amazon Web Services.
            If not specified at pool creation, a set of default values will be
            used.
        azure_attributes:
          $ref: '#/components/schemas/InstancePoolAzureAttributes'
          description: >-
            Attributes related to instance pools running on Azure. If not
            specified at pool creation, a set of default values will be used.
        custom_tags:
          type: object
          description: >-
            Additional tags for pool resources. Databricks will tag all pool
            resources (e.g., AWS instances and EBS volumes) with these tags in
            addition to `default_tags`. Notes: - Currently, Databricks allows at
            most 45 custom tags
        default_tags:
          type: object
          description: >-
            Tags that are added by Databricks regardless of any ``custom_tags``,
            including: - Vendor: Databricks - InstancePoolCreator:
            <user_id_of_creator> - InstancePoolName: <name_of_pool> -
            InstancePoolId: <id_of_pool>
        disk_spec:
          $ref: '#/components/schemas/DiskSpec'
          description: >-
            Defines the specification of the disks that will be attached to all
            spark containers.
        enable_elastic_disk:
          type: boolean
          description: >-
            Autoscaling Local Storage: when enabled, this instances in this pool
            will dynamically acquire additional disk space when its Spark
            workers are running low on disk space. In AWS, this feature requires
            specific AWS permissions to function correctly - refer to the User
            Guide for more details.
        gcp_attributes:
          $ref: '#/components/schemas/InstancePoolGcpAttributes'
          description: >-
            Attributes related to instance pools running on Google Cloud
            Platform. If not specified at pool creation, a set of default values
            will be used.
        idle_instance_autotermination_minutes:
          type: integer
          description: >-
            Automatically terminates the extra instances in the pool cache after
            they are inactive for this time in minutes if min_idle_instances
            requirement is already met. If not set, the extra pool instances
            will be automatically terminated after a default timeout. If
            specified, the threshold must be between 0 and 10000 minutes. Users
            can also set this value to 0 to instantly remove idle instances from
            the cache if min cache size could still hold.
        instance_pool_name:
          type: string
          description: >-
            Pool name requested by the user. Pool name must be unique. Length
            must be between 1 and 100 characters.
        max_capacity:
          type: integer
          description: >-
            Maximum number of outstanding instances to keep in the pool,
            including both instances used by clusters and idle instances.
            Clusters that require further instance provisioning will fail during
            upsize requests.
        min_idle_instances:
          type: integer
          description: Minimum number of idle instances to keep in the instance pool
        node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for the pool.
        node_type_id:
          type: string
          description: >-
            This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark
            nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved
            by using the :method:clusters/listNodeTypes API call.
        preloaded_docker_images:
          type: array
          items:
            $ref: '#/components/schemas/DockerImage'
          description: Custom Docker Image BYOC
        preloaded_spark_versions:
          type: array
          items:
            type: string
          description: >-
            A list containing at most one preloaded Spark image version for the
            pool. Pool-backed clusters started with the preloaded Spark version
            will start faster. A list of available Spark versions can be
            retrieved by using the :method:clusters/sparkVersions API call.
        remote_disk_throughput:
          type: integer
          description: >-
            If set, what the configurable throughput (in Mb/s) for the remote
            disk is. Currently only supported for GCP HYPERDISK_BALANCED types.
        state:
          $ref: '#/components/schemas/InstancePoolState'
          description: Current state of the instance pool.
        stats:
          $ref: '#/components/schemas/InstancePoolStats'
          description: Usage statistics about the instance pool.
        status:
          $ref: '#/components/schemas/InstancePoolStatus'
          description: Status of failed pending instances in the pool.
        total_initial_remote_disk_size:
          type: integer
          description: >-
            If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            types.
      required:
        - instance_pool_id
    GetInstancePoolPermissionLevelsResponse:
      type: object
      properties:
        permission_levels:
          type: array
          items:
            $ref: '#/components/schemas/InstancePoolPermissionsDescription'
    GetSparkVersionsResponse:
      type: object
      properties:
        versions:
          type: array
          items:
            $ref: '#/components/schemas/SparkVersion'
    GlobalInitScriptDetails:
      type: object
      properties:
        created_at:
          type: integer
        created_by:
          type: string
          description: The username of the user who created the script.
        enabled:
          type: boolean
          description: >-
            Specifies whether the script is enabled. The script runs only if
            enabled.
        name:
          type: string
          description: The name of the script
        position:
          type: integer
          description: >-
            The position of a script, where 0 represents the first script to
            run, 1 is the second script to run, in ascending order.
        script_id:
          type: string
          description: The global init script ID.
        updated_at:
          type: integer
          description: >-
            Time when the script was updated, represented as a Unix timestamp in
            milliseconds.
        updated_by:
          type: string
          description: The username of the user who last updated the script
    GlobalInitScriptDetailsWithContent:
      type: object
      properties:
        created_at:
          type: integer
        created_by:
          type: string
          description: The username of the user who created the script.
        enabled:
          type: boolean
          description: >-
            Specifies whether the script is enabled. The script runs only if
            enabled.
        name:
          type: string
          description: The name of the script
        position:
          type: integer
          description: >-
            The position of a script, where 0 represents the first script to
            run, 1 is the second script to run, in ascending order.
        script:
          type: string
          description: The Base64-encoded content of the script.
        script_id:
          type: string
          description: The global init script ID.
        updated_at:
          type: integer
          description: >-
            Time when the script was updated, represented as a Unix timestamp in
            milliseconds.
        updated_by:
          type: string
          description: The username of the user who last updated the script
    InitScriptEventDetails:
      type: object
      properties:
        cluster:
          type: array
          items:
            $ref: '#/components/schemas/InitScriptInfoAndExecutionDetails'
        global_:
          type: array
          items:
            $ref: '#/components/schemas/InitScriptInfoAndExecutionDetails'
          description: The global init scripts associated with this cluster event.
        reported_for_node:
          type: string
          description: >-
            The private ip of the node we are reporting init script execution
            details for (we will select the execution details from only one node
            rather than reporting the execution details from every node to keep
            these event details small) This should only be defined for the
            INIT_SCRIPTS_FINISHED event
    InitScriptInfo:
      type: object
      properties:
        abfss:
          $ref: '#/components/schemas/Adlsgen2Info'
          description: >-
            destination needs to be provided, e.g.
            `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`
        dbfs:
          $ref: '#/components/schemas/DbfsStorageInfo'
          description: >-
            destination needs to be provided. e.g. `{ "dbfs": { "destination" :
            "dbfs:/home/cluster_log" } }`
        file:
          $ref: '#/components/schemas/LocalFileInfo'
          description: >-
            destination needs to be provided, e.g. `{ "file": { "destination":
            "file:/my/local/file.sh" } }`
        gcs:
          $ref: '#/components/schemas/GcsStorageInfo'
          description: >-
            destination needs to be provided, e.g. `{ "gcs": { "destination":
            "gs://my-bucket/file.sh" } }`
        s3:
          $ref: '#/components/schemas/S3StorageInfo'
          description: >-
            destination and either the region or endpoint need to be provided.
            e.g. `{ \"s3\": { \"destination\":
            \"s3://cluster_log_bucket/prefix\", \"region\": \"us-west-2\" } }`
            Cluster iam role is used to access s3, please make sure the cluster
            iam role in `instance_profile_arn` has permission to write data to
            the s3 destination.
        volumes:
          $ref: '#/components/schemas/VolumesStorageInfo'
          description: >-
            destination needs to be provided. e.g. `{ \"volumes\" : {
            \"destination\" : \"/Volumes/my-init.sh\" } }`
        workspace:
          $ref: '#/components/schemas/WorkspaceStorageInfo'
          description: >-
            destination needs to be provided, e.g. `{ "workspace": {
            "destination": "/cluster-init-scripts/setup-datadog.sh" } }`
      description: 'Config for an individual init script Next ID: 11'
    InitScriptInfoAndExecutionDetails:
      type: object
      properties:
        abfss:
          $ref: '#/components/schemas/Adlsgen2Info'
        dbfs:
          $ref: '#/components/schemas/DbfsStorageInfo'
          description: >-
            destination needs to be provided. e.g. `{ "dbfs": { "destination" :
            "dbfs:/home/cluster_log" } }`
        error_message:
          type: string
          description: >-
            Additional details regarding errors (such as a file not found
            message if the status is FAILED_FETCH). This field should only be
            used to provide *additional* information to the status field, not
            duplicate it.
        execution_duration_seconds:
          type: integer
          description: The number duration of the script execution in seconds
        file:
          $ref: '#/components/schemas/LocalFileInfo'
          description: >-
            destination needs to be provided, e.g. `{ "file": { "destination":
            "file:/my/local/file.sh" } }`
        gcs:
          $ref: '#/components/schemas/GcsStorageInfo'
          description: >-
            destination needs to be provided, e.g. `{ "gcs": { "destination":
            "gs://my-bucket/file.sh" } }`
        s3:
          $ref: '#/components/schemas/S3StorageInfo'
          description: >-
            destination and either the region or endpoint need to be provided.
            e.g. `{ \"s3\": { \"destination\":
            \"s3://cluster_log_bucket/prefix\", \"region\": \"us-west-2\" } }`
            Cluster iam role is used to access s3, please make sure the cluster
            iam role in `instance_profile_arn` has permission to write data to
            the s3 destination.
        status:
          $ref: >-
            #/components/schemas/InitScriptExecutionDetailsInitScriptExecutionStatus
          description: The current status of the script
        stderr:
          type: string
          description: >-
            The stderr output from the init script execution. Only populated
            when init scripts debug is enabled and script execution fails.
        volumes:
          $ref: '#/components/schemas/VolumesStorageInfo'
          description: >-
            destination needs to be provided. e.g. `{ \"volumes\" : {
            \"destination\" : \"/Volumes/my-init.sh\" } }`
        workspace:
          $ref: '#/components/schemas/WorkspaceStorageInfo'
          description: >-
            destination needs to be provided, e.g. `{ "workspace": {
            "destination": "/cluster-init-scripts/setup-datadog.sh" } }`
    InstallLibrariesResponse:
      type: object
      properties: {}
    InstancePoolAccessControlRequest:
      type: object
      properties:
        group_name:
          type: string
        permission_level:
          $ref: '#/components/schemas/InstancePoolPermissionLevel'
        service_principal_name:
          type: string
          description: application ID of a service principal
        user_name:
          type: string
          description: name of the user
    InstancePoolAccessControlResponse:
      type: object
      properties:
        all_permissions:
          type: array
          items:
            $ref: '#/components/schemas/InstancePoolPermission'
        display_name:
          type: string
          description: Display name of the user or service principal.
        group_name:
          type: string
          description: name of the group
        service_principal_name:
          type: string
          description: Name of the service principal.
        user_name:
          type: string
          description: name of the user
    InstancePoolAndStats:
      type: object
      properties:
        aws_attributes:
          $ref: '#/components/schemas/InstancePoolAwsAttributes'
        azure_attributes:
          $ref: '#/components/schemas/InstancePoolAzureAttributes'
          description: >-
            Attributes related to instance pools running on Azure. If not
            specified at pool creation, a set of default values will be used.
        custom_tags:
          type: object
          description: >-
            Additional tags for pool resources. Databricks will tag all pool
            resources (e.g., AWS instances and EBS volumes) with these tags in
            addition to `default_tags`. Notes: - Currently, Databricks allows at
            most 45 custom tags
        default_tags:
          type: object
          description: >-
            Tags that are added by Databricks regardless of any ``custom_tags``,
            including: - Vendor: Databricks - InstancePoolCreator:
            <user_id_of_creator> - InstancePoolName: <name_of_pool> -
            InstancePoolId: <id_of_pool>
        disk_spec:
          $ref: '#/components/schemas/DiskSpec'
          description: >-
            Defines the specification of the disks that will be attached to all
            spark containers.
        enable_elastic_disk:
          type: boolean
          description: >-
            Autoscaling Local Storage: when enabled, this instances in this pool
            will dynamically acquire additional disk space when its Spark
            workers are running low on disk space. In AWS, this feature requires
            specific AWS permissions to function correctly - refer to the User
            Guide for more details.
        gcp_attributes:
          $ref: '#/components/schemas/InstancePoolGcpAttributes'
          description: >-
            Attributes related to instance pools running on Google Cloud
            Platform. If not specified at pool creation, a set of default values
            will be used.
        idle_instance_autotermination_minutes:
          type: integer
          description: >-
            Automatically terminates the extra instances in the pool cache after
            they are inactive for this time in minutes if min_idle_instances
            requirement is already met. If not set, the extra pool instances
            will be automatically terminated after a default timeout. If
            specified, the threshold must be between 0 and 10000 minutes. Users
            can also set this value to 0 to instantly remove idle instances from
            the cache if min cache size could still hold.
        instance_pool_id:
          type: string
          description: Canonical unique identifier for the pool.
        instance_pool_name:
          type: string
          description: >-
            Pool name requested by the user. Pool name must be unique. Length
            must be between 1 and 100 characters.
        max_capacity:
          type: integer
          description: >-
            Maximum number of outstanding instances to keep in the pool,
            including both instances used by clusters and idle instances.
            Clusters that require further instance provisioning will fail during
            upsize requests.
        min_idle_instances:
          type: integer
          description: Minimum number of idle instances to keep in the instance pool
        node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for the pool.
        node_type_id:
          type: string
          description: >-
            This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark
            nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved
            by using the :method:clusters/listNodeTypes API call.
        preloaded_docker_images:
          type: array
          items:
            $ref: '#/components/schemas/DockerImage'
          description: Custom Docker Image BYOC
        preloaded_spark_versions:
          type: array
          items:
            type: string
          description: >-
            A list containing at most one preloaded Spark image version for the
            pool. Pool-backed clusters started with the preloaded Spark version
            will start faster. A list of available Spark versions can be
            retrieved by using the :method:clusters/sparkVersions API call.
        remote_disk_throughput:
          type: integer
          description: >-
            If set, what the configurable throughput (in Mb/s) for the remote
            disk is. Currently only supported for GCP HYPERDISK_BALANCED types.
        state:
          $ref: '#/components/schemas/InstancePoolState'
          description: Current state of the instance pool.
        stats:
          $ref: '#/components/schemas/InstancePoolStats'
          description: Usage statistics about the instance pool.
        status:
          $ref: '#/components/schemas/InstancePoolStatus'
          description: Status of failed pending instances in the pool.
        total_initial_remote_disk_size:
          type: integer
          description: >-
            If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            types.
    InstancePoolAwsAttributes:
      type: object
      properties:
        availability:
          $ref: '#/components/schemas/InstancePoolAwsAttributesAvailability'
          description: Availability type used for the spot nodes.
        instance_profile_arn:
          type: string
          description: >-
            All AWS instances belonging to the instance pool will have this
            instance profile. If omitted, instances will initially be launched
            with the workspace's default instance profile. If defined, clusters
            that use the pool will inherit the instance profile, and must not
            specify their own instance profile on cluster creation or update. If
            the pool does not specify an instance profile, clusters using the
            pool may specify any instance profile. The instance profile must
            have previously been added to the Databricks environment by an
            account administrator. This feature may only be available to certain
            customer plans.
        spot_bid_price_percent:
          type: integer
          description: >-
            Calculates the bid price for AWS spot instances, as a percentage of
            the corresponding instance type's on-demand price. For example, if
            this field is set to 50, and the cluster needs a new `r3.xlarge`
            spot instance, then the bid price is half of the price of on-demand
            `r3.xlarge` instances. Similarly, if this field is set to 200, the
            bid price is twice the price of on-demand `r3.xlarge` instances. If
            not specified, the default value is 100. When spot instances are
            requested for this cluster, only spot instances whose bid price
            percentage matches this field will be considered. Note that, for
            safety, we enforce this field to be no more than 10000.
        zone_id:
          type: string
          description: >-
            Identifier for the availability zone/datacenter in which the cluster
            resides. This string will be of a form like "us-west-2a". The
            provided availability zone must be in the same region as the
            Databricks deployment. For example, "us-west-2a" is not a valid zone
            id if the Databricks deployment resides in the "us-east-1" region.
            This is an optional field at cluster creation, and if not specified,
            a default zone will be used. The list of available zones as well as
            the default value can be found by using the `List Zones` method.
      description: >-
        Attributes set during instance pool creation which are related to Amazon
        Web Services.
    InstancePoolAzureAttributes:
      type: object
      properties:
        availability:
          $ref: '#/components/schemas/InstancePoolAzureAttributesAvailability'
          description: Availability type used for the spot nodes.
        spot_bid_max_price:
          type: number
          description: >-
            With variable pricing, you have option to set a max price, in US
            dollars (USD) For example, the value 2 would be a max price of $2.00
            USD per hour. If you set the max price to be -1, the VM won't be
            evicted based on price. The price for the VM will be the current
            price for spot or the price for a standard VM, which ever is less,
            as long as there is capacity and quota available.
      description: Attributes set during instance pool creation which are related to Azure.
    InstancePoolGcpAttributes:
      type: object
      properties:
        gcp_availability:
          $ref: '#/components/schemas/GcpAvailability'
        local_ssd_count:
          type: integer
          description: >-
            If provided, each node in the instance pool will have this number of
            local SSDs attached. Each local SSD is 375GB in size. Refer to [GCP
            documentation] for the supported number of local SSDs for each
            instance type. [GCP documentation]:
            https://cloud.google.com/compute/docs/disks/local-ssd#choose_number_local_ssds
        zone_id:
          type: string
          description: >-
            Identifier for the availability zone/datacenter in which the cluster
            resides. This string will be of a form like "us-west1-a". The
            provided availability zone must be in the same region as the
            Databricks workspace. For example, "us-west1-a" is not a valid zone
            id if the Databricks workspace resides in the "us-east1" region.
            This is an optional field at instance pool creation, and if not
            specified, a default zone will be used. This field can be one of the
            following: - "HA" => High availability, spread nodes across
            availability zones for a Databricks deployment region - A GCP
            availability zone => Pick One of the available zones for (machine
            type + region) from
            https://cloud.google.com/compute/docs/regions-zones (e.g.
            "us-west1-a"). If empty, Databricks picks an availability zone to
            schedule the cluster on.
      description: Attributes set during instance pool creation which are related to GCP.
    InstancePoolPermission:
      type: object
      properties:
        inherited:
          type: boolean
        inherited_from_object:
          type: array
          items:
            type: string
        permission_level:
          $ref: '#/components/schemas/InstancePoolPermissionLevel'
    InstancePoolPermissions:
      type: object
      properties:
        access_control_list:
          type: array
          items:
            $ref: '#/components/schemas/InstancePoolAccessControlResponse'
        object_id:
          type: string
        object_type:
          type: string
    InstancePoolPermissionsDescription:
      type: object
      properties:
        description:
          type: string
        permission_level:
          $ref: '#/components/schemas/InstancePoolPermissionLevel'
    InstancePoolStats:
      type: object
      properties:
        idle_count:
          type: integer
        pending_idle_count:
          type: integer
          description: >-
            Number of pending instances in the pool that are NOT part of a
            cluster.
        pending_used_count:
          type: integer
          description: Number of pending instances in the pool that are part of a cluster.
        used_count:
          type: integer
          description: Number of active instances in the pool that are part of a cluster.
    InstancePoolStatus:
      type: object
      properties:
        pending_instance_errors:
          type: array
          items:
            $ref: '#/components/schemas/PendingInstanceError'
    InstanceProfile:
      type: object
      properties:
        instance_profile_arn:
          type: string
        iam_role_arn:
          type: string
          description: >-
            The AWS IAM role ARN of the role associated with the instance
            profile. This field is required if your role name and instance
            profile name do not match and you want to use the instance profile
            with [Databricks SQL Serverless]. Otherwise, this field is optional.
            [Databricks SQL Serverless]:
            https://docs.databricks.com/sql/admin/serverless.html
        is_meta_instance_profile:
          type: boolean
          description: >-
            Boolean flag indicating whether the instance profile should only be
            used in credential passthrough scenarios. If true, it means the
            instance profile contains an meta IAM role which could assume a wide
            range of roles. Therefore it should always be used with
            authorization. This field is optional, the default value is `false`.
      required:
        - instance_profile_arn
    Library:
      type: object
      properties:
        cran:
          $ref: '#/components/schemas/RCranLibrary'
        egg:
          type: string
          description: >-
            Deprecated. URI of the egg library to install. Installing Python egg
            files is deprecated and is not supported in Databricks Runtime 14.0
            and above.
        jar:
          type: string
          description: >-
            URI of the JAR library to install. Supported URIs include Workspace
            paths, Unity Catalog Volumes paths, and S3 URIs. For example: `{
            "jar": "/Workspace/path/to/library.jar" }`, `{ "jar" :
            "/Volumes/path/to/library.jar" }` or `{ "jar":
            "s3://my-bucket/library.jar" }`. If S3 is used, please make sure the
            cluster has read access on the library. You may need to launch the
            cluster with an IAM role to access the S3 URI.
        maven:
          $ref: '#/components/schemas/MavenLibrary'
          description: >-
            Specification of a maven library to be installed. For example: `{
            "coordinates": "org.jsoup:jsoup:1.7.2" }`
        pypi:
          $ref: '#/components/schemas/PythonPyPiLibrary'
          description: >-
            Specification of a PyPi library to be installed. For example: `{
            "package": "simplejson" }`
        requirements:
          type: string
          description: >-
            URI of the requirements.txt file to install. Only Workspace paths
            and Unity Catalog Volumes paths are supported. For example: `{
            "requirements": "/Workspace/path/to/requirements.txt" }` or `{
            "requirements" : "/Volumes/path/to/requirements.txt" }`
        whl:
          type: string
          description: >-
            URI of the wheel library to install. Supported URIs include
            Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For
            example: `{ "whl": "/Workspace/path/to/library.whl" }`, `{ "whl" :
            "/Volumes/path/to/library.whl" }` or `{ "whl":
            "s3://my-bucket/library.whl" }`. If S3 is used, please make sure the
            cluster has read access on the library. You may need to launch the
            cluster with an IAM role to access the S3 URI.
    LibraryFullStatus:
      type: object
      properties:
        is_library_for_all_clusters:
          type: boolean
          description: >-
            Whether the library was set to be installed on all clusters via the
            libraries UI.
        library:
          $ref: '#/components/schemas/Library'
          description: Unique identifier for the library.
        messages:
          type: array
          items:
            type: string
          description: >-
            All the info and warning messages that have occurred so far for this
            library.
        status:
          $ref: '#/components/schemas/LibraryInstallStatus'
          description: Status of installing the library on the cluster.
      description: The status of the library on a specific cluster.
    ListAllClusterLibraryStatusesResponse:
      type: object
      properties:
        statuses:
          type: array
          items:
            $ref: '#/components/schemas/ClusterLibraryStatuses'
    ListAvailableZonesResponse:
      type: object
      properties:
        default_zone:
          type: string
        zones:
          type: array
          items:
            type: string
          description: The list of available zones (e.g., ['us-west-2c', 'us-east-2']).
    ListClusterCompliancesResponse:
      type: object
      properties:
        clusters:
          type: array
          items:
            $ref: '#/components/schemas/ClusterCompliance'
        next_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the next page
            of results. If the value is "", it means no further results for the
            request.
        prev_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the previous
            page of results. If the value is "", it means no further results for
            the request.
    ListClustersFilterBy:
      type: object
      properties:
        cluster_sources:
          type: array
          items:
            $ref: '#/components/schemas/ClusterSource'
        cluster_states:
          type: array
          items:
            $ref: '#/components/schemas/State'
          description: The current state of the clusters.
        is_pinned:
          type: boolean
          description: Whether the clusters are pinned or not.
        policy_id:
          type: string
          description: >-
            The ID of the cluster policy used to create the cluster if
            applicable.
    ListClustersResponse:
      type: object
      properties:
        clusters:
          type: array
          items:
            $ref: '#/components/schemas/ClusterDetails'
        next_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the next page
            of results. If the value is "", it means no further results for the
            request.
        prev_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the previous
            page of results. If the value is "", it means no further results for
            the request.
    ListClustersSortBy:
      type: object
      properties:
        direction:
          $ref: '#/components/schemas/ListClustersSortByDirection'
        field:
          $ref: '#/components/schemas/ListClustersSortByField'
          description: >-
            The sorting criteria. By default, clusters are sorted by 3 columns
            from highest to lowest precedence: cluster state, pinned or
            unpinned, then cluster name.
    ListGlobalInitScriptsResponse:
      type: object
      properties:
        scripts:
          type: array
          items:
            $ref: '#/components/schemas/GlobalInitScriptDetails'
    ListInstancePools:
      type: object
      properties:
        instance_pools:
          type: array
          items:
            $ref: '#/components/schemas/InstancePoolAndStats'
    ListInstanceProfilesResponse:
      type: object
      properties:
        instance_profiles:
          type: array
          items:
            $ref: '#/components/schemas/InstanceProfile'
    ListNodeTypesResponse:
      type: object
      properties:
        node_types:
          type: array
          items:
            $ref: '#/components/schemas/NodeType'
    ListPoliciesResponse:
      type: object
      properties:
        policies:
          type: array
          items:
            $ref: '#/components/schemas/Policy'
    ListPolicyFamiliesResponse:
      type: object
      properties:
        next_page_token:
          type: string
        policy_families:
          type: array
          items:
            $ref: '#/components/schemas/PolicyFamily'
          description: List of policy families.
    LocalFileInfo:
      type: object
      properties:
        destination:
          type: string
      required:
        - destination
    LogAnalyticsInfo:
      type: object
      properties:
        log_analytics_primary_key:
          type: string
        log_analytics_workspace_id:
          type: string
    LogSyncStatus:
      type: object
      properties:
        last_attempted:
          type: integer
          description: >-
            The timestamp of last attempt. If the last attempt fails,
            `last_exception` will contain the exception in the last attempt.
        last_exception:
          type: string
          description: >-
            The exception thrown in the last attempt, it would be null (omitted
            in the response) if there is no exception in last attempted.
      description: The log delivery status
    MavenLibrary:
      type: object
      properties:
        coordinates:
          type: string
        exclusions:
          type: array
          items:
            type: string
          description: >-
            List of dependences to exclude. For example: `["slf4j:slf4j",
            "*:hadoop-client"]`. Maven dependency exclusions:
            https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html.
        repo:
          type: string
          description: >-
            Maven repo to install the Maven package from. If omitted, both Maven
            Central Repository and Spark Packages are searched.
      required:
        - coordinates
    NodeInstanceType:
      type: object
      properties:
        instance_type_id:
          type: string
          description: Unique identifier across instance types
        local_disk_size_gb:
          type: integer
          description: >-
            Size of the individual local disks attached to this instance (i.e.
            per local disk).
        local_disks:
          type: integer
          description: Number of local disks that are present on this instance.
        local_nvme_disk_size_gb:
          type: integer
          description: >-
            Size of the individual local nvme disks attached to this instance
            (i.e. per local disk).
        local_nvme_disks:
          type: integer
          description: Number of local nvme disks that are present on this instance.
      required:
        - instance_type_id
      description: >-
        This structure embodies the machine type that hosts spark containers
        Note: this should be an
            internal data structure for now It is defined in proto in case we want to send it over the wire
            in the future (which is likely)
    NodeType:
      type: object
      properties:
        node_type_id:
          type: string
          description: Unique identifier for this node type.
        memory_mb:
          type: integer
          description: Memory (in MB) available for this node type.
        num_cores:
          type: number
          description: >-
            Number of CPU cores available for this node type. Note that this can
            be fractional, e.g., 2.5 cores, if the the number of cores on a
            machine instance is not divisible by the number of Spark nodes on
            that machine.
        description:
          type: string
          description: >-
            A string description associated with this node type, e.g.,
            "r3.xlarge".
        instance_type_id:
          type: string
          description: >-
            An identifier for the type of hardware that this node runs on, e.g.,
            "r3.2xlarge" in AWS.
        category:
          type: string
          description: >-
            A descriptive category for this node type. Examples include "Memory
            Optimized" and "Compute Optimized".
        display_order:
          type: integer
          description: >-
            An optional hint at the display order of node types in the UI.
            Within a node type category, lowest numbers come first.
        is_deprecated:
          type: boolean
          description: >-
            Whether the node type is deprecated. Non-deprecated node types offer
            greater performance.
        is_encrypted_in_transit:
          type: boolean
          description: >-
            AWS specific, whether this instance supports encryption in transit,
            used for hipaa and pci workloads.
        is_graviton:
          type: boolean
          description: Whether this is an Arm-based instance.
        is_hidden:
          type: boolean
          description: Whether this node is hidden from presentation in the UI.
        is_io_cache_enabled:
          type: boolean
          description: Whether this node comes with IO cache enabled by default.
        node_info:
          $ref: '#/components/schemas/CloudProviderNodeInfo'
          description: A collection of node type info reported by the cloud provider
        node_instance_type:
          $ref: '#/components/schemas/NodeInstanceType'
          description: The NodeInstanceType object corresponding to instance_type_id
        num_gpus:
          type: integer
          description: Number of GPUs available for this node type.
        photon_driver_capable:
          type: boolean
        photon_worker_capable:
          type: boolean
        support_cluster_tags:
          type: boolean
          description: Whether this node type support cluster tags.
        support_ebs_volumes:
          type: boolean
          description: >-
            Whether this node type support EBS volumes. EBS volumes is disabled
            for node types that we could place multiple corresponding containers
            on the same hosting instance.
        support_port_forwarding:
          type: boolean
          description: Whether this node type supports port forwarding.
      required:
        - node_type_id
        - memory_mb
        - num_cores
        - description
        - instance_type_id
        - category
      description: >-
        A description of a Spark node type including both the dimensions of the
        node and the instance
            type on which it will be hosted.
    NodeTypeFlexibility:
      type: object
      properties:
        alternate_node_type_ids:
          type: array
          items:
            type: string
          description: >-
            A list of node type IDs to use as fallbacks when the primary node
            type is unavailable.
      description: >-
        Configuration for flexible node types, allowing fallback to alternate
        node types during cluster
            launch and upscale.
    PendingInstanceError:
      type: object
      properties:
        instance_id:
          type: string
        message:
          type: string
      description: Error message of a failed pending instances
    PermanentDeleteClusterResponse:
      type: object
      properties: {}
    PinClusterResponse:
      type: object
      properties: {}
    Policy:
      type: object
      properties:
        created_at_timestamp:
          type: integer
          description: >-
            Creation time. The timestamp (in millisecond) when this Cluster
            Policy was created.
        creator_user_name:
          type: string
          description: >-
            Creator user name. The field won't be included in the response if
            the user has already been deleted.
        definition:
          type: string
          description: >-
            Policy definition document expressed in [Databricks Cluster Policy
            Definition Language]. [Databricks Cluster Policy Definition
            Language]:
            https://docs.databricks.com/administration-guide/clusters/policy-definition.html
        description:
          type: string
          description: Additional human-readable description of the cluster policy.
        is_default:
          type: boolean
          description: >-
            If true, policy is a default policy created and managed by
            Databricks. Default policies cannot be deleted, and their policy
            families cannot be changed.
        libraries:
          type: array
          items:
            $ref: '#/components/schemas/Library'
          description: >-
            A list of libraries to be installed on the next cluster restart that
            uses this policy. The maximum number of libraries is 500.
        max_clusters_per_user:
          type: integer
          description: >-
            Max number of clusters per user that can be active using this
            policy. If not present, there is no max limit.
        name:
          type: string
          description: >-
            Cluster Policy name requested by the user. This has to be unique.
            Length must be between 1 and 100 characters.
        policy_family_definition_overrides:
          type: string
          description: >-
            Policy definition JSON document expressed in [Databricks Policy
            Definition Language]. The JSON document must be passed as a string
            and cannot be embedded in the requests. You can use this to
            customize the policy definition inherited from the policy family.
            Policy rules specified here are merged into the inherited policy
            definition. [Databricks Policy Definition Language]:
            https://docs.databricks.com/administration-guide/clusters/policy-definition.html
        policy_family_id:
          type: string
          description: >-
            ID of the policy family. The cluster policy's policy definition
            inherits the policy family's policy definition. Cannot be used with
            `definition`. Use `policy_family_definition_overrides` instead to
            customize the policy definition.
        policy_id:
          type: string
          description: Canonical unique identifier for the Cluster Policy.
      description: Describes a Cluster Policy entity.
    PolicyFamily:
      type: object
      properties:
        definition:
          type: string
        description:
          type: string
          description: Human-readable description of the purpose of the policy family.
        name:
          type: string
          description: Name of the policy family.
        policy_family_id:
          type: string
          description: Unique identifier for the policy family.
    PythonPyPiLibrary:
      type: object
      properties:
        package:
          type: string
        repo:
          type: string
          description: >-
            The repository where the package can be found. If not specified, the
            default pip index is used.
      required:
        - package
    RCranLibrary:
      type: object
      properties:
        package:
          type: string
        repo:
          type: string
          description: >-
            The repository where the package can be found. If not specified, the
            default CRAN repo is used.
      required:
        - package
    RemoveResponse:
      type: object
      properties: {}
    ResizeClusterResponse:
      type: object
      properties: {}
    RestartClusterResponse:
      type: object
      properties: {}
    Results:
      type: object
      properties:
        cause:
          type: string
        data:
          $ref: '#/components/schemas/Any'
        file_name:
          type: string
          description: >-
            The image data in one of the following formats: 1. A Data URL with
            base64-encoded image data: `data:image/{type};base64,{base64-data}`.
            Example: `data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUA...` 2.
            A FileStore file path for large images: `/plots/{filename}.png`.
            Example: `/plots/b6a7ad70-fb2c-4353-8aed-3f1e015174a4.png`
        file_names:
          type: array
          items:
            type: string
          description: >-
            List of image data for multiple images. Each element follows the
            same format as file_name.
        is_json_schema:
          type: boolean
          description: >-
            true if a JSON schema is returned instead of a string representation
            of the Hive type.
        pos:
          type: integer
          description: internal field used by SDK
        result_type:
          $ref: '#/components/schemas/ResultType'
        schema:
          type: array
          items:
            type: object
          description: The table schema
        summary:
          type: string
          description: The summary of the error
        truncated:
          type: boolean
          description: true if partial results are returned.
    S3StorageInfo:
      type: object
      properties:
        destination:
          type: string
          description: >-
            S3 destination, e.g. `s3://my-bucket/some-prefix` Note that logs
            will be delivered using cluster iam role, please make sure you set
            cluster iam role and the role has write access to the destination.
            Please also note that you cannot use AWS keys to deliver logs.
        canned_acl:
          type: string
          description: >-
            (Optional) Set canned access control list for the logs, e.g.
            `bucket-owner-full-control`. If `canned_cal` is set, please make
            sure the cluster iam role has `s3:PutObjectAcl` permission on the
            destination bucket and prefix. The full list of possible canned acl
            can be found at
            http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl.
            Please also note that by default only the object owner gets full
            controls. If you are using cross account role for writing data, you
            may want to set `bucket-owner-full-control` to make bucket owner
            able to read the logs.
        enable_encryption:
          type: boolean
          description: >-
            (Optional) Flag to enable server side encryption, `false` by
            default.
        encryption_type:
          type: string
          description: >-
            (Optional) The encryption type, it could be `sse-s3` or `sse-kms`.
            It will be used only when encryption is enabled and the default type
            is `sse-s3`.
        endpoint:
          type: string
          description: >-
            S3 endpoint, e.g. `https://s3-us-west-2.amazonaws.com`. Either
            region or endpoint needs to be set. If both are set, endpoint will
            be used.
        kms_key:
          type: string
          description: >-
            (Optional) Kms key which will be used if encryption is enabled and
            encryption type is set to `sse-kms`.
        region:
          type: string
          description: >-
            S3 region, e.g. `us-west-2`. Either region or endpoint needs to be
            set. If both are set, endpoint will be used.
      required:
        - destination
      description: A storage location in Amazon S3
    SparkNode:
      type: object
      properties:
        host_private_ip:
          type: string
          description: The private IP address of the host instance.
        instance_id:
          type: string
          description: >-
            Globally unique identifier for the host instance from the cloud
            provider.
        node_aws_attributes:
          $ref: '#/components/schemas/SparkNodeAwsAttributes'
          description: Attributes specific to AWS for a Spark node.
        node_id:
          type: string
          description: Globally unique identifier for this node.
        private_ip:
          type: string
          description: >-
            Private IP address (typically a 10.x.x.x address) of the Spark node.
            Note that this is different from the private IP address of the host
            instance.
        public_dns:
          type: string
          description: >-
            Public DNS address of this node. This address can be used to access
            the Spark JDBC server on the driver node. To communicate with the
            JDBC server, traffic must be manually authorized by adding security
            group rules to the "worker-unmanaged" security group via the AWS
            console.
        start_timestamp:
          type: integer
          description: The timestamp (in millisecond) when the Spark node is launched.
      description: Describes a specific Spark driver or executor.
    SparkNodeAwsAttributes:
      type: object
      properties:
        is_spot:
          type: boolean
          description: Whether this node is on an Amazon spot instance.
      description: Attributes specific to AWS for a Spark node.
    SparkVersion:
      type: object
      properties:
        key:
          type: string
        name:
          type: string
          description: A descriptive name for this Spark version, for example "Spark 2.1".
    StartClusterResponse:
      type: object
      properties: {}
    TerminationReason:
      type: object
      properties:
        code:
          $ref: '#/components/schemas/TerminationReasonCode'
        parameters:
          type: object
          description: >-
            list of parameters that provide additional information about why the
            cluster was terminated
        type:
          $ref: '#/components/schemas/TerminationReasonType'
          description: type of the termination
    UninstallLibrariesResponse:
      type: object
      properties: {}
    UnpinClusterResponse:
      type: object
      properties: {}
    UpdateClusterResource:
      type: object
      properties:
        autoscale:
          $ref: '#/components/schemas/AutoScale'
        autotermination_minutes:
          type: integer
          description: >-
            Automatically terminates the cluster after it is inactive for this
            time in minutes. If not set, this cluster will not be automatically
            terminated. If specified, the threshold must be between 10 and 10000
            minutes. Users can also set this value to 0 to explicitly disable
            automatic termination.
        aws_attributes:
          $ref: '#/components/schemas/AwsAttributes'
          description: >-
            Attributes related to clusters running on Amazon Web Services. If
            not specified at cluster creation, a set of default values will be
            used.
        azure_attributes:
          $ref: '#/components/schemas/AzureAttributes'
          description: >-
            Attributes related to clusters running on Microsoft Azure. If not
            specified at cluster creation, a set of default values will be used.
        cluster_log_conf:
          $ref: '#/components/schemas/ClusterLogConf'
          description: >-
            The configuration for delivering spark logs to a long-term storage
            destination. Three kinds of destinations (DBFS, S3 and Unity Catalog
            volumes) are supported. Only one destination can be specified for
            one cluster. If the conf is given, the logs will be delivered to the
            destination every `5 mins`. The destination of driver logs is
            `$destination/$clusterId/driver`, while the destination of executor
            logs is `$destination/$clusterId/executor`.
        cluster_name:
          type: string
          description: >-
            Cluster name requested by the user. This doesn't have to be unique.
            If not specified at creation, the cluster name will be an empty
            string. For job clusters, the cluster name is automatically set
            based on the job and job run IDs.
        custom_tags:
          type: object
          description: >-
            Additional tags for cluster resources. Databricks will tag all
            cluster resources (e.g., AWS instances and EBS volumes) with these
            tags in addition to `default_tags`. Notes: - Currently, Databricks
            allows at most 45 custom tags - Clusters can only reuse cloud
            resources if the resources' tags are a subset of the cluster tags
        data_security_mode:
          $ref: '#/components/schemas/DataSecurityMode'
        docker_image:
          $ref: '#/components/schemas/DockerImage'
          description: Custom docker image BYOC
        driver_instance_pool_id:
          type: string
          description: >-
            The optional ID of the instance pool for the driver of the cluster
            belongs. The pool cluster uses the instance pool with id
            (instance_pool_id) if the driver pool is not assigned.
        driver_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for the driver node.
        driver_node_type_id:
          type: string
          description: >-
            The node type of the Spark driver. Note that this field is optional;
            if unset, the driver node type will be set as the same value as
            `node_type_id` defined above. This field, along with node_type_id,
            should not be set if virtual_cluster_size is set. If both
            driver_node_type_id, node_type_id, and virtual_cluster_size are
            specified, driver_node_type_id and node_type_id take precedence.
        enable_elastic_disk:
          type: boolean
          description: >-
            Autoscaling Local Storage: when enabled, this cluster will
            dynamically acquire additional disk space when its Spark workers are
            running low on disk space.
        enable_local_disk_encryption:
          type: boolean
          description: Whether to enable LUKS on cluster VMs' local disks
        gcp_attributes:
          $ref: '#/components/schemas/GcpAttributes'
          description: >-
            Attributes related to clusters running on Google Cloud Platform. If
            not specified at cluster creation, a set of default values will be
            used.
        init_scripts:
          type: array
          items:
            $ref: '#/components/schemas/InitScriptInfo'
          description: >-
            The configuration for storing init scripts. Any number of
            destinations can be specified. The scripts are executed sequentially
            in the order provided. If `cluster_log_conf` is specified, init
            script logs are sent to `<destination>/<cluster-ID>/init_scripts`.
        instance_pool_id:
          type: string
          description: The optional ID of the instance pool to which the cluster belongs.
        is_single_node:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`. When set
            to true, Databricks will automatically set single node related
            `custom_tags`, `spark_conf`, and `num_workers`
        kind:
          $ref: '#/components/schemas/Kind'
        node_type_id:
          type: string
          description: >-
            This field encodes, through a single value, the resources available
            to each of the Spark nodes in this cluster. For example, the Spark
            nodes can be provisioned and optimized for memory or compute
            intensive workloads. A list of available node types can be retrieved
            by using the :method:clusters/listNodeTypes API call.
        num_workers:
          type: integer
          description: >-
            Number of worker nodes that this cluster should have. A cluster has
            one Spark Driver and `num_workers` Executors for a total of
            `num_workers` + 1 Spark nodes. Note: When reading the properties of
            a cluster, this field reflects the desired number of workers rather
            than the actual current number of workers. For instance, if a
            cluster is resized from 5 to 10 workers, this field will immediately
            be updated to reflect the target size of 10 workers, whereas the
            workers listed in `spark_info` will gradually increase from 5 to 10
            as the new nodes are provisioned.
        policy_id:
          type: string
          description: >-
            The ID of the cluster policy used to create the cluster if
            applicable.
        remote_disk_throughput:
          type: integer
          description: >-
            If set, what the configurable throughput (in Mb/s) for the remote
            disk is. Currently only supported for GCP HYPERDISK_BALANCED disks.
        runtime_engine:
          $ref: '#/components/schemas/RuntimeEngine'
          description: >-
            Determines the cluster's runtime engine, either standard or Photon.
            This field is not compatible with legacy `spark_version` values that
            contain `-photon-`. Remove `-photon-` from the `spark_version` and
            set `runtime_engine` to `PHOTON`. If left unspecified, the runtime
            engine defaults to standard unless the spark_version contains
            -photon-, in which case Photon will be used.
        single_user_name:
          type: string
          description: Single user name if data_security_mode is `SINGLE_USER`
        spark_conf:
          type: object
          description: >-
            An object containing a set of optional, user-specified Spark
            configuration key-value pairs. Users can also pass in a string of
            extra JVM options to the driver and the executors via
            `spark.driver.extraJavaOptions` and
            `spark.executor.extraJavaOptions` respectively.
        spark_env_vars:
          type: object
          description: >-
            An object containing a set of optional, user-specified environment
            variable key-value pairs. Please note that key-value pair of the
            form (X,Y) will be exported as is (i.e., `export X='Y'`) while
            launching the driver and workers. In order to specify an additional
            set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to
            `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This
            ensures that all default databricks managed environmental variables
            are included as well. Example Spark environment variables:
            `{"SPARK_WORKER_MEMORY": "28000m", "SPARK_LOCAL_DIRS":
            "/local_disk0"}` or `{"SPARK_DAEMON_JAVA_OPTS":
            "$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true"}`
        spark_version:
          type: string
          description: >-
            The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of
            available Spark versions can be retrieved by using the
            :method:clusters/sparkVersions API call.
        ssh_public_keys:
          type: array
          items:
            type: string
          description: >-
            SSH public key contents that will be added to each Spark node in
            this cluster. The corresponding private keys can be used to login
            with the user name `ubuntu` on port `2200`. Up to 10 keys can be
            specified.
        total_initial_remote_disk_size:
          type: integer
          description: >-
            If set, what the total initial volume size (in GB) of the remote
            disks should be. Currently only supported for GCP HYPERDISK_BALANCED
            disks.
        use_ml_runtime:
          type: boolean
          description: >-
            This field can only be used when `kind = CLASSIC_PREVIEW`.
            `effective_spark_version` is determined by `spark_version` (DBR
            release), this field `use_ml_runtime`, and whether `node_type_id` is
            gpu node or not.
        worker_node_type_flexibility:
          $ref: '#/components/schemas/NodeTypeFlexibility'
          description: Flexible node type configuration for worker nodes.
        workload_type:
          $ref: '#/components/schemas/WorkloadType'
    UpdateClusterResponse:
      type: object
      properties: {}
    UpdateResponse:
      type: object
      properties: {}
    VolumesStorageInfo:
      type: object
      properties:
        destination:
          type: string
          description: >-
            UC Volumes destination, e.g.
            `/Volumes/catalog/schema/vol1/init-scripts/setup-datadog.sh` or
            `dbfs:/Volumes/catalog/schema/vol1/init-scripts/setup-datadog.sh`
      required:
        - destination
      description: A storage location back by UC Volumes.
    WorkloadType:
      type: object
      properties:
        clients:
          $ref: '#/components/schemas/ClientsTypes'
          description: >-
            defined what type of clients can use the cluster. E.g. Notebooks,
            Jobs
      required:
        - clients
      description: Cluster Attributes showing for clusters workload types.
    WorkspaceStorageInfo:
      type: object
      properties:
        destination:
          type: string
          description: >-
            wsfs destination, e.g.
            `workspace:/cluster-init-scripts/setup-datadog.sh`
      required:
        - destination
      description: A storage location in Workspace Filesystem (WSFS)
    AwsAvailability:
      type: string
      enum:
        - ON_DEMAND
        - SPOT
        - SPOT_WITH_FALLBACK
      description: >-
        Availability type used for all subsequent nodes past the
        `first_on_demand` ones.


        Note: If `first_on_demand` is zero, this availability type will be used
        for the entire cluster.
    AzureAvailability:
      type: string
      enum:
        - ON_DEMAND_AZURE
        - SPOT_AZURE
        - SPOT_WITH_FALLBACK_AZURE
      description: >-
        Availability type used for all subsequent nodes past the
        `first_on_demand` ones. Note: If

        `first_on_demand` is zero, this availability type will be used for the
        entire cluster.
    CloudProviderNodeStatus:
      type: string
      enum:
        - NotAvailableInRegion
        - NotEnabledOnSubscription
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ClusterPermissionLevel:
      type: string
      enum:
        - CAN_ATTACH_TO
        - CAN_MANAGE
        - CAN_RESTART
      description: Permission level
    ClusterPolicyPermissionLevel:
      type: string
      enum:
        - CAN_USE
      description: Permission level
    ClusterSource:
      type: string
      enum:
        - API
        - JOB
        - MODELS
        - PIPELINE
        - PIPELINE_MAINTENANCE
        - SQL
        - UI
      description: >-
        Determines whether the cluster was created by a user through the UI,
        created by the Databricks

        Jobs Scheduler, or through an API request. This is the same as
        cluster_creator, but read only.
    CommandStatus:
      type: string
      enum:
        - Cancelled
        - Cancelling
        - Error
        - Finished
        - Queued
        - Running
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ContextStatus:
      type: string
      enum:
        - Error
        - Pending
        - Running
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    DataPlaneEventDetailsEventType:
      type: string
      enum:
        - NODE_BLACKLISTED
        - NODE_EXCLUDED_DECOMMISSIONED
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    DataSecurityMode:
      type: string
      enum:
        - DATA_SECURITY_MODE_AUTO
        - DATA_SECURITY_MODE_DEDICATED
        - DATA_SECURITY_MODE_STANDARD
        - LEGACY_PASSTHROUGH
        - LEGACY_SINGLE_USER
        - LEGACY_SINGLE_USER_STANDARD
        - LEGACY_TABLE_ACL
        - NONE
        - SINGLE_USER
        - USER_ISOLATION
      description: >-
        Data security mode decides what data governance model to use when
        accessing data from a cluster.


        The following modes can only be used when `kind = CLASSIC_PREVIEW`. *
        `DATA_SECURITY_MODE_AUTO`:

        Databricks will choose the most appropriate access mode depending on
        your compute configuration.

        * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`. *
        `DATA_SECURITY_MODE_DEDICATED`:

        Alias for `SINGLE_USER`.


        The following modes can be used regardless of `kind`. * `NONE`: No
        security isolation for

        multiple users sharing the cluster. Data governance features are not
        available in this mode. *

        `SINGLE_USER`: A secure cluster that can only be exclusively used by a
        single user specified in

        `single_user_name`. Most programming languages, cluster features and
        data governance features

        are available in this mode. * `USER_ISOLATION`: A secure cluster that
        can be shared by multiple

        users. Cluster users are fully isolated so that they cannot see each
        other's data and

        credentials. Most data governance features are supported in this mode.
        But programming languages

        and cluster features might be limited.


        The following modes are deprecated starting with Databricks Runtime 15.0
        and will be removed for

        future Databricks Runtime versions:


        * `LEGACY_TABLE_ACL`: This mode is for users migrating from legacy Table
        ACL clusters. *

        `LEGACY_PASSTHROUGH`: This mode is for users migrating from legacy
        Passthrough on high

        concurrency clusters. * `LEGACY_SINGLE_USER`: This mode is for users
        migrating from legacy

        Passthrough on standard clusters. * `LEGACY_SINGLE_USER_STANDARD`: This
        mode provides a way that

        doesnt have UC nor passthrough enabled.
    DiskTypeAzureDiskVolumeType:
      type: string
      enum:
        - PREMIUM_LRS
        - STANDARD_LRS
      description: >-
        All Azure Disk types that Databricks supports. See

        https://docs.microsoft.com/en-us/azure/storage/storage-about-disks-and-vhds-linux#types-of-disks
    DiskTypeEbsVolumeType:
      type: string
      enum:
        - GENERAL_PURPOSE_SSD
        - THROUGHPUT_OPTIMIZED_HDD
      description: >-
        All EBS volume types that Databricks supports. See
        https://aws.amazon.com/ebs/details/ for

        details.
    EbsVolumeType:
      type: string
      enum:
        - GENERAL_PURPOSE_SSD
        - THROUGHPUT_OPTIMIZED_HDD
      description: >-
        All EBS volume types that Databricks supports. See
        https://aws.amazon.com/ebs/details/ for

        details.
    EventDetailsCause:
      type: string
      enum:
        - AUTORECOVERY
        - AUTOSCALE
        - AUTOSCALE_V2
        - REPLACE_BAD_NODES
        - USER_REQUEST
      description: The cause of a change in target size.
    EventType:
      type: string
      enum:
        - ADD_NODES_FAILED
        - AUTOMATIC_CLUSTER_UPDATE
        - AUTOSCALING_BACKOFF
        - AUTOSCALING_FAILED
        - AUTOSCALING_STATS_REPORT
        - CLUSTER_MIGRATED
        - CREATING
        - DBFS_DOWN
        - DECOMMISSION_ENDED
        - DECOMMISSION_STARTED
        - DID_NOT_EXPAND_DISK
        - DRIVER_HEALTHY
        - DRIVER_NOT_RESPONDING
        - DRIVER_UNAVAILABLE
        - EDITED
        - EXPANDED_DISK
        - FAILED_TO_EXPAND_DISK
        - INIT_SCRIPTS_FINISHED
        - INIT_SCRIPTS_STARTED
        - METASTORE_DOWN
        - NODES_LOST
        - NODE_BLACKLISTED
        - NODE_EXCLUDED_DECOMMISSIONED
        - PINNED
        - RESIZING
        - RESTARTING
        - RUNNING
        - SPARK_EXCEPTION
        - STARTING
        - TERMINATING
        - UC_VOLUME_MISCONFIGURED
        - UNPINNED
        - UPSIZE_COMPLETED
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    GcpAvailability:
      type: string
      enum:
        - ON_DEMAND_GCP
        - PREEMPTIBLE_GCP
        - PREEMPTIBLE_WITH_FALLBACK_GCP
      description: >-
        This field determines whether the instance pool will contain preemptible
        VMs, on-demand VMs, or

        preemptible VMs with a fallback to on-demand VMs if the former is
        unavailable.
    GetEventsOrder:
      type: string
      enum:
        - ASC
        - DESC
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    HardwareAcceleratorType:
      type: string
      enum:
        - GPU_1xA10
        - GPU_8xH100
      description: >-
        HardwareAcceleratorType: The type of hardware accelerator to use for
        compute workloads. NOTE:

        This enum is referenced and is intended to be used by other Databricks
        services that need to

        specify hardware accelerator requirements for AI compute workloads.
    InitScriptExecutionDetailsInitScriptExecutionStatus:
      type: string
      enum:
        - FAILED_EXECUTION
        - FAILED_FETCH
        - FUSE_MOUNT_FAILED
        - NOT_EXECUTED
        - SKIPPED
        - SUCCEEDED
        - UNKNOWN
      description: Result of attempted script execution
    InstancePoolAwsAttributesAvailability:
      type: string
      enum:
        - ON_DEMAND
        - SPOT
      description: >-
        The set of AWS availability types supported when setting up nodes for a
        cluster.
    InstancePoolAzureAttributesAvailability:
      type: string
      enum:
        - ON_DEMAND_AZURE
        - SPOT_AZURE
      description: >-
        The set of Azure availability types supported when setting up nodes for
        a cluster.
    InstancePoolPermissionLevel:
      type: string
      enum:
        - CAN_ATTACH_TO
        - CAN_MANAGE
      description: Permission level
    InstancePoolState:
      type: string
      enum:
        - ACTIVE
        - DELETED
        - STOPPED
      description: >-
        The state of a Cluster. The current allowable state transitions are as
        follows:


        - ``ACTIVE`` -> ``STOPPED`` - ``ACTIVE`` -> ``DELETED`` - ``STOPPED`` ->
        ``ACTIVE`` -

        ``STOPPED`` -> ``DELETED``
    Kind:
      type: string
      enum:
        - CLASSIC_PREVIEW
      description: >-
        The kind of compute described by this compute specification.


        Depending on `kind`, different validations and default values will be
        applied.


        Clusters with `kind = CLASSIC_PREVIEW` support the following fields,
        whereas clusters with no

        specified `kind` do not. *
        [is_single_node](/api/workspace/clusters/create#is_single_node) *

        [use_ml_runtime](/api/workspace/clusters/create#use_ml_runtime) *

        [data_security_mode](/api/workspace/clusters/create#data_security_mode)
        set to

        `DATA_SECURITY_MODE_AUTO`, `DATA_SECURITY_MODE_DEDICATED`, or
        `DATA_SECURITY_MODE_STANDARD`


        By using the [simple form], your clusters are automatically using `kind
        = CLASSIC_PREVIEW`.


        [simple form]: https://docs.databricks.com/compute/simple-form.html
    Language:
      type: string
      enum:
        - python
        - r
        - scala
        - sql
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    LibraryInstallStatus:
      type: string
      enum:
        - FAILED
        - INSTALLED
        - INSTALLING
        - PENDING
        - RESOLVING
        - RESTORED
        - SKIPPED
        - UNINSTALL_ON_RESTART
      description: The status of a library on a specific cluster.
    ListClustersSortByDirection:
      type: string
      enum:
        - ASC
        - DESC
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ListClustersSortByField:
      type: string
      enum:
        - CLUSTER_NAME
        - DEFAULT
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ListSortColumn:
      type: string
      enum:
        - POLICY_CREATION_TIME
        - POLICY_NAME
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ListSortOrder:
      type: string
      enum:
        - ASC
        - DESC
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ResultType:
      type: string
      enum:
        - error
        - image
        - images
        - table
        - text
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    RuntimeEngine:
      type: string
      enum:
        - 'NULL'
        - PHOTON
        - STANDARD
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    State:
      type: string
      enum:
        - ERROR
        - PENDING
        - RESIZING
        - RESTARTING
        - RUNNING
        - TERMINATED
        - TERMINATING
        - UNKNOWN
      description: >-
        The state of a Cluster. The current allowable state transitions are as
        follows:


        - `PENDING` -> `RUNNING` - `PENDING` -> `TERMINATING` - `RUNNING` ->
        `RESIZING` - `RUNNING` ->

        `RESTARTING` - `RUNNING` -> `TERMINATING` - `RESTARTING` -> `RUNNING` -
        `RESTARTING` ->

        `TERMINATING` - `RESIZING` -> `RUNNING` - `RESIZING` -> `TERMINATING` -
        `TERMINATING` ->

        `TERMINATED`
    TerminationReasonCode:
      type: string
      enum:
        - ABUSE_DETECTED
        - ACCESS_TOKEN_FAILURE
        - ALLOCATION_TIMEOUT
        - ALLOCATION_TIMEOUT_NODE_DAEMON_NOT_READY
        - ALLOCATION_TIMEOUT_NO_HEALTHY_AND_WARMED_UP_CLUSTERS
        - ALLOCATION_TIMEOUT_NO_HEALTHY_CLUSTERS
        - ALLOCATION_TIMEOUT_NO_MATCHED_CLUSTERS
        - ALLOCATION_TIMEOUT_NO_READY_CLUSTERS
        - ALLOCATION_TIMEOUT_NO_UNALLOCATED_CLUSTERS
        - ALLOCATION_TIMEOUT_NO_WARMED_UP_CLUSTERS
        - ATTACH_PROJECT_FAILURE
        - AWS_AUTHORIZATION_FAILURE
        - AWS_INACCESSIBLE_KMS_KEY_FAILURE
        - AWS_INSTANCE_PROFILE_UPDATE_FAILURE
        - AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE
        - AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE
        - AWS_INVALID_KEY_PAIR
        - AWS_INVALID_KMS_KEY_STATE
        - AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE
        - AWS_REQUEST_LIMIT_EXCEEDED
        - AWS_RESOURCE_QUOTA_EXCEEDED
        - AWS_UNSUPPORTED_FAILURE
        - AZURE_BYOK_KEY_PERMISSION_FAILURE
        - AZURE_EPHEMERAL_DISK_FAILURE
        - AZURE_INVALID_DEPLOYMENT_TEMPLATE
        - AZURE_OPERATION_NOT_ALLOWED_EXCEPTION
        - AZURE_PACKED_DEPLOYMENT_PARTIAL_FAILURE
        - AZURE_QUOTA_EXCEEDED_EXCEPTION
        - AZURE_RESOURCE_MANAGER_THROTTLING
        - AZURE_RESOURCE_PROVIDER_THROTTLING
        - AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE
        - AZURE_VM_EXTENSION_FAILURE
        - AZURE_VNET_CONFIGURATION_FAILURE
        - BOOTSTRAP_TIMEOUT
        - BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION
        - BOOTSTRAP_TIMEOUT_DUE_TO_MISCONFIG
        - BUDGET_POLICY_LIMIT_ENFORCEMENT_ACTIVATED
        - BUDGET_POLICY_RESOLUTION_FAILURE
        - CLOUD_ACCOUNT_POD_QUOTA_EXCEEDED
        - CLOUD_ACCOUNT_SETUP_FAILURE
        - CLOUD_OPERATION_CANCELLED
        - CLOUD_PROVIDER_DISK_SETUP_FAILURE
        - CLOUD_PROVIDER_INSTANCE_NOT_LAUNCHED
        - CLOUD_PROVIDER_LAUNCH_FAILURE
        - CLOUD_PROVIDER_LAUNCH_FAILURE_DUE_TO_MISCONFIG
        - CLOUD_PROVIDER_RESOURCE_STOCKOUT
        - CLOUD_PROVIDER_RESOURCE_STOCKOUT_DUE_TO_MISCONFIG
        - CLOUD_PROVIDER_SHUTDOWN
        - CLUSTER_OPERATION_THROTTLED
        - CLUSTER_OPERATION_TIMEOUT
        - COMMUNICATION_LOST
        - CONTAINER_LAUNCH_FAILURE
        - CONTROL_PLANE_CONNECTION_FAILURE
        - CONTROL_PLANE_CONNECTION_FAILURE_DUE_TO_MISCONFIG
        - CONTROL_PLANE_REQUEST_FAILURE
        - CONTROL_PLANE_REQUEST_FAILURE_DUE_TO_MISCONFIG
        - DATABASE_CONNECTION_FAILURE
        - DATA_ACCESS_CONFIG_CHANGED
        - DBFS_COMPONENT_UNHEALTHY
        - DBR_IMAGE_RESOLUTION_FAILURE
        - DISASTER_RECOVERY_REPLICATION
        - DNS_RESOLUTION_ERROR
        - DOCKER_CONTAINER_CREATION_EXCEPTION
        - DOCKER_IMAGE_PULL_FAILURE
        - DOCKER_IMAGE_TOO_LARGE_FOR_INSTANCE_EXCEPTION
        - DOCKER_INVALID_OS_EXCEPTION
        - DRIVER_EVICTION
        - DRIVER_LAUNCH_TIMEOUT
        - DRIVER_NODE_UNREACHABLE
        - DRIVER_OUT_OF_DISK
        - DRIVER_OUT_OF_MEMORY
        - DRIVER_POD_CREATION_FAILURE
        - DRIVER_UNEXPECTED_FAILURE
        - DRIVER_UNHEALTHY
        - DRIVER_UNREACHABLE
        - DRIVER_UNRESPONSIVE
        - DYNAMIC_SPARK_CONF_SIZE_EXCEEDED
        - EOS_SPARK_IMAGE
        - EXECUTION_COMPONENT_UNHEALTHY
        - EXECUTOR_POD_UNSCHEDULED
        - GCP_API_RATE_QUOTA_EXCEEDED
        - GCP_DENIED_BY_ORG_POLICY
        - GCP_FORBIDDEN
        - GCP_IAM_TIMEOUT
        - GCP_INACCESSIBLE_KMS_KEY_FAILURE
        - GCP_INSUFFICIENT_CAPACITY
        - GCP_IP_SPACE_EXHAUSTED
        - GCP_KMS_KEY_PERMISSION_DENIED
        - GCP_NOT_FOUND
        - GCP_QUOTA_EXCEEDED
        - GCP_RESOURCE_QUOTA_EXCEEDED
        - GCP_SERVICE_ACCOUNT_ACCESS_DENIED
        - GCP_SERVICE_ACCOUNT_DELETED
        - GCP_SERVICE_ACCOUNT_NOT_FOUND
        - GCP_SUBNET_NOT_READY
        - GCP_TRUSTED_IMAGE_PROJECTS_VIOLATED
        - GKE_BASED_CLUSTER_TERMINATION
        - GLOBAL_INIT_SCRIPT_FAILURE
        - HIVE_METASTORE_PROVISIONING_FAILURE
        - IMAGE_PULL_PERMISSION_DENIED
        - INACTIVITY
        - INIT_CONTAINER_NOT_FINISHED
        - INIT_SCRIPT_FAILURE
        - INSTANCE_POOL_CLUSTER_FAILURE
        - INSTANCE_POOL_MAX_CAPACITY_REACHED
        - INSTANCE_POOL_NOT_FOUND
        - INSTANCE_UNREACHABLE
        - INSTANCE_UNREACHABLE_DUE_TO_MISCONFIG
        - INTERNAL_CAPACITY_FAILURE
        - INTERNAL_ERROR
        - INVALID_ARGUMENT
        - INVALID_AWS_PARAMETER
        - INVALID_INSTANCE_PLACEMENT_PROTOCOL
        - INVALID_SPARK_IMAGE
        - INVALID_WORKER_IMAGE_FAILURE
        - IN_PENALTY_BOX
        - IP_EXHAUSTION_FAILURE
        - JOB_FINISHED
        - K8S_ACTIVE_POD_QUOTA_EXCEEDED
        - K8S_AUTOSCALING_FAILURE
        - K8S_DBR_CLUSTER_LAUNCH_TIMEOUT
        - LAZY_ALLOCATION_TIMEOUT
        - MAINTENANCE_MODE
        - METASTORE_COMPONENT_UNHEALTHY
        - MTLS_PORT_CONNECTIVITY_FAILURE
        - NEPHOS_RESOURCE_MANAGEMENT
        - NETVISOR_SETUP_TIMEOUT
        - NETWORK_CHECK_CONTROL_PLANE_FAILURE
        - NETWORK_CHECK_CONTROL_PLANE_FAILURE_DUE_TO_MISCONFIG
        - NETWORK_CHECK_DNS_SERVER_FAILURE
        - NETWORK_CHECK_DNS_SERVER_FAILURE_DUE_TO_MISCONFIG
        - NETWORK_CHECK_METADATA_ENDPOINT_FAILURE
        - NETWORK_CHECK_METADATA_ENDPOINT_FAILURE_DUE_TO_MISCONFIG
        - NETWORK_CHECK_MULTIPLE_COMPONENTS_FAILURE
        - NETWORK_CHECK_MULTIPLE_COMPONENTS_FAILURE_DUE_TO_MISCONFIG
        - NETWORK_CHECK_NIC_FAILURE
        - NETWORK_CHECK_NIC_FAILURE_DUE_TO_MISCONFIG
        - NETWORK_CHECK_STORAGE_FAILURE
        - NETWORK_CHECK_STORAGE_FAILURE_DUE_TO_MISCONFIG
        - NETWORK_CONFIGURATION_FAILURE
        - NFS_MOUNT_FAILURE
        - NO_MATCHED_K8S
        - NO_MATCHED_K8S_TESTING_TAG
        - NPIP_TUNNEL_SETUP_FAILURE
        - NPIP_TUNNEL_TOKEN_FAILURE
        - POD_ASSIGNMENT_FAILURE
        - POD_SCHEDULING_FAILURE
        - RATE_LIMITED
        - REQUEST_REJECTED
        - REQUEST_THROTTLED
        - RESOURCE_USAGE_BLOCKED
        - SECRET_CREATION_FAILURE
        - SECRET_PERMISSION_DENIED
        - SECRET_RESOLUTION_ERROR
        - SECURITY_DAEMON_REGISTRATION_EXCEPTION
        - SELF_BOOTSTRAP_FAILURE
        - SERVERLESS_LONG_RUNNING_TERMINATED
        - SKIPPED_SLOW_NODES
        - SLOW_IMAGE_DOWNLOAD
        - SPARK_ERROR
        - SPARK_IMAGE_DOWNLOAD_FAILURE
        - SPARK_IMAGE_DOWNLOAD_THROTTLED
        - SPARK_IMAGE_NOT_FOUND
        - SPARK_STARTUP_FAILURE
        - SPOT_INSTANCE_TERMINATION
        - SSH_BOOTSTRAP_FAILURE
        - STORAGE_DOWNLOAD_FAILURE
        - STORAGE_DOWNLOAD_FAILURE_DUE_TO_MISCONFIG
        - STORAGE_DOWNLOAD_FAILURE_SLOW
        - STORAGE_DOWNLOAD_FAILURE_THROTTLED
        - STS_CLIENT_SETUP_FAILURE
        - SUBNET_EXHAUSTED_FAILURE
        - TEMPORARILY_UNAVAILABLE
        - TRIAL_EXPIRED
        - UNEXPECTED_LAUNCH_FAILURE
        - UNEXPECTED_POD_RECREATION
        - UNKNOWN
        - UNSUPPORTED_INSTANCE_TYPE
        - UPDATE_INSTANCE_PROFILE_FAILURE
        - USAGE_POLICY_ENTITLEMENT_DENIED
        - USER_INITIATED_VM_TERMINATION
        - USER_REQUEST
        - WORKER_SETUP_FAILURE
        - WORKSPACE_CANCELLED_ERROR
        - WORKSPACE_CONFIGURATION_ERROR
        - WORKSPACE_UPDATE
      description: The status code indicating why the cluster was terminated
    TerminationReasonType:
      type: string
      enum:
        - CLIENT_ERROR
        - CLOUD_FAILURE
        - SERVICE_FAULT
        - SUCCESS
      description: type of the termination
  x-stackQL-resources:
    cluster_policies:
      id: databricks_workspace.compute.cluster_policies
      name: cluster_policies
      title: Cluster Policies
      methods:
        cluster_policies_create:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1create/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_delete:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1delete/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_edit:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1edit/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_get:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1get/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_get_permission_levels:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1cluster-policies~1{cluster_policy_id}~1permissionLevels/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_get_permissions:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1cluster-policies~1{cluster_policy_id}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_set_permissions:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1cluster-policies~1{cluster_policy_id}/put
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_update_permissions:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1cluster-policies~1{cluster_policy_id}/patch
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        cluster_policies_list:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1list/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_get_permission_levels
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_get_permissions
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_get
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_create
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_delete
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_edit
        update:
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_update_permissions
        delete: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/cluster_policies/methods/cluster_policies_set_permissions
    clusters:
      id: databricks_workspace.compute.clusters
      name: clusters
      title: Clusters
      methods:
        clusters_change_owner:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1change-owner/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_create:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1create/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_delete:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1delete/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_edit:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1edit/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_events:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1events/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_get:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1get/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_get_permission_levels:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1clusters~1{cluster_id}~1permissionLevels/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_get_permissions:
          operation:
            $ref: '#/paths/~1api~12.0~1permissions~1clusters~1{cluster_id}/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_set_permissions:
          operation:
            $ref: '#/paths/~1api~12.0~1permissions~1clusters~1{cluster_id}/put'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_update_permissions:
          operation:
            $ref: '#/paths/~1api~12.0~1permissions~1clusters~1{cluster_id}/patch'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_list:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1list/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_list_node_types:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1list-node-types/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_list_zones:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1list-zones/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_permanent_delete:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1permanent-delete/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_pin:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1pin/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_resize:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1resize/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_restart:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1restart/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_spark_versions:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1spark-versions/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_start:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1start/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_unpin:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1unpin/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        clusters_update:
          operation:
            $ref: '#/paths/~1api~12.1~1clusters~1update/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_get_permission_levels
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_get_permissions
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_get'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_list'
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_list_node_types
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_list_zones
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_spark_versions
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_change_owner
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_create'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_delete'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_edit'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_events'
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_permanent_delete
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_pin'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_resize'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_restart'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_start'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_unpin'
          - $ref: '#/components/x-stackQL-resources/clusters/methods/clusters_update'
        update:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_update_permissions
        delete: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/clusters_set_permissions
    command_execution:
      id: databricks_workspace.compute.command_execution
      name: command_execution
      title: Command Execution
      methods:
        command_execution_cancel:
          operation:
            $ref: '#/paths/~1api~11.2~1commands~1cancel/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        command_execution_command_status:
          operation:
            $ref: '#/paths/~1api~11.2~1commands~1status/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        command_execution_context_status:
          operation:
            $ref: '#/paths/~1api~11.2~1contexts~1status/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        command_execution_create:
          operation:
            $ref: '#/paths/~1api~11.2~1contexts~1create/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        command_execution_destroy:
          operation:
            $ref: '#/paths/~1api~11.2~1contexts~1destroy/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        command_execution_execute:
          operation:
            $ref: '#/paths/~1api~11.2~1commands~1execute/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/command_execution/methods/command_execution_command_status
          - $ref: >-
              #/components/x-stackQL-resources/command_execution/methods/command_execution_context_status
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/command_execution/methods/command_execution_cancel
          - $ref: >-
              #/components/x-stackQL-resources/command_execution/methods/command_execution_create
          - $ref: >-
              #/components/x-stackQL-resources/command_execution/methods/command_execution_destroy
          - $ref: >-
              #/components/x-stackQL-resources/command_execution/methods/command_execution_execute
        update: []
        delete: []
        replace: []
    global_init_scripts:
      id: databricks_workspace.compute.global_init_scripts
      name: global_init_scripts
      title: Global Init Scripts
      methods:
        global_init_scripts_create:
          operation:
            $ref: '#/paths/~1api~12.0~1global-init-scripts/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        global_init_scripts_list:
          operation:
            $ref: '#/paths/~1api~12.0~1global-init-scripts/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        global_init_scripts_delete:
          operation:
            $ref: '#/paths/~1api~12.0~1global-init-scripts~1{script_id}/delete'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        global_init_scripts_get:
          operation:
            $ref: '#/paths/~1api~12.0~1global-init-scripts~1{script_id}/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        global_init_scripts_update:
          operation:
            $ref: '#/paths/~1api~12.0~1global-init-scripts~1{script_id}/patch'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/global_init_scripts/methods/global_init_scripts_get
          - $ref: >-
              #/components/x-stackQL-resources/global_init_scripts/methods/global_init_scripts_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/global_init_scripts/methods/global_init_scripts_create
        update:
          - $ref: >-
              #/components/x-stackQL-resources/global_init_scripts/methods/global_init_scripts_update
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/global_init_scripts/methods/global_init_scripts_delete
        replace: []
    instance_pools:
      id: databricks_workspace.compute.instance_pools
      name: instance_pools
      title: Instance Pools
      methods:
        instance_pools_create:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-pools~1create/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_delete:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-pools~1delete/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_edit:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-pools~1edit/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_get:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-pools~1get/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_get_permission_levels:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1instance-pools~1{instance_pool_id}~1permissionLevels/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_get_permissions:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1instance-pools~1{instance_pool_id}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_set_permissions:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1instance-pools~1{instance_pool_id}/put
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_update_permissions:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1instance-pools~1{instance_pool_id}/patch
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_pools_list:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-pools~1list/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_get_permission_levels
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_get_permissions
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_get
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_create
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_delete
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_edit
        update:
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_update_permissions
        delete: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/instance_pools/methods/instance_pools_set_permissions
    instance_profiles:
      id: databricks_workspace.compute.instance_profiles
      name: instance_profiles
      title: Instance Profiles
      methods:
        instance_profiles_add:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-profiles~1add/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_profiles_edit:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-profiles~1edit/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_profiles_list:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-profiles~1list/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        instance_profiles_remove:
          operation:
            $ref: '#/paths/~1api~12.0~1instance-profiles~1remove/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/instance_profiles/methods/instance_profiles_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/instance_profiles/methods/instance_profiles_add
          - $ref: >-
              #/components/x-stackQL-resources/instance_profiles/methods/instance_profiles_edit
          - $ref: >-
              #/components/x-stackQL-resources/instance_profiles/methods/instance_profiles_remove
        update: []
        delete: []
        replace: []
    libraries:
      id: databricks_workspace.compute.libraries
      name: libraries
      title: Libraries
      methods:
        libraries_all_cluster_statuses:
          operation:
            $ref: '#/paths/~1api~12.0~1libraries~1all-cluster-statuses/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        libraries_cluster_status:
          operation:
            $ref: '#/paths/~1api~12.0~1libraries~1cluster-status/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        libraries_install:
          operation:
            $ref: '#/paths/~1api~12.0~1libraries~1install/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        libraries_uninstall:
          operation:
            $ref: '#/paths/~1api~12.0~1libraries~1uninstall/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/libraries/methods/libraries_all_cluster_statuses
          - $ref: >-
              #/components/x-stackQL-resources/libraries/methods/libraries_cluster_status
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/libraries/methods/libraries_install
          - $ref: >-
              #/components/x-stackQL-resources/libraries/methods/libraries_uninstall
        update: []
        delete: []
        replace: []
    policy_compliance_for_clusters:
      id: databricks_workspace.compute.policy_compliance_for_clusters
      name: policy_compliance_for_clusters
      title: Policy Compliance For Clusters
      methods:
        policy_compliance_for_clusters_enforce_compliance:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1enforce-compliance/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        policy_compliance_for_clusters_get_compliance:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1get-compliance/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        policy_compliance_for_clusters_list_compliance:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1clusters~1list-compliance/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/policy_compliance_for_clusters/methods/policy_compliance_for_clusters_get_compliance
          - $ref: >-
              #/components/x-stackQL-resources/policy_compliance_for_clusters/methods/policy_compliance_for_clusters_list_compliance
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/policy_compliance_for_clusters/methods/policy_compliance_for_clusters_enforce_compliance
        update: []
        delete: []
        replace: []
    policy_families:
      id: databricks_workspace.compute.policy_families
      name: policy_families
      title: Policy Families
      methods:
        policy_families_get:
          operation:
            $ref: '#/paths/~1api~12.0~1policy-families~1{policy_family_id}/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        policy_families_list:
          operation:
            $ref: '#/paths/~1api~12.0~1policy-families/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/policy_families/methods/policy_families_get
          - $ref: >-
              #/components/x-stackQL-resources/policy_families/methods/policy_families_list
        insert: []
        update: []
        delete: []
        replace: []
