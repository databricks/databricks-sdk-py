openapi: 3.0.0
info:
  title: Databricks Jobs API (workspace)
  description: >-
    OpenAPI specification for the Databricks jobs service (workspace-level
    APIs), generated from the Databricks Python SDK.
  version: 0.1.0
servers:
  url: https://{deployment_name}.cloud.databricks.com
  variables:
    deployment_name:
      description: The Databricks Workspace Deployment Name
      default: dbc-abcd0123-a1bc
paths:
  /api/2.2/jobs/runs/cancel-all:
    post:
      operationId: jobs_cancel_all_runs
      summary: >-
        Cancels all active runs of a job. The runs are canceled asynchronously,
        so it doesn't prevent new runs
      tags:
        - jobs
      description: >-
        Cancels all active runs of a job. The runs are canceled asynchronously,
        so it doesn't prevent new runs

        from being started.


        :param all_queued_runs: bool (optional)
          Optional boolean parameter to cancel all queued runs. If no job_id is provided, all queued runs in
          the workspace are canceled.
        :param job_id: int (optional)
          The canonical identifier of the job to cancel all runs of.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                all_queued_runs:
                  type: string
                  description: >-
                    Optional boolean parameter to cancel all queued runs. If no
                    job_id is provided, all queued runs in the workspace are
                    canceled.
                job_id:
                  type: string
                  description: The canonical identifier of the job to cancel all runs of.
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/cancel:
    post:
      operationId: jobs_cancel_run
      summary: >-
        Cancels a job run or a task run. The run is canceled asynchronously, so
        it may still be running when
      tags:
        - jobs
      description: >-
        Cancels a job run or a task run. The run is canceled asynchronously, so
        it may still be running when

        this request completes.


        :param run_id: int
          This field is required.

        :returns:
          Long-running operation waiter for :class:`Run`.
          See :method:wait_get_run_job_terminated_or_skipped for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                run_id:
                  type: integer
                  description: This field is required.
              required:
                - run_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[Run]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/create:
    post:
      operationId: jobs_create
      summary: Create a new job.
      tags:
        - jobs
      description: >-
        Create a new job.


        :param access_control_list: List[:class:`JobAccessControlRequest`]
        (optional)
          List of permissions to set on the job.
        :param budget_policy_id: str (optional)
          The id of the user specified budget policy to use for this job. If not specified, a default budget
          policy may be applied when creating or modifying the job. See `effective_budget_policy_id` for the
          budget policy used by this workload.
        :param continuous: :class:`Continuous` (optional)
          An optional continuous property for this job. The continuous property will ensure that there is
          always one run executing. Only one of `schedule` and `continuous` can be used.
        :param deployment: :class:`JobDeployment` (optional)
          Deployment information for jobs managed by external sources.
        :param description: str (optional)
          An optional description for the job. The maximum length is 27700 characters in UTF-8 encoding.
        :param edit_mode: :class:`JobEditMode` (optional)
          Edit mode of the job.

          * `UI_LOCKED`: The job is in a locked UI state and cannot be modified. * `EDITABLE`: The job is in
          an editable state and can be modified.
        :param email_notifications: :class:`JobEmailNotifications` (optional)
          An optional set of email addresses that is notified when runs of this job begin or complete as well
          as when this job is deleted.
        :param environments: List[:class:`JobEnvironment`] (optional)
          A list of task execution environment specifications that can be referenced by serverless tasks of
          this job. For serverless notebook tasks, if the environment_key is not specified, the notebook
          environment will be used if present. If a jobs environment is specified, it will override the
          notebook environment. For other serverless tasks, the task environment is required to be specified
          using environment_key in the task settings.
        :param format: :class:`Format` (optional)
          Used to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When
          using the Jobs API 2.1 this value is always set to `"MULTI_TASK"`.
        :param git_source: :class:`GitSource` (optional)
          An optional specification for a remote Git repository containing the source code used by tasks.
          Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.

          If `git_source` is set, these tasks retrieve the file from the remote repository by default.
          However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.

          Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are
          used, `git_source` must be defined on the job.
        :param health: :class:`JobsHealthRules` (optional)

        :param job_clusters: List[:class:`JobCluster`] (optional)
          A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries
          cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.
        :param max_concurrent_runs: int (optional)
          An optional maximum allowed number of concurrent runs of the job. Set this value if you want to be
          able to execute multiple runs of the same job concurrently. This is useful for example if you
          trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each
          other, or if you want to trigger multiple runs which differ by their input parameters. This setting
          affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent
          active runs. Then setting the concurrency to 3 won’t kill any of the active runs. However, from
          then on, new runs are skipped unless there are fewer than 3 active runs. This value cannot exceed
          1000. Setting this value to `0` causes all new runs to be skipped.
        :param name: str (optional)
          An optional name for the job. The maximum length is 4096 bytes in UTF-8 encoding.
        :param notification_settings: :class:`JobNotificationSettings`
        (optional)
          Optional notification settings that are used when sending notifications to each of the
          `email_notifications` and `webhook_notifications` for this job.
        :param parameters: List[:class:`JobParameterDefinition`] (optional)
          Job-level parameter definitions
        :param performance_target: :class:`PerformanceTarget` (optional)
          The performance mode on a serverless job. This field determines the level of compute performance or
          cost-efficiency for the run. The performance target does not apply to tasks that run on Serverless
          GPU compute.

          * `STANDARD`: Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:
          Prioritizes fast startup and execution times through rapid scaling and optimized cluster
          performance.
        :param queue: :class:`QueueSettings` (optional)
          The queue settings of the job.
        :param run_as: :class:`JobRunAs` (optional)
          The user or service principal that the job runs as, if specified in the request. This field
          indicates the explicit configuration of `run_as` for the job. To find the value in all cases,
          explicit or implicit, use `run_as_user_name`.
        :param schedule: :class:`CronSchedule` (optional)
          An optional periodic schedule for this job. The default behavior is that the job only runs when
          triggered by clicking “Run Now” in the Jobs UI or sending an API request to `runNow`.
        :param tags: Dict[str,str] (optional)
          A map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs
          clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added
          to the job.
        :param tasks: List[:class:`Task`] (optional)
          A list of task specifications to be executed by this job. It supports up to 1000 elements in write
          endpoints (:method:jobs/create, :method:jobs/reset, :method:jobs/update, :method:jobs/submit). Read
          endpoints return only 100 tasks. If more than 100 tasks are available, you can paginate through them
          using :method:jobs/get. Use the `next_page_token` field at the object root to determine if more
          results are available.
        :param timeout_seconds: int (optional)
          An optional timeout applied to each run of this job. A value of `0` means no timeout.
        :param trigger: :class:`TriggerSettings` (optional)
          A configuration to trigger a run when certain conditions are met. The default behavior is that the
          job runs only when triggered by clicking “Run Now” in the Jobs UI or sending an API request to
          `runNow`.
        :param usage_policy_id: str (optional)
          The id of the user specified usage policy to use for this job. If not specified, a default usage
          policy may be applied when creating or modifying the job. See `effective_usage_policy_id` for the
          usage policy used by this workload.
        :param webhook_notifications: :class:`WebhookNotifications` (optional)
          A collection of system notification IDs to notify when runs of this job begin or complete.

        :returns: :class:`CreateResponse`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: List of permissions to set on the job.
                budget_policy_id:
                  type: string
                  description: >-
                    The id of the user specified budget policy to use for this
                    job. If not specified, a default budget policy may be
                    applied when creating or modifying the job. See
                    `effective_budget_policy_id` for the budget policy used by
                    this workload.
                continuous:
                  type: string
                  description: >-
                    An optional continuous property for this job. The continuous
                    property will ensure that there is always one run executing.
                    Only one of `schedule` and `continuous` can be used.
                deployment:
                  type: string
                  description: Deployment information for jobs managed by external sources.
                description:
                  type: string
                  description: >-
                    An optional description for the job. The maximum length is
                    27700 characters in UTF-8 encoding.
                edit_mode:
                  type: string
                  description: >-
                    Edit mode of the job. * `UI_LOCKED`: The job is in a locked
                    UI state and cannot be modified. * `EDITABLE`: The job is in
                    an editable state and can be modified.
                email_notifications:
                  type: string
                  description: >-
                    An optional set of email addresses that is notified when
                    runs of this job begin or complete as well as when this job
                    is deleted.
                environments:
                  type: string
                  description: >-
                    A list of task execution environment specifications that can
                    be referenced by serverless tasks of this job. For
                    serverless notebook tasks, if the environment_key is not
                    specified, the notebook environment will be used if present.
                    If a jobs environment is specified, it will override the
                    notebook environment. For other serverless tasks, the task
                    environment is required to be specified using
                    environment_key in the task settings.
                format:
                  type: string
                  description: >-
                    Used to tell what is the format of the job. This field is
                    ignored in Create/Update/Reset calls. When using the Jobs
                    API 2.1 this value is always set to `"MULTI_TASK"`.
                git_source:
                  type: string
                  description: >-
                    An optional specification for a remote Git repository
                    containing the source code used by tasks. Version-controlled
                    source code is supported by notebook, dbt, Python script,
                    and SQL File tasks. If `git_source` is set, these tasks
                    retrieve the file from the remote repository by default.
                    However, this behavior can be overridden by setting `source`
                    to `WORKSPACE` on the task. Note: dbt and SQL File tasks
                    support only version-controlled sources. If dbt or SQL File
                    tasks are used, `git_source` must be defined on the job.
                health:
                  type: string
                  description: >-
                    :param job_clusters: List[:class:`JobCluster`] (optional) A
                    list of job cluster specifications that can be shared and
                    reused by tasks of this job. Libraries cannot be declared in
                    a shared job cluster. You must declare dependent libraries
                    in task settings.
                job_clusters:
                  type: string
                max_concurrent_runs:
                  type: string
                  description: >-
                    An optional maximum allowed number of concurrent runs of the
                    job. Set this value if you want to be able to execute
                    multiple runs of the same job concurrently. This is useful
                    for example if you trigger your job on a frequent schedule
                    and want to allow consecutive runs to overlap with each
                    other, or if you want to trigger multiple runs which differ
                    by their input parameters. This setting affects only new
                    runs. For example, suppose the job’s concurrency is 4 and
                    there are 4 concurrent active runs. Then setting the
                    concurrency to 3 won’t kill any of the active runs. However,
                    from then on, new runs are skipped unless there are fewer
                    than 3 active runs. This value cannot exceed 1000. Setting
                    this value to `0` causes all new runs to be skipped.
                name:
                  type: string
                  description: >-
                    An optional name for the job. The maximum length is 4096
                    bytes in UTF-8 encoding.
                notification_settings:
                  type: string
                  description: >-
                    Optional notification settings that are used when sending
                    notifications to each of the `email_notifications` and
                    `webhook_notifications` for this job.
                parameters:
                  type: string
                  description: Job-level parameter definitions
                performance_target:
                  type: string
                  description: >-
                    The performance mode on a serverless job. This field
                    determines the level of compute performance or
                    cost-efficiency for the run. The performance target does not
                    apply to tasks that run on Serverless GPU compute. *
                    `STANDARD`: Enables cost-efficient execution of serverless
                    workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast
                    startup and execution times through rapid scaling and
                    optimized cluster performance.
                queue:
                  type: string
                  description: The queue settings of the job.
                run_as:
                  type: string
                  description: >-
                    The user or service principal that the job runs as, if
                    specified in the request. This field indicates the explicit
                    configuration of `run_as` for the job. To find the value in
                    all cases, explicit or implicit, use `run_as_user_name`.
                schedule:
                  type: string
                  description: >-
                    An optional periodic schedule for this job. The default
                    behavior is that the job only runs when triggered by
                    clicking “Run Now” in the Jobs UI or sending an API request
                    to `runNow`.
                tags:
                  type: string
                  description: >-
                    A map of tags associated with the job. These are forwarded
                    to the cluster as cluster tags for jobs clusters, and are
                    subject to the same limitations as cluster tags. A maximum
                    of 25 tags can be added to the job.
                tasks:
                  type: string
                  description: >-
                    A list of task specifications to be executed by this job. It
                    supports up to 1000 elements in write endpoints
                    (:method:jobs/create, :method:jobs/reset,
                    :method:jobs/update, :method:jobs/submit). Read endpoints
                    return only 100 tasks. If more than 100 tasks are available,
                    you can paginate through them using :method:jobs/get. Use
                    the `next_page_token` field at the object root to determine
                    if more results are available.
                timeout_seconds:
                  type: string
                  description: >-
                    An optional timeout applied to each run of this job. A value
                    of `0` means no timeout.
                trigger:
                  type: string
                  description: >-
                    A configuration to trigger a run when certain conditions are
                    met. The default behavior is that the job runs only when
                    triggered by clicking “Run Now” in the Jobs UI or sending an
                    API request to `runNow`.
                usage_policy_id:
                  type: string
                  description: >-
                    The id of the user specified usage policy to use for this
                    job. If not specified, a default usage policy may be applied
                    when creating or modifying the job. See
                    `effective_usage_policy_id` for the usage policy used by
                    this workload.
                webhook_notifications:
                  type: string
                  description: >-
                    A collection of system notification IDs to notify when runs
                    of this job begin or complete.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/delete:
    post:
      operationId: jobs_delete
      summary: Deletes a job.
      tags:
        - jobs
      description: |-
        Deletes a job.

        :param job_id: int
          The canonical identifier of the job to delete. This field is required.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: >-
                    The canonical identifier of the job to delete. This field is
                    required.
              required:
                - job_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/delete:
    post:
      operationId: jobs_delete_run
      summary: Deletes a non-active run. Returns an error if the run is active.
      tags:
        - jobs
      description: |-
        Deletes a non-active run. Returns an error if the run is active.

        :param run_id: int
          ID of the run to delete.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                run_id:
                  type: integer
                  description: ID of the run to delete.
              required:
                - run_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/export:
    get:
      operationId: jobs_export_run
      summary: Export and retrieve the job run task.
      tags:
        - jobs
      description: |-
        Export and retrieve the job run task.

        :param run_id: int
          The canonical identifier for the run. This field is required.
        :param views_to_export: :class:`ViewsToExport` (optional)
          Which views to export (CODE, DASHBOARDS, or ALL). Defaults to CODE.

        :returns: :class:`ExportRunOutput`
      parameters:
        - name: run_id
          in: query
          required: true
          schema:
            type: integer
          description: The canonical identifier for the run. This field is required.
        - name: views_to_export
          in: query
          required: false
          schema:
            type: string
          description: Which views to export (CODE, DASHBOARDS, or ALL). Defaults to CODE.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExportRunOutput'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/get:
    get:
      operationId: jobs_get
      summary: Retrieves the details for a single job.
      tags:
        - jobs
      description: >-
        Retrieves the details for a single job.


        Large arrays in the results will be paginated when they exceed 100
        elements. A request for a single

        job will return all properties for that job, and the first 100 elements
        of array properties (`tasks`,

        `job_clusters`, `environments` and `parameters`). Use the
        `next_page_token` field to check for more

        results and pass its value as the `page_token` in subsequent requests.
        If any array properties have

        more than 100 elements, additional results will be returned on
        subsequent requests. Arrays without

        additional results will be empty on later pages.


        :param job_id: int
          The canonical identifier of the job to retrieve information about. This field is required.
        :param page_token: str (optional)
          Use `next_page_token` returned from the previous GetJob response to request the next page of the
          job's array properties.

        :returns: :class:`Job`
      parameters:
        - name: job_id
          in: query
          required: true
          schema:
            type: integer
          description: >-
            The canonical identifier of the job to retrieve information about.
            This field is required.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use `next_page_token` returned from the previous GetJob response to
            request the next page of the job's array properties.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Job'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/jobs/{job_id}/permissionLevels:
    get:
      operationId: jobs_get_permission_levels
      summary: Gets the permission levels that a user can have on an object.
      tags:
        - jobs
      description: |-
        Gets the permission levels that a user can have on an object.

        :param job_id: str
          The job for which to get or manage permissions.

        :returns: :class:`GetJobPermissionLevelsResponse`
      parameters:
        - name: job_id
          in: path
          required: true
          schema:
            type: string
          description: The job for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetJobPermissionLevelsResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/permissions/jobs/{job_id}:
    get:
      operationId: jobs_get_permissions
      summary: >-
        Gets the permissions of a job. Jobs can inherit permissions from their
        root object.
      tags:
        - jobs
      description: >-
        Gets the permissions of a job. Jobs can inherit permissions from their
        root object.


        :param job_id: str
          The job for which to get or manage permissions.

        :returns: :class:`JobPermissions`
      parameters:
        - name: job_id
          in: path
          required: true
          schema:
            type: string
          description: The job for which to get or manage permissions.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    put:
      operationId: jobs_set_permissions
      summary: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct
      tags:
        - jobs
      description: >-
        Sets permissions on an object, replacing existing permissions if they
        exist. Deletes all direct

        permissions if none are specified. Objects can inherit permissions from
        their root object.


        :param job_id: str
          The job for which to get or manage permissions.
        :param access_control_list: List[:class:`JobAccessControlRequest`]
        (optional)


        :returns: :class:`JobPermissions`
      parameters:
        - name: job_id
          in: path
          required: true
          schema:
            type: string
          description: The job for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`JobPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
    patch:
      operationId: jobs_update_permissions
      summary: >-
        Updates the permissions on a job. Jobs can inherit permissions from
        their root object.
      tags:
        - jobs
      description: >-
        Updates the permissions on a job. Jobs can inherit permissions from
        their root object.


        :param job_id: str
          The job for which to get or manage permissions.
        :param access_control_list: List[:class:`JobAccessControlRequest`]
        (optional)


        :returns: :class:`JobPermissions`
      parameters:
        - name: job_id
          in: path
          required: true
          schema:
            type: string
          description: The job for which to get or manage permissions.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: ':returns: :class:`JobPermissions`'
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobPermissions'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/get:
    get:
      operationId: jobs_get_run
      summary: Retrieves the metadata of a run.
      tags:
        - jobs
      description: >-
        Retrieves the metadata of a run.


        Large arrays in the results will be paginated when they exceed 100
        elements. A request for a single

        run will return all properties for that run, and the first 100 elements
        of array properties (`tasks`,

        `job_clusters`, `job_parameters` and `repair_history`). Use the
        next_page_token field to check for

        more results and pass its value as the page_token in subsequent
        requests. If any array properties have

        more than 100 elements, additional results will be returned on
        subsequent requests. Arrays without

        additional results will be empty on later pages.


        :param run_id: int
          The canonical identifier of the run for which to retrieve the metadata. This field is required.
        :param include_history: bool (optional)
          Whether to include the repair history in the response.
        :param include_resolved_values: bool (optional)
          Whether to include resolved parameter values in the response.
        :param page_token: str (optional)
          Use `next_page_token` returned from the previous GetRun response to request the next page of the
          run's array properties.

        :returns: :class:`Run`
      parameters:
        - name: run_id
          in: query
          required: true
          schema:
            type: integer
          description: >-
            The canonical identifier of the run for which to retrieve the
            metadata. This field is required.
        - name: include_history
          in: query
          required: false
          schema:
            type: string
          description: Whether to include the repair history in the response.
        - name: include_resolved_values
          in: query
          required: false
          schema:
            type: string
          description: Whether to include resolved parameter values in the response.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use `next_page_token` returned from the previous GetRun response to
            request the next page of the run's array properties.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Run'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/get-output:
    get:
      operationId: jobs_get_run_output
      summary: >-
        Retrieve the output and metadata of a single task run. When a notebook
        task returns a value through
      tags:
        - jobs
      description: >-
        Retrieve the output and metadata of a single task run. When a notebook
        task returns a value through

        the `dbutils.notebook.exit()` call, you can use this endpoint to
        retrieve that value. Databricks

        restricts this API to returning the first 5 MB of the output. To return
        a larger result, you can store

        job results in a cloud storage service.


        This endpoint validates that the __run_id__ parameter is valid and
        returns an HTTP status code 400 if

        the __run_id__ parameter is invalid. Runs are automatically removed
        after 60 days. If you to want to

        reference them beyond 60 days, you must save old run results before they
        expire.


        :param run_id: int
          The canonical identifier for the run.

        :returns: :class:`RunOutput`
      parameters:
        - name: run_id
          in: query
          required: true
          schema:
            type: integer
          description: The canonical identifier for the run.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunOutput'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/list:
    get:
      operationId: jobs_list
      summary: Retrieves a list of jobs.
      tags:
        - jobs
      description: |-
        Retrieves a list of jobs.

        :param expand_tasks: bool (optional)
          Whether to include task and cluster details in the response. Note that only the first 100 elements
          will be shown. Use :method:jobs/get to paginate through all tasks and clusters.
        :param limit: int (optional)
          The number of jobs to return. This value must be greater than 0 and less or equal to 100. The
          default value is 20.
        :param name: str (optional)
          A filter on the list based on the exact (case insensitive) job name.
        :param offset: int (optional)
          The offset of the first job to return, relative to the most recently created job. Deprecated since
          June 2023. Use `page_token` to iterate through the pages instead.
        :param page_token: str (optional)
          Use `next_page_token` or `prev_page_token` returned from the previous request to list the next or
          previous page of jobs respectively.

        :returns: Iterator over :class:`BaseJob`
      parameters:
        - name: expand_tasks
          in: query
          required: false
          schema:
            type: string
          description: >-
            Whether to include task and cluster details in the response. Note
            that only the first 100 elements will be shown. Use :method:jobs/get
            to paginate through all tasks and clusters.
        - name: limit
          in: query
          required: false
          schema:
            type: string
          description: >-
            The number of jobs to return. This value must be greater than 0 and
            less or equal to 100. The default value is 20.
        - name: name
          in: query
          required: false
          schema:
            type: string
          description: A filter on the list based on the exact (case insensitive) job name.
        - name: offset
          in: query
          required: false
          schema:
            type: string
          description: >-
            The offset of the first job to return, relative to the most recently
            created job. Deprecated since June 2023. Use `page_token` to iterate
            through the pages instead.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use `next_page_token` or `prev_page_token` returned from the
            previous request to list the next or previous page of jobs
            respectively.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[BaseJob]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/list:
    get:
      operationId: jobs_list_runs
      summary: List runs in descending order by start time.
      tags:
        - jobs
      description: |-
        List runs in descending order by start time.

        :param active_only: bool (optional)
          If active_only is `true`, only active runs are included in the results; otherwise, lists both active
          and completed runs. An active run is a run in the `QUEUED`, `PENDING`, `RUNNING`, or `TERMINATING`.
          This field cannot be `true` when completed_only is `true`.
        :param completed_only: bool (optional)
          If completed_only is `true`, only completed runs are included in the results; otherwise, lists both
          active and completed runs. This field cannot be `true` when active_only is `true`.
        :param expand_tasks: bool (optional)
          Whether to include task and cluster details in the response. Note that only the first 100 elements
          will be shown. Use :method:jobs/getrun to paginate through all tasks and clusters.
        :param job_id: int (optional)
          The job for which to list runs. If omitted, the Jobs service lists runs from all jobs.
        :param limit: int (optional)
          The number of runs to return. This value must be greater than 0 and less than 25. The default value
          is 20. If a request specifies a limit of 0, the service instead uses the maximum limit.
        :param offset: int (optional)
          The offset of the first run to return, relative to the most recent run. Deprecated since June 2023.
          Use `page_token` to iterate through the pages instead.
        :param page_token: str (optional)
          Use `next_page_token` or `prev_page_token` returned from the previous request to list the next or
          previous page of runs respectively.
        :param run_type: :class:`RunType` (optional)
          The type of runs to return. For a description of run types, see :method:jobs/getRun.
        :param start_time_from: int (optional)
          Show runs that started _at or after_ this value. The value must be a UTC timestamp in milliseconds.
          Can be combined with _start_time_to_ to filter by a time range.
        :param start_time_to: int (optional)
          Show runs that started _at or before_ this value. The value must be a UTC timestamp in milliseconds.
          Can be combined with _start_time_from_ to filter by a time range.

        :returns: Iterator over :class:`BaseRun`
      parameters:
        - name: active_only
          in: query
          required: false
          schema:
            type: string
          description: >-
            If active_only is `true`, only active runs are included in the
            results; otherwise, lists both active and completed runs. An active
            run is a run in the `QUEUED`, `PENDING`, `RUNNING`, or
            `TERMINATING`. This field cannot be `true` when completed_only is
            `true`.
        - name: completed_only
          in: query
          required: false
          schema:
            type: string
          description: >-
            If completed_only is `true`, only completed runs are included in the
            results; otherwise, lists both active and completed runs. This field
            cannot be `true` when active_only is `true`.
        - name: expand_tasks
          in: query
          required: false
          schema:
            type: string
          description: >-
            Whether to include task and cluster details in the response. Note
            that only the first 100 elements will be shown. Use
            :method:jobs/getrun to paginate through all tasks and clusters.
        - name: job_id
          in: query
          required: false
          schema:
            type: string
          description: >-
            The job for which to list runs. If omitted, the Jobs service lists
            runs from all jobs.
        - name: limit
          in: query
          required: false
          schema:
            type: string
          description: >-
            The number of runs to return. This value must be greater than 0 and
            less than 25. The default value is 20. If a request specifies a
            limit of 0, the service instead uses the maximum limit.
        - name: offset
          in: query
          required: false
          schema:
            type: string
          description: >-
            The offset of the first run to return, relative to the most recent
            run. Deprecated since June 2023. Use `page_token` to iterate through
            the pages instead.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use `next_page_token` or `prev_page_token` returned from the
            previous request to list the next or previous page of runs
            respectively.
        - name: run_type
          in: query
          required: false
          schema:
            type: string
          description: >-
            The type of runs to return. For a description of run types, see
            :method:jobs/getRun.
        - name: start_time_from
          in: query
          required: false
          schema:
            type: string
          description: >-
            Show runs that started _at or after_ this value. The value must be a
            UTC timestamp in milliseconds. Can be combined with _start_time_to_
            to filter by a time range.
        - name: start_time_to
          in: query
          required: false
          schema:
            type: string
          description: >-
            Show runs that started _at or before_ this value. The value must be
            a UTC timestamp in milliseconds. Can be combined with
            _start_time_from_ to filter by a time range.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[BaseRun]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/repair:
    post:
      operationId: jobs_repair_run
      summary: >-
        Re-run one or more tasks. Tasks are re-run as part of the original job
        run. They use the current job
      tags:
        - jobs
      description: >-
        Re-run one or more tasks. Tasks are re-run as part of the original job
        run. They use the current job

        and task settings, and can be viewed in the history for the original job
        run.


        :param run_id: int
          The job run ID of the run to repair. The run must not be in progress.
        :param dbt_commands: List[str] (optional)
          An array of commands to execute for jobs with the dbt task, for example `"dbt_commands": ["dbt
          deps", "dbt seed", "dbt deps", "dbt seed", "dbt run"]`

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param jar_params: List[str] (optional)
          A list of parameters for jobs with Spark JAR tasks, for example `"jar_params": ["john doe", "35"]`.
          The parameters are used to invoke the main function of the main class specified in the Spark JAR
          task. If not specified upon `run-now`, it defaults to an empty list. jar_params cannot be specified
          in conjunction with notebook_params. The JSON representation of this field (for example
          `{"jar_params":["john doe","35"]}`) cannot exceed 10,000 bytes.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param job_parameters: Dict[str,str] (optional)
          Job-level parameters used in the run. for example `"param": "overriding_val"`
        :param latest_repair_id: int (optional)
          The ID of the latest repair. This parameter is not required when repairing a run for the first time,
          but must be provided on subsequent requests to repair the same run.
        :param notebook_params: Dict[str,str] (optional)
          A map from keys to values for jobs with notebook task, for example `"notebook_params": {"name":
          "john doe", "age": "35"}`. The map is passed to the notebook and is accessible through the
          [dbutils.widgets.get] function.

          If not specified upon `run-now`, the triggered run uses the job’s base parameters.

          notebook_params cannot be specified in conjunction with jar_params.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          The JSON representation of this field (for example `{"notebook_params":{"name":"john
          doe","age":"35"}}`) cannot exceed 10,000 bytes.

          [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param performance_target: :class:`PerformanceTarget` (optional)
          The performance mode on a serverless job. The performance target determines the level of compute
          performance or cost-efficiency for the run. This field overrides the performance target defined on
          the job level.

          * `STANDARD`: Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:
          Prioritizes fast startup and execution times through rapid scaling and optimized cluster
          performance.
        :param pipeline_params: :class:`PipelineParams` (optional)
          Controls whether the pipeline should perform a full refresh
        :param python_named_params: Dict[str,str] (optional)

        :param python_params: List[str] (optional)
          A list of parameters for jobs with Python tasks, for example `"python_params": ["john doe", "35"]`.
          The parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it
          would overwrite the parameters specified in job setting. The JSON representation of this field (for
          example `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          Important

          These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters
          returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and
          emojis.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param rerun_all_failed_tasks: bool (optional)
          If true, repair all failed tasks. Only one of `rerun_tasks` or `rerun_all_failed_tasks` can be used.
        :param rerun_dependent_tasks: bool (optional)
          If true, repair all tasks that depend on the tasks in `rerun_tasks`, even if they were previously
          successful. Can be also used in combination with `rerun_all_failed_tasks`.
        :param rerun_tasks: List[str] (optional)
          The task keys of the task runs to repair.
        :param spark_submit_params: List[str] (optional)
          A list of parameters for jobs with spark submit task, for example `"spark_submit_params":
          ["--class", "org.apache.spark.examples.SparkPi"]`. The parameters are passed to spark-submit script
          as command-line parameters. If specified upon `run-now`, it would overwrite the parameters specified
          in job setting. The JSON representation of this field (for example `{"python_params":["john
          doe","35"]}`) cannot exceed 10,000 bytes.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          Important

          These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters
          returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and
          emojis.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param sql_params: Dict[str,str] (optional)
          A map from keys to values for jobs with SQL task, for example `"sql_params": {"name": "john doe",
          "age": "35"}`. The SQL alert task does not support custom parameters.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown

        :returns:
          Long-running operation waiter for :class:`Run`.
          See :method:wait_get_run_job_terminated_or_skipped for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                run_id:
                  type: integer
                  description: >-
                    The job run ID of the run to repair. The run must not be in
                    progress.
                dbt_commands:
                  type: string
                  description: >-
                    An array of commands to execute for jobs with the dbt task,
                    for example `"dbt_commands": ["dbt deps", "dbt seed", "dbt
                    deps", "dbt seed", "dbt run"]` ⚠ **Deprecation note** Use
                    [job parameters] to pass information down to tasks. [job
                    parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                jar_params:
                  type: string
                  description: >-
                    A list of parameters for jobs with Spark JAR tasks, for
                    example `"jar_params": ["john doe", "35"]`. The parameters
                    are used to invoke the main function of the main class
                    specified in the Spark JAR task. If not specified upon
                    `run-now`, it defaults to an empty list. jar_params cannot
                    be specified in conjunction with notebook_params. The JSON
                    representation of this field (for example
                    `{"jar_params":["john doe","35"]}`) cannot exceed 10,000
                    bytes. ⚠ **Deprecation note** Use [job parameters] to pass
                    information down to tasks. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                job_parameters:
                  type: string
                  description: >-
                    Job-level parameters used in the run. for example `"param":
                    "overriding_val"`
                latest_repair_id:
                  type: string
                  description: >-
                    The ID of the latest repair. This parameter is not required
                    when repairing a run for the first time, but must be
                    provided on subsequent requests to repair the same run.
                notebook_params:
                  type: string
                  description: >-
                    A map from keys to values for jobs with notebook task, for
                    example `"notebook_params": {"name": "john doe", "age":
                    "35"}`. The map is passed to the notebook and is accessible
                    through the [dbutils.widgets.get] function. If not specified
                    upon `run-now`, the triggered run uses the job’s base
                    parameters. notebook_params cannot be specified in
                    conjunction with jar_params. ⚠ **Deprecation note** Use [job
                    parameters] to pass information down to tasks. The JSON
                    representation of this field (for example
                    `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot
                    exceed 10,000 bytes. [dbutils.widgets.get]:
                    https://docs.databricks.com/dev-tools/databricks-utils.html
                    [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                performance_target:
                  type: string
                  description: >-
                    The performance mode on a serverless job. The performance
                    target determines the level of compute performance or
                    cost-efficiency for the run. This field overrides the
                    performance target defined on the job level. * `STANDARD`:
                    Enables cost-efficient execution of serverless workloads. *
                    `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and
                    execution times through rapid scaling and optimized cluster
                    performance.
                pipeline_params:
                  type: string
                  description: Controls whether the pipeline should perform a full refresh
                python_named_params:
                  type: string
                  description: >-
                    :param python_params: List[str] (optional) A list of
                    parameters for jobs with Python tasks, for example
                    `"python_params": ["john doe", "35"]`. The parameters are
                    passed to Python file as command-line parameters. If
                    specified upon `run-now`, it would overwrite the parameters
                    specified in job setting. The JSON representation of this
                    field (for example `{"python_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job
                    parameters] to pass information down to tasks. Important
                    These parameters accept only Latin characters (ASCII
                    character set). Using non-ASCII characters returns an error.
                    Examples of invalid, non-ASCII characters are Chinese,
                    Japanese kanjis, and emojis. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                python_params:
                  type: string
                rerun_all_failed_tasks:
                  type: string
                  description: >-
                    If true, repair all failed tasks. Only one of `rerun_tasks`
                    or `rerun_all_failed_tasks` can be used.
                rerun_dependent_tasks:
                  type: string
                  description: >-
                    If true, repair all tasks that depend on the tasks in
                    `rerun_tasks`, even if they were previously successful. Can
                    be also used in combination with `rerun_all_failed_tasks`.
                rerun_tasks:
                  type: string
                  description: The task keys of the task runs to repair.
                spark_submit_params:
                  type: string
                  description: >-
                    A list of parameters for jobs with spark submit task, for
                    example `"spark_submit_params": ["--class",
                    "org.apache.spark.examples.SparkPi"]`. The parameters are
                    passed to spark-submit script as command-line parameters. If
                    specified upon `run-now`, it would overwrite the parameters
                    specified in job setting. The JSON representation of this
                    field (for example `{"python_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job
                    parameters] to pass information down to tasks. Important
                    These parameters accept only Latin characters (ASCII
                    character set). Using non-ASCII characters returns an error.
                    Examples of invalid, non-ASCII characters are Chinese,
                    Japanese kanjis, and emojis. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                sql_params:
                  type: string
                  description: >-
                    A map from keys to values for jobs with SQL task, for
                    example `"sql_params": {"name": "john doe", "age": "35"}`.
                    The SQL alert task does not support custom parameters. ⚠
                    **Deprecation note** Use [job parameters] to pass
                    information down to tasks. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
              required:
                - run_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[Run]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/reset:
    post:
      operationId: jobs_reset
      summary: >-
        Overwrite all settings for the given job. Use the [_Update_
        endpoint](:method:jobs/update) to update
      tags:
        - jobs
      description: >-
        Overwrite all settings for the given job. Use the [_Update_
        endpoint](:method:jobs/update) to update

        job settings partially.


        :param job_id: int
          The canonical identifier of the job to reset. This field is required.
        :param new_settings: :class:`JobSettings`
          The new settings of the job. These settings completely replace the old settings.

          Changes to the field `JobBaseSettings.timeout_seconds` are applied to active runs. Changes to other
          fields are applied to future runs only.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: >-
                    The canonical identifier of the job to reset. This field is
                    required.
                new_settings:
                  type: string
                  description: >-
                    The new settings of the job. These settings completely
                    replace the old settings. Changes to the field
                    `JobBaseSettings.timeout_seconds` are applied to active
                    runs. Changes to other fields are applied to future runs
                    only.
              required:
                - job_id
                - new_settings
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/run-now:
    post:
      operationId: jobs_run_now
      summary: Run a job and return the `run_id` of the triggered run.
      tags:
        - jobs
      description: |-
        Run a job and return the `run_id` of the triggered run.

        :param job_id: int
          The ID of the job to be executed
        :param dbt_commands: List[str] (optional)
          An array of commands to execute for jobs with the dbt task, for example `"dbt_commands": ["dbt
          deps", "dbt seed", "dbt deps", "dbt seed", "dbt run"]`

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param idempotency_token: str (optional)
          An optional token to guarantee the idempotency of job run requests. If a run with the provided token
          already exists, the request does not create a new run but returns the ID of the existing run
          instead. If a run with the provided token is deleted, an error is returned.

          If you specify the idempotency token, upon failure you can retry until the request succeeds.
          Databricks guarantees that exactly one run is launched with that idempotency token.

          This token must have at most 64 characters.

          For more information, see [How to ensure idempotency for jobs].

          [How to ensure idempotency for jobs]: https://kb.databricks.com/jobs/jobs-idempotency.html
        :param jar_params: List[str] (optional)
          A list of parameters for jobs with Spark JAR tasks, for example `"jar_params": ["john doe", "35"]`.
          The parameters are used to invoke the main function of the main class specified in the Spark JAR
          task. If not specified upon `run-now`, it defaults to an empty list. jar_params cannot be specified
          in conjunction with notebook_params. The JSON representation of this field (for example
          `{"jar_params":["john doe","35"]}`) cannot exceed 10,000 bytes.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param job_parameters: Dict[str,str] (optional)
          Job-level parameters used in the run. for example `"param": "overriding_val"`
        :param notebook_params: Dict[str,str] (optional)
          A map from keys to values for jobs with notebook task, for example `"notebook_params": {"name":
          "john doe", "age": "35"}`. The map is passed to the notebook and is accessible through the
          [dbutils.widgets.get] function.

          If not specified upon `run-now`, the triggered run uses the job’s base parameters.

          notebook_params cannot be specified in conjunction with jar_params.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          The JSON representation of this field (for example `{"notebook_params":{"name":"john
          doe","age":"35"}}`) cannot exceed 10,000 bytes.

          [dbutils.widgets.get]: https://docs.databricks.com/dev-tools/databricks-utils.html
          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param only: List[str] (optional)
          A list of task keys to run inside of the job. If this field is not provided, all tasks in the job
          will be run.
        :param performance_target: :class:`PerformanceTarget` (optional)
          The performance mode on a serverless job. The performance target determines the level of compute
          performance or cost-efficiency for the run. This field overrides the performance target defined on
          the job level.

          * `STANDARD`: Enables cost-efficient execution of serverless workloads. * `PERFORMANCE_OPTIMIZED`:
          Prioritizes fast startup and execution times through rapid scaling and optimized cluster
          performance.
        :param pipeline_params: :class:`PipelineParams` (optional)
          Controls whether the pipeline should perform a full refresh
        :param python_named_params: Dict[str,str] (optional)
        :param python_params: List[str] (optional)
          A list of parameters for jobs with Python tasks, for example `"python_params": ["john doe", "35"]`.
          The parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it
          would overwrite the parameters specified in job setting. The JSON representation of this field (for
          example `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          Important

          These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters
          returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and
          emojis.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param queue: :class:`QueueSettings` (optional)
          The queue settings of the run.
        :param spark_submit_params: List[str] (optional)
          A list of parameters for jobs with spark submit task, for example `"spark_submit_params":
          ["--class", "org.apache.spark.examples.SparkPi"]`. The parameters are passed to spark-submit script
          as command-line parameters. If specified upon `run-now`, it would overwrite the parameters specified
          in job setting. The JSON representation of this field (for example `{"python_params":["john
          doe","35"]}`) cannot exceed 10,000 bytes.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          Important

          These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters
          returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and
          emojis.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        :param sql_params: Dict[str,str] (optional)
          A map from keys to values for jobs with SQL task, for example `"sql_params": {"name": "john doe",
          "age": "35"}`. The SQL alert task does not support custom parameters.

          ⚠ **Deprecation note** Use [job parameters] to pass information down to tasks.

          [job parameters]: https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown

        :returns:
          Long-running operation waiter for :class:`Run`.
          See :method:wait_get_run_job_terminated_or_skipped for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: The ID of the job to be executed
                dbt_commands:
                  type: string
                  description: >-
                    An array of commands to execute for jobs with the dbt task,
                    for example `"dbt_commands": ["dbt deps", "dbt seed", "dbt
                    deps", "dbt seed", "dbt run"]` ⚠ **Deprecation note** Use
                    [job parameters] to pass information down to tasks. [job
                    parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                idempotency_token:
                  type: string
                  description: >-
                    An optional token to guarantee the idempotency of job run
                    requests. If a run with the provided token already exists,
                    the request does not create a new run but returns the ID of
                    the existing run instead. If a run with the provided token
                    is deleted, an error is returned. If you specify the
                    idempotency token, upon failure you can retry until the
                    request succeeds. Databricks guarantees that exactly one run
                    is launched with that idempotency token. This token must
                    have at most 64 characters. For more information, see [How
                    to ensure idempotency for jobs]. [How to ensure idempotency
                    for jobs]:
                    https://kb.databricks.com/jobs/jobs-idempotency.html
                jar_params:
                  type: string
                  description: >-
                    A list of parameters for jobs with Spark JAR tasks, for
                    example `"jar_params": ["john doe", "35"]`. The parameters
                    are used to invoke the main function of the main class
                    specified in the Spark JAR task. If not specified upon
                    `run-now`, it defaults to an empty list. jar_params cannot
                    be specified in conjunction with notebook_params. The JSON
                    representation of this field (for example
                    `{"jar_params":["john doe","35"]}`) cannot exceed 10,000
                    bytes. ⚠ **Deprecation note** Use [job parameters] to pass
                    information down to tasks. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                job_parameters:
                  type: string
                  description: >-
                    Job-level parameters used in the run. for example `"param":
                    "overriding_val"`
                notebook_params:
                  type: string
                  description: >-
                    A map from keys to values for jobs with notebook task, for
                    example `"notebook_params": {"name": "john doe", "age":
                    "35"}`. The map is passed to the notebook and is accessible
                    through the [dbutils.widgets.get] function. If not specified
                    upon `run-now`, the triggered run uses the job’s base
                    parameters. notebook_params cannot be specified in
                    conjunction with jar_params. ⚠ **Deprecation note** Use [job
                    parameters] to pass information down to tasks. The JSON
                    representation of this field (for example
                    `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot
                    exceed 10,000 bytes. [dbutils.widgets.get]:
                    https://docs.databricks.com/dev-tools/databricks-utils.html
                    [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                only:
                  type: string
                  description: >-
                    A list of task keys to run inside of the job. If this field
                    is not provided, all tasks in the job will be run.
                performance_target:
                  type: string
                  description: >-
                    The performance mode on a serverless job. The performance
                    target determines the level of compute performance or
                    cost-efficiency for the run. This field overrides the
                    performance target defined on the job level. * `STANDARD`:
                    Enables cost-efficient execution of serverless workloads. *
                    `PERFORMANCE_OPTIMIZED`: Prioritizes fast startup and
                    execution times through rapid scaling and optimized cluster
                    performance.
                pipeline_params:
                  type: string
                  description: Controls whether the pipeline should perform a full refresh
                python_named_params:
                  type: string
                  description: >-
                    :param python_params: List[str] (optional) A list of
                    parameters for jobs with Python tasks, for example
                    `"python_params": ["john doe", "35"]`. The parameters are
                    passed to Python file as command-line parameters. If
                    specified upon `run-now`, it would overwrite the parameters
                    specified in job setting. The JSON representation of this
                    field (for example `{"python_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job
                    parameters] to pass information down to tasks. Important
                    These parameters accept only Latin characters (ASCII
                    character set). Using non-ASCII characters returns an error.
                    Examples of invalid, non-ASCII characters are Chinese,
                    Japanese kanjis, and emojis. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                python_params:
                  type: string
                queue:
                  type: string
                  description: The queue settings of the run.
                spark_submit_params:
                  type: string
                  description: >-
                    A list of parameters for jobs with spark submit task, for
                    example `"spark_submit_params": ["--class",
                    "org.apache.spark.examples.SparkPi"]`. The parameters are
                    passed to spark-submit script as command-line parameters. If
                    specified upon `run-now`, it would overwrite the parameters
                    specified in job setting. The JSON representation of this
                    field (for example `{"python_params":["john doe","35"]}`)
                    cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use [job
                    parameters] to pass information down to tasks. Important
                    These parameters accept only Latin characters (ASCII
                    character set). Using non-ASCII characters returns an error.
                    Examples of invalid, non-ASCII characters are Chinese,
                    Japanese kanjis, and emojis. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
                sql_params:
                  type: string
                  description: >-
                    A map from keys to values for jobs with SQL task, for
                    example `"sql_params": {"name": "john doe", "age": "35"}`.
                    The SQL alert task does not support custom parameters. ⚠
                    **Deprecation note** Use [job parameters] to pass
                    information down to tasks. [job parameters]:
                    https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
              required:
                - job_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[Run]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/runs/submit:
    post:
      operationId: jobs_submit
      summary: >-
        Submit a one-time run. This endpoint allows you to submit a workload
        directly without creating a job.
      tags:
        - jobs
      description: >-
        Submit a one-time run. This endpoint allows you to submit a workload
        directly without creating a job.

        Runs submitted using this endpoint don’t display in the UI. Use the
        `jobs/runs/get` API to check the

        run state after the job is submitted.


        **Important:** Jobs submitted using this endpoint are not saved as a
        job. They do not show up in the

        Jobs UI, and do not retry when they fail. Because they are not saved,
        Databricks cannot auto-optimize

        serverless compute in case of failure. If your job fails, you may want
        to use classic compute to

        specify the compute needs for the job. Alternatively, use the `POST
        /jobs/create` and `POST

        /jobs/run-now` endpoints to create and run a saved job.


        :param access_control_list: List[:class:`JobAccessControlRequest`]
        (optional)
          List of permissions to set on the job.
        :param budget_policy_id: str (optional)
          The user specified id of the budget policy to use for this one-time run. If not specified, the run
          will be not be attributed to any budget policy.
        :param email_notifications: :class:`JobEmailNotifications` (optional)
          An optional set of email addresses notified when the run begins or completes.
        :param environments: List[:class:`JobEnvironment`] (optional)
          A list of task execution environment specifications that can be referenced by tasks of this run.
        :param git_source: :class:`GitSource` (optional)
          An optional specification for a remote Git repository containing the source code used by tasks.
          Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.

          If `git_source` is set, these tasks retrieve the file from the remote repository by default.
          However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.

          Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are
          used, `git_source` must be defined on the job.
        :param health: :class:`JobsHealthRules` (optional)

        :param idempotency_token: str (optional)
          An optional token that can be used to guarantee the idempotency of job run requests. If a run with
          the provided token already exists, the request does not create a new run but returns the ID of the
          existing run instead. If a run with the provided token is deleted, an error is returned.

          If you specify the idempotency token, upon failure you can retry until the request succeeds.
          Databricks guarantees that exactly one run is launched with that idempotency token.

          This token must have at most 64 characters.

          For more information, see [How to ensure idempotency for jobs].

          [How to ensure idempotency for jobs]: https://kb.databricks.com/jobs/jobs-idempotency.html
        :param notification_settings: :class:`JobNotificationSettings`
        (optional)
          Optional notification settings that are used when sending notifications to each of the
          `email_notifications` and `webhook_notifications` for this run.
        :param queue: :class:`QueueSettings` (optional)
          The queue settings of the one-time run.
        :param run_as: :class:`JobRunAs` (optional)
          Specifies the user or service principal that the job runs as. If not specified, the job runs as the
          user who submits the request.
        :param run_name: str (optional)
          An optional name for the run. The default value is `Untitled`.
        :param tasks: List[:class:`SubmitTask`] (optional)

        :param timeout_seconds: int (optional)
          An optional timeout applied to each run of this job. A value of `0` means no timeout.
        :param usage_policy_id: str (optional)
          The user specified id of the usage policy to use for this one-time run. If not specified, a default
          usage policy may be applied when creating or modifying the job.
        :param webhook_notifications: :class:`WebhookNotifications` (optional)
          A collection of system notification IDs to notify when the run begins or completes.

        :returns:
          Long-running operation waiter for :class:`Run`.
          See :method:wait_get_run_job_terminated_or_skipped for more details.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                access_control_list:
                  type: string
                  description: List of permissions to set on the job.
                budget_policy_id:
                  type: string
                  description: >-
                    The user specified id of the budget policy to use for this
                    one-time run. If not specified, the run will be not be
                    attributed to any budget policy.
                email_notifications:
                  type: string
                  description: >-
                    An optional set of email addresses notified when the run
                    begins or completes.
                environments:
                  type: string
                  description: >-
                    A list of task execution environment specifications that can
                    be referenced by tasks of this run.
                git_source:
                  type: string
                  description: >-
                    An optional specification for a remote Git repository
                    containing the source code used by tasks. Version-controlled
                    source code is supported by notebook, dbt, Python script,
                    and SQL File tasks. If `git_source` is set, these tasks
                    retrieve the file from the remote repository by default.
                    However, this behavior can be overridden by setting `source`
                    to `WORKSPACE` on the task. Note: dbt and SQL File tasks
                    support only version-controlled sources. If dbt or SQL File
                    tasks are used, `git_source` must be defined on the job.
                health:
                  type: string
                  description: >-
                    :param idempotency_token: str (optional) An optional token
                    that can be used to guarantee the idempotency of job run
                    requests. If a run with the provided token already exists,
                    the request does not create a new run but returns the ID of
                    the existing run instead. If a run with the provided token
                    is deleted, an error is returned. If you specify the
                    idempotency token, upon failure you can retry until the
                    request succeeds. Databricks guarantees that exactly one run
                    is launched with that idempotency token. This token must
                    have at most 64 characters. For more information, see [How
                    to ensure idempotency for jobs]. [How to ensure idempotency
                    for jobs]:
                    https://kb.databricks.com/jobs/jobs-idempotency.html
                idempotency_token:
                  type: string
                notification_settings:
                  type: string
                  description: >-
                    Optional notification settings that are used when sending
                    notifications to each of the `email_notifications` and
                    `webhook_notifications` for this run.
                queue:
                  type: string
                  description: The queue settings of the one-time run.
                run_as:
                  type: string
                  description: >-
                    Specifies the user or service principal that the job runs
                    as. If not specified, the job runs as the user who submits
                    the request.
                run_name:
                  type: string
                  description: >-
                    An optional name for the run. The default value is
                    `Untitled`.
                tasks:
                  type: string
                  description: >-
                    :param timeout_seconds: int (optional) An optional timeout
                    applied to each run of this job. A value of `0` means no
                    timeout.
                timeout_seconds:
                  type: string
                usage_policy_id:
                  type: string
                  description: >-
                    The user specified id of the usage policy to use for this
                    one-time run. If not specified, a default usage policy may
                    be applied when creating or modifying the job.
                webhook_notifications:
                  type: string
                  description: >-
                    A collection of system notification IDs to notify when the
                    run begins or completes.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Wait[Run]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.2/jobs/update:
    post:
      operationId: jobs_update
      summary: >-
        Add, update, or remove specific settings of an existing job. Use the
        [_Reset_
      tags:
        - jobs
      description: >-
        Add, update, or remove specific settings of an existing job. Use the
        [_Reset_

        endpoint](:method:jobs/reset) to overwrite all job settings.


        :param job_id: int
          The canonical identifier of the job to update. This field is required.
        :param fields_to_remove: List[str] (optional)
          Remove top-level fields in the job settings. Removing nested fields is not supported, except for
          tasks and job clusters (`tasks/task_1`). This field is optional.
        :param new_settings: :class:`JobSettings` (optional)
          The new settings for the job.

          Top-level fields specified in `new_settings` are completely replaced, except for arrays which are
          merged. That is, new and existing entries are completely replaced based on the respective key
          fields, i.e. `task_key` or `job_cluster_key`, while previous entries are kept.

          Partially updating nested fields is not supported.

          Changes to the field `JobSettings.timeout_seconds` are applied to active runs. Changes to other
          fields are applied to future runs only.
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: >-
                    The canonical identifier of the job to update. This field is
                    required.
                fields_to_remove:
                  type: string
                  description: >-
                    Remove top-level fields in the job settings. Removing nested
                    fields is not supported, except for tasks and job clusters
                    (`tasks/task_1`). This field is optional.
                new_settings:
                  type: string
                  description: >-
                    The new settings for the job. Top-level fields specified in
                    `new_settings` are completely replaced, except for arrays
                    which are merged. That is, new and existing entries are
                    completely replaced based on the respective key fields, i.e.
                    `task_key` or `job_cluster_key`, while previous entries are
                    kept. Partially updating nested fields is not supported.
                    Changes to the field `JobSettings.timeout_seconds` are
                    applied to active runs. Changes to other fields are applied
                    to future runs only.
              required:
                - job_id
      responses:
        '200':
          description: Success
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/jobs/enforce-compliance:
    post:
      operationId: policy_compliance_for_jobs_enforce_compliance
      summary: >-
        Updates a job so the job clusters that are created when running the job
        (specified in `new_cluster`)
      tags:
        - jobs
        - policy_compliance_for_jobs
      description: >-
        Updates a job so the job clusters that are created when running the job
        (specified in `new_cluster`)

        are compliant with the current versions of their respective cluster
        policies. All-purpose clusters

        used in the job will not be updated.


        :param job_id: int
          The ID of the job you want to enforce policy compliance on.
        :param validate_only: bool (optional)
          If set, previews changes made to the job to comply with its policy, but does not update the job.

        :returns: :class:`EnforcePolicyComplianceResponse`
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                job_id:
                  type: integer
                  description: The ID of the job you want to enforce policy compliance on.
                validate_only:
                  type: string
                  description: >-
                    If set, previews changes made to the job to comply with its
                    policy, but does not update the job.
              required:
                - job_id
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/EnforcePolicyComplianceResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/jobs/get-compliance:
    get:
      operationId: policy_compliance_for_jobs_get_compliance
      summary: >-
        Returns the policy compliance status of a job. Jobs could be out of
        compliance if a cluster policy
      tags:
        - jobs
        - policy_compliance_for_jobs
      description: >-
        Returns the policy compliance status of a job. Jobs could be out of
        compliance if a cluster policy

        they use was updated after the job was last edited and some of its job
        clusters no longer comply with

        their updated policies.


        :param job_id: int
          The ID of the job whose compliance status you are requesting.

        :returns: :class:`GetPolicyComplianceResponse`
      parameters:
        - name: job_id
          in: query
          required: true
          schema:
            type: integer
          description: The ID of the job whose compliance status you are requesting.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GetPolicyComplianceResponse'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
  /api/2.0/policies/jobs/list-compliance:
    get:
      operationId: policy_compliance_for_jobs_list_compliance
      summary: >-
        Returns the policy compliance status of all jobs that use a given
        policy. Jobs could be out of
      tags:
        - jobs
        - policy_compliance_for_jobs
      description: >-
        Returns the policy compliance status of all jobs that use a given
        policy. Jobs could be out of

        compliance if a cluster policy they use was updated after the job was
        last edited and its job clusters

        no longer comply with the updated policy.


        :param policy_id: str
          Canonical unique identifier for the cluster policy.
        :param page_size: int (optional)
          Use this field to specify the maximum number of results to be returned by the server. The server may
          further constrain the maximum number of results returned in a single page.
        :param page_token: str (optional)
          A page token that can be used to navigate to the next page or previous page as returned by
          `next_page_token` or `prev_page_token`.

        :returns: Iterator over :class:`JobCompliance`
      parameters:
        - name: policy_id
          in: query
          required: true
          schema:
            type: string
          description: Canonical unique identifier for the cluster policy.
        - name: page_size
          in: query
          required: false
          schema:
            type: string
          description: >-
            Use this field to specify the maximum number of results to be
            returned by the server. The server may further constrain the maximum
            number of results returned in a single page.
        - name: page_token
          in: query
          required: false
          schema:
            type: string
          description: >-
            A page token that can be used to navigate to the next page or
            previous page as returned by `next_page_token` or `prev_page_token`.
      responses:
        '200':
          description: Success
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Iterator[JobCompliance]'
        default:
          description: Error response
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                  message:
                    type: string
components:
  schemas:
    BaseJob:
      type: object
      properties:
        created_time:
          type: integer
        creator_user_name:
          type: string
          description: >-
            The creator user name. This field won’t be included in the response
            if the user has already been deleted.
        effective_budget_policy_id:
          type: string
          description: >-
            The id of the budget policy used by this job for cost attribution
            purposes. This may be set through (in order of precedence): 1.
            Budget admins through the account or workspace console 2. Jobs UI in
            the job details page and Jobs API using `budget_policy_id` 3.
            Inferred default based on accessible budget policies of the run_as
            identity on job creation or modification.
        effective_usage_policy_id:
          type: string
          description: >-
            The id of the usage policy used by this job for cost attribution
            purposes.
        has_more:
          type: boolean
          description: >-
            Indicates if the job has more array properties (`tasks`,
            `job_clusters`) that are not shown. They can be accessed via
            :method:jobs/get endpoint. It is only relevant for API 2.2
            :method:jobs/list requests with `expand_tasks=true`.
        job_id:
          type: integer
          description: The canonical identifier for this job.
        settings:
          $ref: '#/components/schemas/JobSettings'
          description: >-
            Settings for this job and all of its runs. These settings can be
            updated using the `resetJob` method.
        trigger_state:
          $ref: '#/components/schemas/TriggerStateProto'
          description: State of the trigger associated with the job.
    BaseRun:
      type: object
      properties:
        attempt_number:
          type: integer
        cleanup_duration:
          type: integer
          description: >-
            The time in milliseconds it took to terminate the cluster and clean
            up any associated artifacts. The duration of a task run is the sum
            of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `cleanup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the
            value of the `run_duration` field.
        cluster_instance:
          $ref: '#/components/schemas/ClusterInstance'
          description: >-
            The cluster used for this run. If the run is specified to use a new
            cluster, this field is set once the Jobs service has requested a
            cluster for the run.
        cluster_spec:
          $ref: '#/components/schemas/ClusterSpec'
          description: >-
            A snapshot of the job’s cluster specification when this run was
            created.
        creator_user_name:
          type: string
          description: >-
            The creator user name. This field won’t be included in the response
            if the user has already been deleted.
        description:
          type: string
          description: Description of the run
        effective_performance_target:
          $ref: '#/components/schemas/PerformanceTarget'
          description: >-
            The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on
            the request depending on whether the performance mode is supported
            by the job type. * `STANDARD`: Enables cost-efficient execution of
            serverless workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast
            startup and execution times through rapid scaling and optimized
            cluster performance.
        effective_usage_policy_id:
          type: string
          description: >-
            The id of the usage policy used by this run for cost attribution
            purposes.
        end_time:
          type: integer
          description: >-
            The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still
            running.
        execution_duration:
          type: integer
          description: >-
            The time in milliseconds it took to execute the commands in the JAR
            or notebook until they completed, failed, timed out, were cancelled,
            or encountered an unexpected error. The duration of a task run is
            the sum of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `execution_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the
            value of the `run_duration` field.
        git_source:
          $ref: '#/components/schemas/GitSource'
          description: >-
            An optional specification for a remote Git repository containing the
            source code used by tasks. Version-controlled source code is
            supported by notebook, dbt, Python script, and SQL File tasks. If
            `git_source` is set, these tasks retrieve the file from the remote
            repository by default. However, this behavior can be overridden by
            setting `source` to `WORKSPACE` on the task. Note: dbt and SQL File
            tasks support only version-controlled sources. If dbt or SQL File
            tasks are used, `git_source` must be defined on the job.
        has_more:
          type: boolean
          description: >-
            Indicates if the run has more array properties (`tasks`,
            `job_clusters`) that are not shown. They can be accessed via
            :method:jobs/getrun endpoint. It is only relevant for API 2.2
            :method:jobs/listruns requests with `expand_tasks=true`.
        job_clusters:
          type: array
          items:
            $ref: '#/components/schemas/JobCluster'
          description: >-
            A list of job cluster specifications that can be shared and reused
            by tasks of this job. Libraries cannot be declared in a shared job
            cluster. You must declare dependent libraries in task settings. If
            more than 100 job clusters are available, you can paginate through
            them using :method:jobs/getrun.
        job_id:
          type: integer
          description: The canonical identifier of the job that contains this run.
        job_parameters:
          type: array
          items:
            $ref: '#/components/schemas/JobParameter'
          description: Job-level parameters used in the run
        job_run_id:
          type: integer
          description: >-
            ID of the job run that this run belongs to. For legacy and
            single-task job runs the field is populated with the job run ID. For
            task runs, the field is populated with the ID of the job run that
            the task run belongs to.
        number_in_job:
          type: integer
          description: >-
            A unique identifier for this job run. This is set to the same value
            as `run_id`.
        original_attempt_run_id:
          type: integer
          description: >-
            If this run is a retry of a prior run attempt, this field contains
            the run_id of the original attempt; otherwise, it is the same as the
            run_id.
        overriding_parameters:
          $ref: '#/components/schemas/RunParameters'
          description: The parameters used for this run.
        queue_duration:
          type: integer
          description: The time in milliseconds that the run has spent in the queue.
        repair_history:
          type: array
          items:
            $ref: '#/components/schemas/RepairHistoryItem'
          description: The repair history of the run.
        run_duration:
          type: integer
          description: >-
            The time in milliseconds it took the job run and all of its repairs
            to finish.
        run_id:
          type: integer
          description: >-
            The canonical identifier of the run. This ID is unique across all
            runs of all jobs.
        run_name:
          type: string
          description: >-
            An optional name for the run. The maximum length is 4096 bytes in
            UTF-8 encoding.
        run_page_url:
          type: string
          description: The URL to the detail page of the run.
        run_type:
          $ref: '#/components/schemas/RunType'
        schedule:
          $ref: '#/components/schemas/CronSchedule'
          description: >-
            The cron schedule that triggered this run if it was triggered by the
            periodic scheduler.
        setup_duration:
          type: integer
          description: >-
            The time in milliseconds it took to set up the cluster. For runs
            that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The
            duration of a task run is the sum of the `setup_duration`,
            `execution_duration`, and the `cleanup_duration`. The
            `setup_duration` field is set to 0 for multitask job runs. The total
            duration of a multitask job run is the value of the `run_duration`
            field.
        start_time:
          type: integer
          description: >-
            The time at which this run was started in epoch milliseconds
            (milliseconds since 1/1/1970 UTC). This may not be the time when the
            job task starts executing, for example, if the job is scheduled to
            run on a new cluster, this is the time the cluster creation call is
            issued.
        state:
          $ref: '#/components/schemas/RunState'
          description: Deprecated. Please use the `status` field instead.
        status:
          $ref: '#/components/schemas/RunStatus'
        tasks:
          type: array
          items:
            $ref: '#/components/schemas/RunTask'
          description: >-
            The list of tasks performed by the run. Each task has its own
            `run_id` which you can use to call `JobsGetOutput` to retrieve the
            run resutls. If more than 100 tasks are available, you can paginate
            through them using :method:jobs/getrun. Use the `next_page_token`
            field at the object root to determine if more results are available.
        trigger:
          $ref: '#/components/schemas/TriggerType'
        trigger_info:
          $ref: '#/components/schemas/TriggerInfo'
    CleanRoomTaskRunState:
      type: object
      properties:
        life_cycle_state:
          $ref: '#/components/schemas/CleanRoomTaskRunLifeCycleState'
          description: >-
            A value indicating the run's current lifecycle state. This field is
            always available in the response. Note: Additional states might be
            introduced in future releases.
        result_state:
          $ref: '#/components/schemas/CleanRoomTaskRunResultState'
          description: >-
            A value indicating the run's result. This field is only available
            for terminal lifecycle states. Note: Additional states might be
            introduced in future releases.
      description: Stores the run state of the clean rooms notebook task.
    CleanRoomsNotebookTask:
      type: object
      properties:
        clean_room_name:
          type: string
          description: The clean room that the notebook belongs to.
        notebook_name:
          type: string
          description: Name of the notebook being run.
        etag:
          type: string
          description: >-
            Checksum to validate the freshness of the notebook resource (i.e.
            the notebook being run is the latest version). It can be fetched by
            calling the :method:cleanroomassets/get API.
        notebook_base_parameters:
          type: object
          description: Base parameters to be used for the clean room notebook job.
      required:
        - clean_room_name
        - notebook_name
      description: >-
        Clean Rooms notebook task for V1 Clean Room service (GA). Replaces the
        deprecated
            CleanRoomNotebookTask (defined above) which was for V0 service.
    CleanRoomsNotebookTaskCleanRoomsNotebookTaskOutput:
      type: object
      properties:
        clean_room_job_run_state:
          $ref: '#/components/schemas/CleanRoomTaskRunState'
        notebook_output:
          $ref: '#/components/schemas/NotebookOutput'
          description: The notebook output for the clean room run
        output_schema_info:
          $ref: '#/components/schemas/OutputSchemaInfo'
          description: >-
            Information on how to access the output schema for the clean room
            run
    ClusterInstance:
      type: object
      properties:
        cluster_id:
          type: string
        spark_context_id:
          type: string
          description: >-
            The canonical identifier for the Spark context used by a run. This
            field is filled in once the run begins execution. This value can be
            used to view the Spark UI by browsing to
            `/#setting/sparkui/$cluster_id/$spark_context_id`. The Spark UI
            continues to be available after the run has completed. The response
            won’t include this field if the identifier is not available yet.
    ClusterSpec:
      type: object
      properties:
        existing_cluster_id:
          type: string
        job_cluster_key:
          type: string
          description: >-
            If job_cluster_key, this task is executed reusing the cluster
            specified in `job.settings.job_clusters`.
        libraries:
          type: string
          description: >-
            An optional list of libraries to be installed on the cluster. The
            default value is an empty list.
        new_cluster:
          type: string
          description: >-
            If new_cluster, a description of a new cluster that is created for
            each run.
    Compute:
      type: object
      properties:
        hardware_accelerator:
          type: string
    ComputeConfig:
      type: object
      properties:
        num_gpus:
          type: integer
        gpu_node_pool_id:
          type: string
          description: IDof the GPU pool to use.
        gpu_type:
          type: string
          description: GPU type.
      required:
        - num_gpus
    ConditionTask:
      type: object
      properties:
        op:
          $ref: '#/components/schemas/ConditionTaskOp'
        left:
          type: string
          description: >-
            The left operand of the condition task. Can be either a string value
            or a job state or parameter reference.
        right:
          type: string
          description: >-
            The right operand of the condition task. Can be either a string
            value or a job state or parameter reference.
      required:
        - op
        - left
        - right
    Continuous:
      type: object
      properties:
        pause_status:
          $ref: '#/components/schemas/PauseStatus'
        task_retry_mode:
          $ref: '#/components/schemas/TaskRetryMode'
          description: >-
            Indicate whether the continuous job is applying task level retries
            or not. Defaults to NEVER.
    CreateResponse:
      type: object
      properties:
        job_id:
          type: integer
          description: The canonical identifier for the newly created job.
      description: Job was created successfully
    CronSchedule:
      type: object
      properties:
        quartz_cron_expression:
          type: string
        timezone_id:
          type: string
          description: >-
            A Java timezone ID. The schedule for a job is resolved with respect
            to this timezone. See [Java TimeZone] for details. This field is
            required. [Java TimeZone]:
            https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html
        pause_status:
          $ref: '#/components/schemas/PauseStatus'
          description: Indicate whether this schedule is paused or not.
      required:
        - quartz_cron_expression
        - timezone_id
    DashboardPageSnapshot:
      type: object
      properties:
        page_display_name:
          type: string
        widget_error_details:
          type: array
          items:
            $ref: '#/components/schemas/WidgetErrorDetail'
    DashboardTask:
      type: object
      properties:
        dashboard_id:
          type: string
          description: The identifier of the dashboard to refresh.
        filters:
          type: object
          description: >-
            Dashboard task parameters. Used to apply dashboard filter values
            during dashboard task execution. Parameter values get applied to any
            dashboard filters that have a matching URL identifier as the
            parameter key. The parameter value format is dependent on the filter
            type: - For text and single-select filters, provide a single value
            (e.g. `"value"`) - For date and datetime filters, provide the value
            in ISO 8601 format (e.g. `"2000-01-01T00:00:00"`) - For multi-select
            filters, provide a JSON array of values (e.g.
            `"[\"value1\",\"value2\"]"`) - For range and date range filters,
            provide a JSON object with `start` and `end` (e.g.
            `"{\"start\":\"1\",\"end\":\"10\"}"`)
        subscription:
          $ref: '#/components/schemas/Subscription'
          description: >-
            Optional: subscription configuration for sending the dashboard
            snapshot.
        warehouse_id:
          type: string
          description: >-
            Optional: The warehouse id to execute the dashboard with for the
            schedule. If not specified, the default warehouse of the dashboard
            will be used.
      description: Configures the Lakeview Dashboard job task type.
    DashboardTaskOutput:
      type: object
      properties:
        page_snapshots:
          type: array
          items:
            $ref: '#/components/schemas/DashboardPageSnapshot'
    DbtCloudJobRunStep:
      type: object
      properties:
        index:
          type: integer
          description: Orders the steps in the job
        logs:
          type: string
          description: Output of the step
        name:
          type: string
          description: Name of the step in the job
        status:
          $ref: '#/components/schemas/DbtPlatformRunStatus'
          description: State of the step
      description: >-
        Format of response retrieved from dbt Cloud, for inclusion in output
        Deprecated in favor of
            DbtPlatformJobRunStep
    DbtCloudTask:
      type: object
      properties:
        connection_resource_name:
          type: string
          description: >-
            The resource name of the UC connection that authenticates the dbt
            Cloud for this task
        dbt_cloud_job_id:
          type: integer
          description: Id of the dbt Cloud job to be triggered
      description: Deprecated in favor of DbtPlatformTask
    DbtCloudTaskOutput:
      type: object
      properties:
        dbt_cloud_job_run_id:
          type: integer
          description: Id of the job run in dbt Cloud
        dbt_cloud_job_run_output:
          type: array
          items:
            $ref: '#/components/schemas/DbtCloudJobRunStep'
          description: Steps of the job run as received from dbt Cloud
        dbt_cloud_job_run_url:
          type: string
          description: Url where full run details can be viewed
      description: Deprecated in favor of DbtPlatformTaskOutput
    DbtOutput:
      type: object
      properties:
        artifacts_headers:
          type: object
        artifacts_link:
          type: string
          description: >-
            A pre-signed URL to download the (compressed) dbt artifacts. This
            link is valid for a limited time (30 minutes). This information is
            only available after the run has finished.
    DbtPlatformJobRunStep:
      type: object
      properties:
        index:
          type: integer
          description: Orders the steps in the job
        logs:
          type: string
          description: Output of the step
        logs_truncated:
          type: boolean
          description: >-
            Whether the logs of this step have been truncated. If true, the logs
            has been truncated to 10000 characters.
        name:
          type: string
          description: Name of the step in the job
        name_truncated:
          type: boolean
          description: >-
            Whether the name of the job has been truncated. If true, the name
            has been truncated to 100 characters.
        status:
          $ref: '#/components/schemas/DbtPlatformRunStatus'
          description: State of the step
      description: Format of response retrieved from dbt platform, for inclusion in output
    DbtPlatformTask:
      type: object
      properties:
        connection_resource_name:
          type: string
        dbt_platform_job_id:
          type: string
          description: >-
            Id of the dbt platform job to be triggered. Specified as a string
            for maximum compatibility with clients.
    DbtPlatformTaskOutput:
      type: object
      properties:
        dbt_platform_job_run_id:
          type: string
        dbt_platform_job_run_output:
          type: array
          items:
            $ref: '#/components/schemas/DbtPlatformJobRunStep'
          description: Steps of the job run as received from dbt platform
        dbt_platform_job_run_url:
          type: string
          description: Url where full run details can be viewed
        steps_truncated:
          type: boolean
          description: >-
            Whether the number of steps in the output has been truncated. If
            true, the output will contain the first 20 steps of the output.
    DbtTask:
      type: object
      properties:
        commands:
          type: array
          items:
            type: string
        catalog:
          type: string
          description: >-
            Optional name of the catalog to use. The value is the top level in
            the 3-level namespace of Unity Catalog (catalog / schema /
            relation). The catalog value can only be specified if a warehouse_id
            is specified. Requires dbt-databricks >= 1.1.1.
        profiles_directory:
          type: string
          description: >-
            Optional (relative) path to the profiles directory. Can only be
            specified if no warehouse_id is specified. If no warehouse_id is
            specified and this folder is unset, the root directory is used.
        project_directory:
          type: string
          description: >-
            Path to the project directory. Optional for Git sourced tasks, in
            which case if no value is provided, the root of the Git repository
            is used.
        schema:
          type: string
          description: >-
            Optional schema to write to. This parameter is only used when a
            warehouse_id is also provided. If not provided, the `default` schema
            is used.
        source:
          $ref: '#/components/schemas/Source'
          description: >-
            Optional location type of the project directory. When set to
            `WORKSPACE`, the project will be retrieved from the local Databricks
            workspace. When set to `GIT`, the project will be retrieved from a
            Git repository defined in `git_source`. If the value is empty, the
            task will use `GIT` if `git_source` is defined and `WORKSPACE`
            otherwise. * `WORKSPACE`: Project is located in Databricks
            workspace. * `GIT`: Project is located in cloud Git provider.
        warehouse_id:
          type: string
          description: >-
            ID of the SQL warehouse to connect to. If provided, we automatically
            generate and provide the profile and connection details to dbt. It
            can be overridden on a per-command basis by using the
            `--profiles-dir` command line argument.
      required:
        - commands
    EnforcePolicyComplianceForJobResponseJobClusterSettingsChange:
      type: object
      properties:
        field:
          type: string
          description: >-
            The field where this change would be made, prepended with the job
            cluster key.
        new_value:
          type: string
          description: >-
            The new value of this field after enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string.
            This is intended to be read by a human. The typed new value of this
            field can be retrieved by reading the settings field in the API
            response.
        previous_value:
          type: string
          description: >-
            The previous value of this field before enforcing policy compliance
            (either a number, a boolean, or a string) converted to a string.
            This is intended to be read by a human. The type of the field can be
            retrieved by reading the settings field in the API response.
      description: >-
        Represents a change to the job cluster's settings that would be required
        for the job clusters to
            become compliant with their policies.
    EnforcePolicyComplianceResponse:
      type: object
      properties:
        has_changes:
          type: boolean
        job_cluster_changes:
          type: array
          items:
            $ref: >-
              #/components/schemas/EnforcePolicyComplianceForJobResponseJobClusterSettingsChange
          description: >-
            A list of job cluster changes that have been made to the job’s
            cluster settings in order for all job clusters to become compliant
            with their policies.
        settings:
          $ref: '#/components/schemas/JobSettings'
          description: >-
            Updated job settings after policy enforcement. Policy enforcement
            only applies to job clusters that are created when running the job
            (which are specified in new_cluster) and does not apply to existing
            all-purpose clusters. Updated job settings are derived by applying
            policy default values to the existing job clusters in order to
            satisfy policy requirements.
    ExportRunOutput:
      type: object
      properties:
        views:
          type: array
          items:
            $ref: '#/components/schemas/ViewItem'
          description: >-
            The exported content in HTML format (one for every view item). To
            extract the HTML notebook from the JSON response, download and run
            this [Python script](/_static/examples/extract.py).
      description: Run was exported successfully.
    FileArrivalTriggerConfiguration:
      type: object
      properties:
        url:
          type: string
        min_time_between_triggers_seconds:
          type: integer
          description: >-
            If set, the trigger starts a run only after the specified amount of
            time passed since the last time the trigger fired. The minimum
            allowed value is 60 seconds
        wait_after_last_change_seconds:
          type: integer
          description: >-
            If set, the trigger starts a run only after no file activity has
            occurred for the specified amount of time. This makes it possible to
            wait for a batch of incoming files to arrive before triggering a
            run. The minimum allowed value is 60 seconds.
      required:
        - url
    FileArrivalTriggerState:
      type: object
      properties:
        using_file_events:
          type: boolean
    ForEachStats:
      type: object
      properties:
        error_message_stats:
          type: array
          items:
            $ref: '#/components/schemas/ForEachTaskErrorMessageStats'
        task_run_stats:
          $ref: '#/components/schemas/ForEachTaskTaskRunStats'
          description: >-
            Describes stats of the iteration. Only latest retries are
            considered.
    ForEachTask:
      type: object
      properties:
        inputs:
          type: string
        task:
          $ref: '#/components/schemas/Task'
          description: >-
            Configuration for the task that will be run for each element in the
            array
        concurrency:
          type: integer
          description: >-
            An optional maximum allowed number of concurrent runs of the task.
            Set this value if you want to be able to execute multiple runs of
            the task concurrently.
      required:
        - inputs
        - task
    ForEachTaskErrorMessageStats:
      type: object
      properties:
        count:
          type: integer
        error_message:
          type: string
          description: Describes the error message occured during the iterations.
        termination_category:
          type: string
          description: Describes the termination reason for the error message.
    ForEachTaskTaskRunStats:
      type: object
      properties:
        active_iterations:
          type: integer
        completed_iterations:
          type: integer
          description: Describes the number of failed and succeeded iteration runs.
        failed_iterations:
          type: integer
          description: Describes the number of failed iteration runs.
        scheduled_iterations:
          type: integer
          description: Describes the number of iteration runs that have been scheduled.
        succeeded_iterations:
          type: integer
          description: Describes the number of succeeded iteration runs.
        total_iterations:
          type: integer
          description: Describes the length of the list of items to iterate over.
    GenAiComputeTask:
      type: object
      properties:
        dl_runtime_image:
          type: string
        command:
          type: string
          description: Command launcher to run the actual script, e.g. bash, python etc.
        compute:
          $ref: '#/components/schemas/ComputeConfig'
        mlflow_experiment_name:
          type: string
          description: >-
            Optional string containing the name of the MLflow experiment to log
            the run to. If name is not found, backend will create the mlflow
            experiment using the name.
        source:
          $ref: '#/components/schemas/Source'
          description: >-
            Optional location type of the training script. When set to
            `WORKSPACE`, the script will be retrieved from the local Databricks
            workspace. When set to `GIT`, the script will be retrieved from a
            Git repository defined in `git_source`. If the value is empty, the
            task will use `GIT` if `git_source` is defined and `WORKSPACE`
            otherwise. * `WORKSPACE`: Script is located in Databricks workspace.
            * `GIT`: Script is located in cloud Git provider.
        training_script_path:
          type: string
          description: >-
            The training script file path to be executed. Cloud file URIs (such
            as dbfs:/, s3:/, adls:/, gcs:/) and workspace paths are supported.
            For python files stored in the Databricks workspace, the path must
            be absolute and begin with `/`. For files stored in a remote
            repository, the path must be relative. This field is required.
        yaml_parameters:
          type: string
          description: >-
            Optional string containing model parameters passed to the training
            script in yaml format. If present, then the content in
            yaml_parameters_file_path will be ignored.
        yaml_parameters_file_path:
          type: string
          description: >-
            Optional path to a YAML file containing model parameters passed to
            the training script.
      required:
        - dl_runtime_image
    GetJobPermissionLevelsResponse:
      type: object
      properties:
        permission_levels:
          type: array
          items:
            $ref: '#/components/schemas/JobPermissionsDescription'
    GetPolicyComplianceResponse:
      type: object
      properties:
        is_compliant:
          type: boolean
        violations:
          type: object
          description: >-
            An object containing key-value mappings representing the first 200
            policy validation errors. The keys indicate the path where the
            policy validation error is occurring. An identifier for the job
            cluster is prepended to the path. The values indicate an error
            message describing the policy validation error.
    GitSnapshot:
      type: object
      properties:
        used_commit:
          type: string
          description: >-
            Commit that was used to execute the run. If git_branch was
            specified, this points to the HEAD of the branch at the time of the
            run; if git_tag was specified, this points to the commit the tag
            points to.
      description: >-
        Read-only state of the remote repository at the time the job was run.
        This field is only
            included on job runs.
    GitSource:
      type: object
      properties:
        git_url:
          type: string
          description: URL of the repository to be cloned by this job.
        git_provider:
          $ref: '#/components/schemas/GitProvider'
          description: >-
            Unique identifier of the service used to host the Git repository.
            The value is case insensitive.
        git_branch:
          type: string
          description: >-
            Name of the branch to be checked out and used by this job. This
            field cannot be specified in conjunction with git_tag or git_commit.
        git_commit:
          type: string
          description: >-
            Commit to be checked out and used by this job. This field cannot be
            specified in conjunction with git_branch or git_tag.
        git_snapshot:
          $ref: '#/components/schemas/GitSnapshot'
        git_tag:
          type: string
          description: >-
            Name of the tag to be checked out and used by this job. This field
            cannot be specified in conjunction with git_branch or git_commit.
        job_source:
          $ref: '#/components/schemas/JobSource'
          description: >-
            The source of the job specification in the remote repository when
            the job is source controlled.
      required:
        - git_url
        - git_provider
      description: >-
        An optional specification for a remote Git repository containing the
        source code used by tasks.
            Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.

            If `git_source` is set, these tasks retrieve the file from the remote repository by default.
            However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.

            Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks
            are used, `git_source` must be defined on the job.
    Job:
      type: object
      properties:
        created_time:
          type: integer
          description: >-
            The time at which this job was created in epoch milliseconds
            (milliseconds since 1/1/1970 UTC).
        creator_user_name:
          type: string
          description: >-
            The creator user name. This field won’t be included in the response
            if the user has already been deleted.
        effective_budget_policy_id:
          type: string
          description: >-
            The id of the budget policy used by this job for cost attribution
            purposes. This may be set through (in order of precedence): 1.
            Budget admins through the account or workspace console 2. Jobs UI in
            the job details page and Jobs API using `budget_policy_id` 3.
            Inferred default based on accessible budget policies of the run_as
            identity on job creation or modification.
        effective_usage_policy_id:
          type: string
          description: >-
            The id of the usage policy used by this job for cost attribution
            purposes.
        has_more:
          type: boolean
          description: >-
            Indicates if the job has more array properties (`tasks`,
            `job_clusters`) that are not shown. They can be accessed via
            :method:jobs/get endpoint. It is only relevant for API 2.2
            :method:jobs/list requests with `expand_tasks=true`.
        job_id:
          type: integer
          description: The canonical identifier for this job.
        next_page_token:
          type: string
          description: A token that can be used to list the next page of array properties.
        run_as_user_name:
          type: string
          description: >-
            The email of an active workspace user or the application ID of a
            service principal that the job runs as. This value can be changed by
            setting the `run_as` field when creating or updating a job. By
            default, `run_as_user_name` is based on the current job settings and
            is set to the creator of the job if job access control is disabled
            or to the user with the `is_owner` permission if job access control
            is enabled.
        settings:
          $ref: '#/components/schemas/JobSettings'
          description: >-
            Settings for this job and all of its runs. These settings can be
            updated using the `resetJob` method.
        trigger_state:
          $ref: '#/components/schemas/TriggerStateProto'
          description: State of the trigger associated with the job.
      description: Job was retrieved successfully.
    JobAccessControlRequest:
      type: object
      properties:
        group_name:
          type: string
        permission_level:
          $ref: '#/components/schemas/JobPermissionLevel'
        service_principal_name:
          type: string
          description: application ID of a service principal
        user_name:
          type: string
          description: name of the user
    JobAccessControlResponse:
      type: object
      properties:
        all_permissions:
          type: array
          items:
            $ref: '#/components/schemas/JobPermission'
        display_name:
          type: string
          description: Display name of the user or service principal.
        group_name:
          type: string
          description: name of the group
        service_principal_name:
          type: string
          description: Name of the service principal.
        user_name:
          type: string
          description: name of the user
    JobCluster:
      type: object
      properties:
        job_cluster_key:
          type: string
        new_cluster:
          type: string
          description: >-
            If new_cluster, a description of a cluster that is created for each
            task.
      required:
        - job_cluster_key
        - new_cluster
    JobCompliance:
      type: object
      properties:
        job_id:
          type: integer
        is_compliant:
          type: boolean
          description: >-
            Whether this job is in compliance with the latest version of its
            policy.
        violations:
          type: object
          description: >-
            An object containing key-value mappings representing the first 200
            policy validation errors. The keys indicate the path where the
            policy validation error is occurring. An identifier for the job
            cluster is prepended to the path. The values indicate an error
            message describing the policy validation error.
      required:
        - job_id
    JobDeployment:
      type: object
      properties:
        kind:
          $ref: '#/components/schemas/JobDeploymentKind'
        metadata_file_path:
          type: string
          description: Path of the file that contains deployment metadata.
      required:
        - kind
    JobEmailNotifications:
      type: object
      properties:
        no_alert_for_skipped_runs:
          type: boolean
        on_duration_warning_threshold_exceeded:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when the duration of a run
            exceeds the threshold specified for the `RUN_DURATION_SECONDS`
            metric in the `health` field. If no rule for the
            `RUN_DURATION_SECONDS` metric is specified in the `health` field for
            the job, notifications are not sent.
        on_failure:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when a run unsuccessfully
            completes. A run is considered to have completed unsuccessfully if
            it ends with an `INTERNAL_ERROR` `life_cycle_state` or a `FAILED`,
            or `TIMED_OUT` result_state. If this is not specified on job
            creation, reset, or update the list is empty, and notifications are
            not sent.
        on_start:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when a run begins. If not
            specified on job creation, reset, or update, the list is empty, and
            notifications are not sent.
        on_streaming_backlog_exceeded:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to notify when any streaming backlog
            thresholds are exceeded for any stream. Streaming backlog thresholds
            can be set in the `health` field using the following metrics:
            `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`,
            `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`. Alerting
            is based on the 10-minute average of these metrics. If the issue
            persists, notifications are resent every 30 minutes.
        on_success:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when a run successfully
            completes. A run is considered to have completed successfully if it
            ends with a `TERMINATED` `life_cycle_state` and a `SUCCESS`
            result_state. If not specified on job creation, reset, or update,
            the list is empty, and notifications are not sent.
    JobEnvironment:
      type: object
      properties:
        environment_key:
          type: string
        spec:
          type: string
      required:
        - environment_key
    JobNotificationSettings:
      type: object
      properties:
        no_alert_for_canceled_runs:
          type: boolean
        no_alert_for_skipped_runs:
          type: boolean
          description: >-
            If true, do not send notifications to recipients specified in
            `on_failure` if the run is skipped.
    JobParameter:
      type: object
      properties:
        default:
          type: string
        name:
          type: string
          description: The name of the parameter
        value:
          type: string
          description: The value used in the run
    JobParameterDefinition:
      type: object
      properties:
        name:
          type: string
        default:
          type: string
          description: Default value of the parameter.
      required:
        - name
        - default
    JobPermission:
      type: object
      properties:
        inherited:
          type: boolean
        inherited_from_object:
          type: array
          items:
            type: string
        permission_level:
          $ref: '#/components/schemas/JobPermissionLevel'
    JobPermissions:
      type: object
      properties:
        access_control_list:
          type: array
          items:
            $ref: '#/components/schemas/JobAccessControlResponse'
        object_id:
          type: string
        object_type:
          type: string
    JobPermissionsDescription:
      type: object
      properties:
        description:
          type: string
        permission_level:
          $ref: '#/components/schemas/JobPermissionLevel'
    JobRunAs:
      type: object
      properties:
        group_name:
          type: string
          description: >-
            Group name of an account group assigned to the workspace. Setting
            this field requires being a member of the group.
        service_principal_name:
          type: string
          description: >-
            Application ID of an active service principal. Setting this field
            requires the `servicePrincipal/user` role.
        user_name:
          type: string
          description: >-
            The email of an active workspace user. Non-admin users can only set
            this field to their own email.
      description: >-
        Write-only setting. Specifies the user or service principal that the job
        runs as. If not
            specified, the job runs as the user who created the job.

            Either `user_name` or `service_principal_name` should be specified. If not, an error is thrown.
    JobSettings:
      type: object
      properties:
        budget_policy_id:
          type: string
        continuous:
          $ref: '#/components/schemas/Continuous'
          description: >-
            An optional continuous property for this job. The continuous
            property will ensure that there is always one run executing. Only
            one of `schedule` and `continuous` can be used.
        deployment:
          $ref: '#/components/schemas/JobDeployment'
          description: Deployment information for jobs managed by external sources.
        description:
          type: string
          description: >-
            An optional description for the job. The maximum length is 27700
            characters in UTF-8 encoding.
        edit_mode:
          $ref: '#/components/schemas/JobEditMode'
          description: >-
            Edit mode of the job. * `UI_LOCKED`: The job is in a locked UI state
            and cannot be modified. * `EDITABLE`: The job is in an editable
            state and can be modified.
        email_notifications:
          $ref: '#/components/schemas/JobEmailNotifications'
          description: >-
            An optional set of email addresses that is notified when runs of
            this job begin or complete as well as when this job is deleted.
        environments:
          type: array
          items:
            $ref: '#/components/schemas/JobEnvironment'
          description: >-
            A list of task execution environment specifications that can be
            referenced by serverless tasks of this job. For serverless notebook
            tasks, if the environment_key is not specified, the notebook
            environment will be used if present. If a jobs environment is
            specified, it will override the notebook environment. For other
            serverless tasks, the task environment is required to be specified
            using environment_key in the task settings.
        format:
          $ref: '#/components/schemas/Format'
          description: >-
            Used to tell what is the format of the job. This field is ignored in
            Create/Update/Reset calls. When using the Jobs API 2.1 this value is
            always set to `"MULTI_TASK"`.
        git_source:
          $ref: '#/components/schemas/GitSource'
          description: >-
            An optional specification for a remote Git repository containing the
            source code used by tasks. Version-controlled source code is
            supported by notebook, dbt, Python script, and SQL File tasks. If
            `git_source` is set, these tasks retrieve the file from the remote
            repository by default. However, this behavior can be overridden by
            setting `source` to `WORKSPACE` on the task. Note: dbt and SQL File
            tasks support only version-controlled sources. If dbt or SQL File
            tasks are used, `git_source` must be defined on the job.
        health:
          $ref: '#/components/schemas/JobsHealthRules'
        job_clusters:
          type: array
          items:
            $ref: '#/components/schemas/JobCluster'
          description: >-
            A list of job cluster specifications that can be shared and reused
            by tasks of this job. Libraries cannot be declared in a shared job
            cluster. You must declare dependent libraries in task settings.
        max_concurrent_runs:
          type: integer
          description: >-
            An optional maximum allowed number of concurrent runs of the job.
            Set this value if you want to be able to execute multiple runs of
            the same job concurrently. This is useful for example if you trigger
            your job on a frequent schedule and want to allow consecutive runs
            to overlap with each other, or if you want to trigger multiple runs
            which differ by their input parameters. This setting affects only
            new runs. For example, suppose the job’s concurrency is 4 and there
            are 4 concurrent active runs. Then setting the concurrency to 3
            won’t kill any of the active runs. However, from then on, new runs
            are skipped unless there are fewer than 3 active runs. This value
            cannot exceed 1000. Setting this value to `0` causes all new runs to
            be skipped.
        name:
          type: string
          description: >-
            An optional name for the job. The maximum length is 4096 bytes in
            UTF-8 encoding.
        notification_settings:
          $ref: '#/components/schemas/JobNotificationSettings'
          description: >-
            Optional notification settings that are used when sending
            notifications to each of the `email_notifications` and
            `webhook_notifications` for this job.
        parameters:
          type: array
          items:
            $ref: '#/components/schemas/JobParameterDefinition'
          description: Job-level parameter definitions
        performance_target:
          $ref: '#/components/schemas/PerformanceTarget'
          description: >-
            The performance mode on a serverless job. This field determines the
            level of compute performance or cost-efficiency for the run. The
            performance target does not apply to tasks that run on Serverless
            GPU compute. * `STANDARD`: Enables cost-efficient execution of
            serverless workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast
            startup and execution times through rapid scaling and optimized
            cluster performance.
        queue:
          $ref: '#/components/schemas/QueueSettings'
          description: The queue settings of the job.
        run_as:
          $ref: '#/components/schemas/JobRunAs'
          description: >-
            The user or service principal that the job runs as, if specified in
            the request. This field indicates the explicit configuration of
            `run_as` for the job. To find the value in all cases, explicit or
            implicit, use `run_as_user_name`.
        schedule:
          $ref: '#/components/schemas/CronSchedule'
          description: >-
            An optional periodic schedule for this job. The default behavior is
            that the job only runs when triggered by clicking “Run Now” in the
            Jobs UI or sending an API request to `runNow`.
        tags:
          type: object
          description: >-
            A map of tags associated with the job. These are forwarded to the
            cluster as cluster tags for jobs clusters, and are subject to the
            same limitations as cluster tags. A maximum of 25 tags can be added
            to the job.
        tasks:
          type: array
          items:
            $ref: '#/components/schemas/Task'
          description: >-
            A list of task specifications to be executed by this job. It
            supports up to 1000 elements in write endpoints
            (:method:jobs/create, :method:jobs/reset, :method:jobs/update,
            :method:jobs/submit). Read endpoints return only 100 tasks. If more
            than 100 tasks are available, you can paginate through them using
            :method:jobs/get. Use the `next_page_token` field at the object root
            to determine if more results are available.
        timeout_seconds:
          type: integer
          description: >-
            An optional timeout applied to each run of this job. A value of `0`
            means no timeout.
        trigger:
          $ref: '#/components/schemas/TriggerSettings'
          description: >-
            A configuration to trigger a run when certain conditions are met.
            The default behavior is that the job runs only when triggered by
            clicking “Run Now” in the Jobs UI or sending an API request to
            `runNow`.
        usage_policy_id:
          type: string
          description: >-
            The id of the user specified usage policy to use for this job. If
            not specified, a default usage policy may be applied when creating
            or modifying the job. See `effective_usage_policy_id` for the usage
            policy used by this workload.
        webhook_notifications:
          $ref: '#/components/schemas/WebhookNotifications'
          description: >-
            A collection of system notification IDs to notify when runs of this
            job begin or complete.
    JobSource:
      type: object
      properties:
        job_config_path:
          type: string
          description: Path of the job YAML file that contains the job specification.
        import_from_git_branch:
          type: string
          description: Name of the branch which the job is imported from.
        dirty_state:
          $ref: '#/components/schemas/JobSourceDirtyState'
          description: >-
            Dirty state indicates the job is not fully synced with the job
            specification in the remote repository. Possible values are: *
            `NOT_SYNCED`: The job is not yet synced with the remote job
            specification. Import the remote job specification from UI to make
            the job fully synced. * `DISCONNECTED`: The job is temporary
            disconnected from the remote job specification and is allowed for
            live edit. Import the remote job specification again from UI to make
            the job fully synced.
      required:
        - job_config_path
        - import_from_git_branch
      description: >-
        The source of the job specification in the remote repository when the
        job is source controlled.
    JobsHealthRule:
      type: object
      properties:
        metric:
          $ref: '#/components/schemas/JobsHealthMetric'
        op:
          $ref: '#/components/schemas/JobsHealthOperator'
        value:
          type: integer
          description: >-
            Specifies the threshold value that the health metric should obey to
            satisfy the health rule.
      required:
        - metric
        - op
        - value
    JobsHealthRules:
      type: object
      properties:
        rules:
          type: array
          items:
            $ref: '#/components/schemas/JobsHealthRule'
      description: An optional set of health rules that can be defined for this job.
    ListJobComplianceForPolicyResponse:
      type: object
      properties:
        jobs:
          type: array
          items:
            $ref: '#/components/schemas/JobCompliance'
        next_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the next page
            of results. If this field is not in the response, it means no
            further results for the request.
        prev_page_token:
          type: string
          description: >-
            This field represents the pagination token to retrieve the previous
            page of results. If this field is not in the response, it means no
            further results for the request.
    ListJobsResponse:
      type: object
      properties:
        has_more:
          type: boolean
          description: >-
            If true, additional jobs matching the provided filter are available
            for listing.
        jobs:
          type: array
          items:
            $ref: '#/components/schemas/BaseJob'
          description: >-
            The list of jobs. Only included in the response if there are jobs to
            list.
        next_page_token:
          type: string
          description: >-
            A token that can be used to list the next page of jobs (if
            applicable).
        prev_page_token:
          type: string
          description: >-
            A token that can be used to list the previous page of jobs (if
            applicable).
      description: List of jobs was retrieved successfully.
    ListRunsResponse:
      type: object
      properties:
        has_more:
          type: boolean
          description: >-
            If true, additional runs matching the provided filter are available
            for listing.
        next_page_token:
          type: string
          description: >-
            A token that can be used to list the next page of runs (if
            applicable).
        prev_page_token:
          type: string
          description: >-
            A token that can be used to list the previous page of runs (if
            applicable).
        runs:
          type: array
          items:
            $ref: '#/components/schemas/BaseRun'
          description: >-
            A list of runs, from most recently started to least. Only included
            in the response if there are runs to list.
      description: List of runs was retrieved successfully.
    ModelTriggerConfiguration:
      type: object
      properties:
        condition:
          $ref: '#/components/schemas/ModelTriggerConfigurationCondition'
        aliases:
          type: array
          items:
            type: string
          description: >-
            Aliases of the model versions to monitor. Can only be used in
            conjunction with condition MODEL_ALIAS_SET.
        min_time_between_triggers_seconds:
          type: integer
          description: >-
            If set, the trigger starts a run only after the specified amount of
            time has passed since the last time the trigger fired. The minimum
            allowed value is 60 seconds.
        securable_name:
          type: string
          description: >-
            Name of the securable to monitor ("mycatalog.myschema.mymodel" in
            the case of model-level triggers, "mycatalog.myschema" in the case
            of schema-level triggers) or empty in the case of metastore-level
            triggers.
        wait_after_last_change_seconds:
          type: integer
          description: >-
            If set, the trigger starts a run only after no model updates have
            occurred for the specified time and can be used to wait for a series
            of model updates before triggering a run. The minimum allowed value
            is 60 seconds.
      required:
        - condition
    NotebookOutput:
      type: object
      properties:
        result:
          type: string
        truncated:
          type: boolean
          description: Whether or not the result was truncated.
    NotebookTask:
      type: object
      properties:
        notebook_path:
          type: string
        base_parameters:
          type: object
          description: >-
            Base parameters to be used for each run of this job. If the run is
            initiated by a call to :method:jobs/run Now with parameters
            specified, the two parameters maps are merged. If the same key is
            specified in `base_parameters` and in `run-now`, the value from
            `run-now` is used. Use [Task parameter variables] to set parameters
            containing information about job runs. If the notebook takes a
            parameter that is not specified in the job’s `base_parameters` or
            the `run-now` override parameters, the default value from the
            notebook is used. Retrieve these parameters in a notebook using
            [dbutils.widgets.get]. The JSON representation of this field cannot
            exceed 1MB. [Task parameter variables]:
            https://docs.databricks.com/jobs.html#parameter-variables
            [dbutils.widgets.get]:
            https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-widgets
        source:
          $ref: '#/components/schemas/Source'
          description: >-
            Optional location type of the notebook. When set to `WORKSPACE`, the
            notebook will be retrieved from the local Databricks workspace. When
            set to `GIT`, the notebook will be retrieved from a Git repository
            defined in `git_source`. If the value is empty, the task will use
            `GIT` if `git_source` is defined and `WORKSPACE` otherwise. *
            `WORKSPACE`: Notebook is located in Databricks workspace. * `GIT`:
            Notebook is located in cloud Git provider.
        warehouse_id:
          type: string
          description: >-
            Optional `warehouse_id` to run the notebook on a SQL warehouse.
            Classic SQL warehouses are NOT supported, please use serverless or
            pro SQL warehouses. Note that SQL warehouses only support SQL cells;
            if the notebook contains non-SQL cells, the run will fail.
      required:
        - notebook_path
    OutputSchemaInfo:
      type: object
      properties:
        catalog_name:
          type: string
        expiration_time:
          type: integer
          description: >-
            The expiration time for the output schema as a Unix timestamp in
            milliseconds.
        schema_name:
          type: string
      description: >-
        Stores the catalog name, schema name, and the output schema expiration
        time for the clean room
            run.
    PeriodicTriggerConfiguration:
      type: object
      properties:
        interval:
          type: integer
        unit:
          $ref: '#/components/schemas/PeriodicTriggerConfigurationTimeUnit'
          description: The unit of time for the interval.
      required:
        - interval
        - unit
    PipelineParams:
      type: object
      properties:
        full_refresh:
          type: boolean
    PipelineTask:
      type: object
      properties:
        pipeline_id:
          type: string
        full_refresh:
          type: boolean
          description: If true, triggers a full refresh on the delta live table.
      required:
        - pipeline_id
    PowerBiModel:
      type: object
      properties:
        authentication_method:
          $ref: '#/components/schemas/AuthenticationMethod'
        model_name:
          type: string
          description: The name of the Power BI model
        overwrite_existing:
          type: boolean
          description: Whether to overwrite existing Power BI models
        storage_mode:
          $ref: '#/components/schemas/StorageMode'
          description: The default storage mode of the Power BI model
        workspace_name:
          type: string
          description: The name of the Power BI workspace of the model
    PowerBiTable:
      type: object
      properties:
        catalog:
          type: string
        name:
          type: string
          description: The table name in Databricks
        schema:
          type: string
          description: The schema name in Databricks
        storage_mode:
          $ref: '#/components/schemas/StorageMode'
          description: The Power BI storage mode of the table
    PowerBiTask:
      type: object
      properties:
        connection_resource_name:
          type: string
        power_bi_model:
          $ref: '#/components/schemas/PowerBiModel'
          description: The semantic model to update
        refresh_after_update:
          type: boolean
          description: Whether the model should be refreshed after the update
        tables:
          type: array
          items:
            $ref: '#/components/schemas/PowerBiTable'
          description: The tables to be exported to Power BI
        warehouse_id:
          type: string
          description: The SQL warehouse ID to use as the Power BI data source
    PythonWheelTask:
      type: object
      properties:
        package_name:
          type: string
        entry_point:
          type: string
          description: >-
            Named entry point to use, if it does not exist in the metadata of
            the package it executes the function from the package directly using
            `$packageName.$entryPoint()`
        named_parameters:
          type: object
          description: >-
            Command-line parameters passed to Python wheel task in the form of
            `["--name=task", "--data=dbfs:/path/to/data.json"]`. Leave it empty
            if `parameters` is not null.
        parameters:
          type: array
          items:
            type: string
          description: >-
            Command-line parameters passed to Python wheel task. Leave it empty
            if `named_parameters` is not null.
      required:
        - package_name
        - entry_point
    QueueDetails:
      type: object
      properties:
        code:
          $ref: '#/components/schemas/QueueDetailsCodeCode'
        message:
          type: string
          description: >-
            A descriptive message with the queuing details. This field is
            unstructured, and its exact format is subject to change.
    QueueSettings:
      type: object
      properties:
        enabled:
          type: boolean
      required:
        - enabled
    RepairHistoryItem:
      type: object
      properties:
        effective_performance_target:
          $ref: '#/components/schemas/PerformanceTarget'
        end_time:
          type: integer
          description: The end time of the (repaired) run.
        id:
          type: integer
          description: >-
            The ID of the repair. Only returned for the items that represent a
            repair in `repair_history`.
        start_time:
          type: integer
          description: The start time of the (repaired) run.
        state:
          $ref: '#/components/schemas/RunState'
          description: Deprecated. Please use the `status` field instead.
        status:
          $ref: '#/components/schemas/RunStatus'
        task_run_ids:
          type: array
          items:
            type: integer
          description: >-
            The run IDs of the task runs that ran as part of this repair history
            item.
        type:
          $ref: '#/components/schemas/RepairHistoryItemType'
          description: >-
            The repair history item type. Indicates whether a run is the
            original run or a repair run.
    RepairRunResponse:
      type: object
      properties:
        repair_id:
          type: integer
          description: >-
            The ID of the repair. Must be provided in subsequent repairs using
            the `latest_repair_id` field to ensure sequential repairs.
      description: Run repair was initiated.
    ResolvedConditionTaskValues:
      type: object
      properties:
        left:
          type: string
        right:
          type: string
    ResolvedDbtTaskValues:
      type: object
      properties:
        commands:
          type: array
          items:
            type: string
    ResolvedNotebookTaskValues:
      type: object
      properties:
        base_parameters:
          type: object
    ResolvedParamPairValues:
      type: object
      properties:
        parameters:
          type: object
    ResolvedPythonWheelTaskValues:
      type: object
      properties:
        named_parameters:
          type: object
        parameters:
          type: array
          items:
            type: string
    ResolvedRunJobTaskValues:
      type: object
      properties:
        job_parameters:
          type: object
        parameters:
          type: object
    ResolvedStringParamsValues:
      type: object
      properties:
        parameters:
          type: array
          items:
            type: string
    ResolvedValues:
      type: object
      properties:
        condition_task:
          $ref: '#/components/schemas/ResolvedConditionTaskValues'
        dbt_task:
          $ref: '#/components/schemas/ResolvedDbtTaskValues'
        notebook_task:
          $ref: '#/components/schemas/ResolvedNotebookTaskValues'
        python_wheel_task:
          $ref: '#/components/schemas/ResolvedPythonWheelTaskValues'
        run_job_task:
          $ref: '#/components/schemas/ResolvedRunJobTaskValues'
        simulation_task:
          $ref: '#/components/schemas/ResolvedParamPairValues'
        spark_jar_task:
          $ref: '#/components/schemas/ResolvedStringParamsValues'
        spark_python_task:
          $ref: '#/components/schemas/ResolvedStringParamsValues'
        spark_submit_task:
          $ref: '#/components/schemas/ResolvedStringParamsValues'
        sql_task:
          $ref: '#/components/schemas/ResolvedParamPairValues'
    Run:
      type: object
      properties:
        attempt_number:
          type: integer
          description: >-
            The sequence number of this run attempt for a triggered job run. The
            initial attempt of a run has an attempt_number of 0. If the initial
            run attempt fails, and the job has a retry policy (`max_retries` >
            0), subsequent runs are created with an `original_attempt_run_id` of
            the original attempt’s ID and an incrementing `attempt_number`. Runs
            are retried only until they succeed, and the maximum
            `attempt_number` is the same as the `max_retries` value for the job.
        cleanup_duration:
          type: integer
          description: >-
            The time in milliseconds it took to terminate the cluster and clean
            up any associated artifacts. The duration of a task run is the sum
            of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `cleanup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the
            value of the `run_duration` field.
        cluster_instance:
          $ref: '#/components/schemas/ClusterInstance'
          description: >-
            The cluster used for this run. If the run is specified to use a new
            cluster, this field is set once the Jobs service has requested a
            cluster for the run.
        cluster_spec:
          $ref: '#/components/schemas/ClusterSpec'
          description: >-
            A snapshot of the job’s cluster specification when this run was
            created.
        creator_user_name:
          type: string
          description: >-
            The creator user name. This field won’t be included in the response
            if the user has already been deleted.
        description:
          type: string
          description: Description of the run
        effective_performance_target:
          $ref: '#/components/schemas/PerformanceTarget'
          description: >-
            The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on
            the request depending on whether the performance mode is supported
            by the job type. * `STANDARD`: Enables cost-efficient execution of
            serverless workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast
            startup and execution times through rapid scaling and optimized
            cluster performance.
        effective_usage_policy_id:
          type: string
          description: >-
            The id of the usage policy used by this run for cost attribution
            purposes.
        end_time:
          type: integer
          description: >-
            The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still
            running.
        execution_duration:
          type: integer
          description: >-
            The time in milliseconds it took to execute the commands in the JAR
            or notebook until they completed, failed, timed out, were cancelled,
            or encountered an unexpected error. The duration of a task run is
            the sum of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `execution_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the
            value of the `run_duration` field.
        git_source:
          $ref: '#/components/schemas/GitSource'
          description: >-
            An optional specification for a remote Git repository containing the
            source code used by tasks. Version-controlled source code is
            supported by notebook, dbt, Python script, and SQL File tasks. If
            `git_source` is set, these tasks retrieve the file from the remote
            repository by default. However, this behavior can be overridden by
            setting `source` to `WORKSPACE` on the task. Note: dbt and SQL File
            tasks support only version-controlled sources. If dbt or SQL File
            tasks are used, `git_source` must be defined on the job.
        has_more:
          type: boolean
          description: >-
            Indicates if the run has more array properties (`tasks`,
            `job_clusters`) that are not shown. They can be accessed via
            :method:jobs/getrun endpoint. It is only relevant for API 2.2
            :method:jobs/listruns requests with `expand_tasks=true`.
        iterations:
          type: array
          items:
            $ref: '#/components/schemas/RunTask'
          description: >-
            Only populated by for-each iterations. The parent for-each task is
            located in tasks array.
        job_clusters:
          type: array
          items:
            $ref: '#/components/schemas/JobCluster'
          description: >-
            A list of job cluster specifications that can be shared and reused
            by tasks of this job. Libraries cannot be declared in a shared job
            cluster. You must declare dependent libraries in task settings. If
            more than 100 job clusters are available, you can paginate through
            them using :method:jobs/getrun.
        job_id:
          type: integer
          description: The canonical identifier of the job that contains this run.
        job_parameters:
          type: array
          items:
            $ref: '#/components/schemas/JobParameter'
          description: Job-level parameters used in the run
        job_run_id:
          type: integer
          description: >-
            ID of the job run that this run belongs to. For legacy and
            single-task job runs the field is populated with the job run ID. For
            task runs, the field is populated with the ID of the job run that
            the task run belongs to.
        next_page_token:
          type: string
          description: A token that can be used to list the next page of array properties.
        number_in_job:
          type: integer
          description: >-
            A unique identifier for this job run. This is set to the same value
            as `run_id`.
        original_attempt_run_id:
          type: integer
          description: >-
            If this run is a retry of a prior run attempt, this field contains
            the run_id of the original attempt; otherwise, it is the same as the
            run_id.
        overriding_parameters:
          $ref: '#/components/schemas/RunParameters'
          description: The parameters used for this run.
        queue_duration:
          type: integer
          description: The time in milliseconds that the run has spent in the queue.
        repair_history:
          type: array
          items:
            $ref: '#/components/schemas/RepairHistoryItem'
          description: The repair history of the run.
        run_duration:
          type: integer
          description: >-
            The time in milliseconds it took the job run and all of its repairs
            to finish.
        run_id:
          type: integer
          description: >-
            The canonical identifier of the run. This ID is unique across all
            runs of all jobs.
        run_name:
          type: string
          description: >-
            An optional name for the run. The maximum length is 4096 bytes in
            UTF-8 encoding.
        run_page_url:
          type: string
          description: The URL to the detail page of the run.
        run_type:
          $ref: '#/components/schemas/RunType'
        schedule:
          $ref: '#/components/schemas/CronSchedule'
          description: >-
            The cron schedule that triggered this run if it was triggered by the
            periodic scheduler.
        setup_duration:
          type: integer
          description: >-
            The time in milliseconds it took to set up the cluster. For runs
            that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The
            duration of a task run is the sum of the `setup_duration`,
            `execution_duration`, and the `cleanup_duration`. The
            `setup_duration` field is set to 0 for multitask job runs. The total
            duration of a multitask job run is the value of the `run_duration`
            field.
        start_time:
          type: integer
          description: >-
            The time at which this run was started in epoch milliseconds
            (milliseconds since 1/1/1970 UTC). This may not be the time when the
            job task starts executing, for example, if the job is scheduled to
            run on a new cluster, this is the time the cluster creation call is
            issued.
        state:
          $ref: '#/components/schemas/RunState'
          description: Deprecated. Please use the `status` field instead.
        status:
          $ref: '#/components/schemas/RunStatus'
        tasks:
          type: array
          items:
            $ref: '#/components/schemas/RunTask'
          description: >-
            The list of tasks performed by the run. Each task has its own
            `run_id` which you can use to call `JobsGetOutput` to retrieve the
            run resutls. If more than 100 tasks are available, you can paginate
            through them using :method:jobs/getrun. Use the `next_page_token`
            field at the object root to determine if more results are available.
        trigger:
          $ref: '#/components/schemas/TriggerType'
        trigger_info:
          $ref: '#/components/schemas/TriggerInfo'
      description: Run was retrieved successfully
    RunConditionTask:
      type: object
      properties:
        op:
          $ref: '#/components/schemas/ConditionTaskOp'
        left:
          type: string
          description: >-
            The left operand of the condition task. Can be either a string value
            or a job state or parameter reference.
        right:
          type: string
          description: >-
            The right operand of the condition task. Can be either a string
            value or a job state or parameter reference.
        outcome:
          type: string
          description: >-
            The condition expression evaluation result. Filled in if the task
            was successfully completed. Can be `"true"` or `"false"`
      required:
        - op
        - left
        - right
    RunForEachTask:
      type: object
      properties:
        inputs:
          type: string
        task:
          $ref: '#/components/schemas/Task'
          description: >-
            Configuration for the task that will be run for each element in the
            array
        concurrency:
          type: integer
          description: >-
            An optional maximum allowed number of concurrent runs of the task.
            Set this value if you want to be able to execute multiple runs of
            the task concurrently.
        stats:
          $ref: '#/components/schemas/ForEachStats'
          description: >-
            Read only field. Populated for GetRun and ListRuns RPC calls and
            stores the execution stats of an For each task
      required:
        - inputs
        - task
    RunJobOutput:
      type: object
      properties:
        run_id:
          type: integer
    RunJobTask:
      type: object
      properties:
        job_id:
          type: integer
        dbt_commands:
          type: array
          items:
            type: string
          description: >-
            An array of commands to execute for jobs with the dbt task, for
            example `"dbt_commands": ["dbt deps", "dbt seed", "dbt deps", "dbt
            seed", "dbt run"]` ⚠ **Deprecation note** Use [job parameters] to
            pass information down to tasks. [job parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        jar_params:
          type: array
          items:
            type: string
          description: >-
            A list of parameters for jobs with Spark JAR tasks, for example
            `"jar_params": ["john doe", "35"]`. The parameters are used to
            invoke the main function of the main class specified in the Spark
            JAR task. If not specified upon `run-now`, it defaults to an empty
            list. jar_params cannot be specified in conjunction with
            notebook_params. The JSON representation of this field (for example
            `{"jar_params":["john doe","35"]}`) cannot exceed 10,000 bytes. ⚠
            **Deprecation note** Use [job parameters] to pass information down
            to tasks. [job parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        job_parameters:
          type: object
          description: Job-level parameters used to trigger the job.
        notebook_params:
          type: object
          description: >-
            A map from keys to values for jobs with notebook task, for example
            `"notebook_params": {"name": "john doe", "age": "35"}`. The map is
            passed to the notebook and is accessible through the
            [dbutils.widgets.get] function. If not specified upon `run-now`, the
            triggered run uses the job’s base parameters. notebook_params cannot
            be specified in conjunction with jar_params. ⚠ **Deprecation note**
            Use [job parameters] to pass information down to tasks. The JSON
            representation of this field (for example
            `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot exceed
            10,000 bytes. [dbutils.widgets.get]:
            https://docs.databricks.com/dev-tools/databricks-utils.html [job
            parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        pipeline_params:
          $ref: '#/components/schemas/PipelineParams'
          description: Controls whether the pipeline should perform a full refresh
        python_named_params:
          type: object
        python_params:
          type: array
          items:
            type: string
          description: >-
            A list of parameters for jobs with Python tasks, for example
            `"python_params": ["john doe", "35"]`. The parameters are passed to
            Python file as command-line parameters. If specified upon `run-now`,
            it would overwrite the parameters specified in job setting. The JSON
            representation of this field (for example `{"python_params":["john
            doe","35"]}`) cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks. Important These
            parameters accept only Latin characters (ASCII character set). Using
            non-ASCII characters returns an error. Examples of invalid,
            non-ASCII characters are Chinese, Japanese kanjis, and emojis. [job
            parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        spark_submit_params:
          type: array
          items:
            type: string
          description: >-
            A list of parameters for jobs with spark submit task, for example
            `"spark_submit_params": ["--class",
            "org.apache.spark.examples.SparkPi"]`. The parameters are passed to
            spark-submit script as command-line parameters. If specified upon
            `run-now`, it would overwrite the parameters specified in job
            setting. The JSON representation of this field (for example
            `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes. ⚠
            **Deprecation note** Use [job parameters] to pass information down
            to tasks. Important These parameters accept only Latin characters
            (ASCII character set). Using non-ASCII characters returns an error.
            Examples of invalid, non-ASCII characters are Chinese, Japanese
            kanjis, and emojis. [job parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        sql_params:
          type: object
          description: >-
            A map from keys to values for jobs with SQL task, for example
            `"sql_params": {"name": "john doe", "age": "35"}`. The SQL alert
            task does not support custom parameters. ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks. [job
            parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
      required:
        - job_id
    RunNowResponse:
      type: object
      properties:
        number_in_job:
          type: integer
          description: >-
            A unique identifier for this job run. This is set to the same value
            as `run_id`.
        run_id:
          type: integer
          description: The globally unique ID of the newly triggered run.
      description: Run was started successfully.
    RunOutput:
      type: object
      properties:
        clean_rooms_notebook_output:
          $ref: >-
            #/components/schemas/CleanRoomsNotebookTaskCleanRoomsNotebookTaskOutput
          description: The output of a clean rooms notebook task, if available
        dashboard_output:
          $ref: '#/components/schemas/DashboardTaskOutput'
          description: The output of a dashboard task, if available
        dbt_cloud_output:
          $ref: '#/components/schemas/DbtCloudTaskOutput'
          description: Deprecated in favor of the new dbt_platform_output
        dbt_output:
          $ref: '#/components/schemas/DbtOutput'
          description: The output of a dbt task, if available.
        dbt_platform_output:
          $ref: '#/components/schemas/DbtPlatformTaskOutput'
        error:
          type: string
          description: >-
            An error message indicating why a task failed or why output is not
            available. The message is unstructured, and its exact format is
            subject to change.
        error_trace:
          type: string
          description: >-
            If there was an error executing the run, this field contains any
            available stack traces.
        info:
          type: string
        logs:
          type: string
          description: >-
            The output from tasks that write to standard streams (stdout/stderr)
            such as spark_jar_task, spark_python_task, python_wheel_task. It's
            not supported for the notebook_task, pipeline_task or
            spark_submit_task. Databricks restricts this API to return the last
            5 MB of these logs.
        logs_truncated:
          type: boolean
          description: Whether the logs are truncated.
        metadata:
          $ref: '#/components/schemas/Run'
          description: All details of the run except for its output.
        notebook_output:
          $ref: '#/components/schemas/NotebookOutput'
          description: >-
            The output of a notebook task, if available. A notebook task that
            terminates (either successfully or with a failure) without calling
            `dbutils.notebook.exit()` is considered to have an empty output.
            This field is set but its result value is empty. Databricks
            restricts this API to return the first 5 MB of the output. To return
            a larger result, use the [ClusterLogConf] field to configure log
            storage for the job cluster. [ClusterLogConf]:
            https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterlogconf
        run_job_output:
          $ref: '#/components/schemas/RunJobOutput'
          description: The output of a run job task, if available
        sql_output:
          $ref: '#/components/schemas/SqlOutput'
          description: The output of a SQL task, if available.
      description: Run output was retrieved successfully.
    RunParameters:
      type: object
      properties:
        dbt_commands:
          type: array
          items:
            type: string
        jar_params:
          type: array
          items:
            type: string
          description: >-
            A list of parameters for jobs with Spark JAR tasks, for example
            `"jar_params": ["john doe", "35"]`. The parameters are used to
            invoke the main function of the main class specified in the Spark
            JAR task. If not specified upon `run-now`, it defaults to an empty
            list. jar_params cannot be specified in conjunction with
            notebook_params. The JSON representation of this field (for example
            `{"jar_params":["john doe","35"]}`) cannot exceed 10,000 bytes. ⚠
            **Deprecation note** Use [job parameters] to pass information down
            to tasks. [job parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        notebook_params:
          type: object
          description: >-
            A map from keys to values for jobs with notebook task, for example
            `"notebook_params": {"name": "john doe", "age": "35"}`. The map is
            passed to the notebook and is accessible through the
            [dbutils.widgets.get] function. If not specified upon `run-now`, the
            triggered run uses the job’s base parameters. notebook_params cannot
            be specified in conjunction with jar_params. ⚠ **Deprecation note**
            Use [job parameters] to pass information down to tasks. The JSON
            representation of this field (for example
            `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot exceed
            10,000 bytes. [dbutils.widgets.get]:
            https://docs.databricks.com/dev-tools/databricks-utils.html [job
            parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        pipeline_params:
          $ref: '#/components/schemas/PipelineParams'
          description: Controls whether the pipeline should perform a full refresh
        python_named_params:
          type: object
        python_params:
          type: array
          items:
            type: string
          description: >-
            A list of parameters for jobs with Python tasks, for example
            `"python_params": ["john doe", "35"]`. The parameters are passed to
            Python file as command-line parameters. If specified upon `run-now`,
            it would overwrite the parameters specified in job setting. The JSON
            representation of this field (for example `{"python_params":["john
            doe","35"]}`) cannot exceed 10,000 bytes. ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks. Important These
            parameters accept only Latin characters (ASCII character set). Using
            non-ASCII characters returns an error. Examples of invalid,
            non-ASCII characters are Chinese, Japanese kanjis, and emojis. [job
            parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        spark_submit_params:
          type: array
          items:
            type: string
          description: >-
            A list of parameters for jobs with spark submit task, for example
            `"spark_submit_params": ["--class",
            "org.apache.spark.examples.SparkPi"]`. The parameters are passed to
            spark-submit script as command-line parameters. If specified upon
            `run-now`, it would overwrite the parameters specified in job
            setting. The JSON representation of this field (for example
            `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes. ⚠
            **Deprecation note** Use [job parameters] to pass information down
            to tasks. Important These parameters accept only Latin characters
            (ASCII character set). Using non-ASCII characters returns an error.
            Examples of invalid, non-ASCII characters are Chinese, Japanese
            kanjis, and emojis. [job parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
        sql_params:
          type: object
          description: >-
            A map from keys to values for jobs with SQL task, for example
            `"sql_params": {"name": "john doe", "age": "35"}`. The SQL alert
            task does not support custom parameters. ⚠ **Deprecation note** Use
            [job parameters] to pass information down to tasks. [job
            parameters]:
            https://docs.databricks.com/jobs/job-parameters.html#job-parameter-pushdown
    RunState:
      type: object
      properties:
        life_cycle_state:
          $ref: '#/components/schemas/RunLifeCycleState'
          description: >-
            A value indicating the run's current lifecycle state. This field is
            always available in the response. Note: Additional states might be
            introduced in future releases.
        queue_reason:
          type: string
          description: The reason indicating why the run was queued.
        result_state:
          $ref: '#/components/schemas/RunResultState'
          description: >-
            A value indicating the run's result. This field is only available
            for terminal lifecycle states. Note: Additional states might be
            introduced in future releases.
        state_message:
          type: string
          description: >-
            A descriptive message for the current state. This field is
            unstructured, and its exact format is subject to change.
        user_cancelled_or_timedout:
          type: boolean
          description: >-
            A value indicating whether a run was canceled manually by a user or
            by the scheduler because the run timed out.
      description: The current state of the run.
    RunStatus:
      type: object
      properties:
        queue_details:
          $ref: '#/components/schemas/QueueDetails'
          description: If the run was queued, details about the reason for queuing the run.
        state:
          $ref: '#/components/schemas/RunLifecycleStateV2State'
        termination_details:
          $ref: '#/components/schemas/TerminationDetails'
          description: >-
            If the run is in a TERMINATING or TERMINATED state, details about
            the reason for terminating the run.
      description: The current status of the run
    RunTask:
      type: object
      properties:
        task_key:
          type: string
          description: >-
            A unique name for the task. This field is used to refer to this task
            from other tasks. This field is required and must be unique within
            its parent job. On Update or Reset, this field is used to reference
            the tasks to be updated or reset.
        attempt_number:
          type: integer
          description: >-
            The sequence number of this run attempt for a triggered job run. The
            initial attempt of a run has an attempt_number of 0. If the initial
            run attempt fails, and the job has a retry policy (`max_retries` >
            0), subsequent runs are created with an `original_attempt_run_id` of
            the original attempt’s ID and an incrementing `attempt_number`. Runs
            are retried only until they succeed, and the maximum
            `attempt_number` is the same as the `max_retries` value for the job.
        clean_rooms_notebook_task:
          $ref: '#/components/schemas/CleanRoomsNotebookTask'
          description: >-
            The task runs a [clean rooms] notebook when the
            `clean_rooms_notebook_task` field is present. [clean rooms]:
            https://docs.databricks.com/clean-rooms/index.html
        cleanup_duration:
          type: integer
          description: >-
            The time in milliseconds it took to terminate the cluster and clean
            up any associated artifacts. The duration of a task run is the sum
            of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `cleanup_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the
            value of the `run_duration` field.
        cluster_instance:
          $ref: '#/components/schemas/ClusterInstance'
          description: >-
            The cluster used for this run. If the run is specified to use a new
            cluster, this field is set once the Jobs service has requested a
            cluster for the run.
        compute:
          $ref: '#/components/schemas/Compute'
          description: Task level compute configuration.
        condition_task:
          $ref: '#/components/schemas/RunConditionTask'
          description: >-
            The task evaluates a condition that can be used to control the
            execution of other tasks when the `condition_task` field is present.
            The condition task does not require a cluster to execute and does
            not support retries or notifications.
        dashboard_task:
          $ref: '#/components/schemas/DashboardTask'
          description: The task refreshes a dashboard and sends a snapshot to subscribers.
        dbt_cloud_task:
          $ref: '#/components/schemas/DbtCloudTask'
          description: >-
            Task type for dbt cloud, deprecated in favor of the new name
            dbt_platform_task
        dbt_platform_task:
          $ref: '#/components/schemas/DbtPlatformTask'
        dbt_task:
          $ref: '#/components/schemas/DbtTask'
          description: >-
            The task runs one or more dbt commands when the `dbt_task` field is
            present. The dbt task requires both Databricks SQL and the ability
            to use a serverless or a pro SQL warehouse.
        depends_on:
          type: array
          items:
            $ref: '#/components/schemas/TaskDependency'
          description: >-
            An optional array of objects specifying the dependency graph of the
            task. All tasks specified in this field must complete successfully
            before executing this task. The key is `task_key`, and the value is
            the name assigned to the dependent task.
        description:
          type: string
          description: An optional description for this task.
        effective_performance_target:
          $ref: '#/components/schemas/PerformanceTarget'
          description: >-
            The actual performance target used by the serverless run during
            execution. This can differ from the client-set performance target on
            the request depending on whether the performance mode is supported
            by the job type. * `STANDARD`: Enables cost-efficient execution of
            serverless workloads. * `PERFORMANCE_OPTIMIZED`: Prioritizes fast
            startup and execution times through rapid scaling and optimized
            cluster performance.
        email_notifications:
          $ref: '#/components/schemas/JobEmailNotifications'
          description: >-
            An optional set of email addresses notified when the task run begins
            or completes. The default behavior is to not send any emails.
        end_time:
          type: integer
          description: >-
            The time at which this run ended in epoch milliseconds (milliseconds
            since 1/1/1970 UTC). This field is set to 0 if the job is still
            running.
        environment_key:
          type: string
          description: >-
            The key that references an environment spec in a job. This field is
            required for Python script, Python wheel and dbt tasks when using
            serverless compute.
        execution_duration:
          type: integer
          description: >-
            The time in milliseconds it took to execute the commands in the JAR
            or notebook until they completed, failed, timed out, were cancelled,
            or encountered an unexpected error. The duration of a task run is
            the sum of the `setup_duration`, `execution_duration`, and the
            `cleanup_duration`. The `execution_duration` field is set to 0 for
            multitask job runs. The total duration of a multitask job run is the
            value of the `run_duration` field.
        existing_cluster_id:
          type: string
          description: >-
            If existing_cluster_id, the ID of an existing cluster that is used
            for all runs. When running jobs or tasks on an existing cluster, you
            may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater
            reliability
        for_each_task:
          $ref: '#/components/schemas/RunForEachTask'
          description: >-
            The task executes a nested task for every input provided when the
            `for_each_task` field is present.
        gen_ai_compute_task:
          $ref: '#/components/schemas/GenAiComputeTask'
        git_source:
          $ref: '#/components/schemas/GitSource'
          description: >-
            An optional specification for a remote Git repository containing the
            source code used by tasks. Version-controlled source code is
            supported by notebook, dbt, Python script, and SQL File tasks. If
            `git_source` is set, these tasks retrieve the file from the remote
            repository by default. However, this behavior can be overridden by
            setting `source` to `WORKSPACE` on the task. Note: dbt and SQL File
            tasks support only version-controlled sources. If dbt or SQL File
            tasks are used, `git_source` must be defined on the job.
        job_cluster_key:
          type: string
          description: >-
            If job_cluster_key, this task is executed reusing the cluster
            specified in `job.settings.job_clusters`.
        libraries:
          type: string
          description: >-
            An optional list of libraries to be installed on the cluster. The
            default value is an empty list.
        new_cluster:
          type: string
          description: >-
            If new_cluster, a description of a new cluster that is created for
            each run.
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
          description: The task runs a notebook when the `notebook_task` field is present.
        notification_settings:
          $ref: '#/components/schemas/TaskNotificationSettings'
          description: >-
            Optional notification settings that are used when sending
            notifications to each of the `email_notifications` and
            `webhook_notifications` for this task run.
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
          description: >-
            The task triggers a pipeline update when the `pipeline_task` field
            is present. Only pipelines configured to use triggered more are
            supported.
        power_bi_task:
          $ref: '#/components/schemas/PowerBiTask'
          description: >-
            The task triggers a Power BI semantic model update when the
            `power_bi_task` field is present.
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
          description: >-
            The task runs a Python wheel when the `python_wheel_task` field is
            present.
        queue_duration:
          type: integer
          description: The time in milliseconds that the run has spent in the queue.
        resolved_values:
          $ref: '#/components/schemas/ResolvedValues'
          description: Parameter values including resolved references
        run_duration:
          type: integer
          description: >-
            The time in milliseconds it took the job run and all of its repairs
            to finish.
        run_id:
          type: integer
          description: The ID of the task run.
        run_if:
          $ref: '#/components/schemas/RunIf'
          description: >-
            An optional value indicating the condition that determines whether
            the task should be run once its dependencies have been completed.
            When omitted, defaults to `ALL_SUCCESS`. See :method:jobs/create for
            a list of possible values.
        run_job_task:
          $ref: '#/components/schemas/RunJobTask'
          description: >-
            The task triggers another job when the `run_job_task` field is
            present.
        run_page_url:
          type: string
        setup_duration:
          type: integer
          description: >-
            The time in milliseconds it took to set up the cluster. For runs
            that run on new clusters this is the cluster creation time, for runs
            that run on existing clusters this time should be very short. The
            duration of a task run is the sum of the `setup_duration`,
            `execution_duration`, and the `cleanup_duration`. The
            `setup_duration` field is set to 0 for multitask job runs. The total
            duration of a multitask job run is the value of the `run_duration`
            field.
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
          description: The task runs a JAR when the `spark_jar_task` field is present.
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
          description: >-
            The task runs a Python file when the `spark_python_task` field is
            present.
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
          description: >-
            (Legacy) The task runs the spark-submit script when the
            spark_submit_task field is present. Databricks recommends using the
            spark_jar_task instead; see [Spark Submit task for
            jobs](/jobs/spark-submit).
        sql_task:
          $ref: '#/components/schemas/SqlTask'
          description: >-
            The task runs a SQL query or file, or it refreshes a SQL alert or a
            legacy SQL dashboard when the `sql_task` field is present.
        start_time:
          type: integer
          description: >-
            The time at which this run was started in epoch milliseconds
            (milliseconds since 1/1/1970 UTC). This may not be the time when the
            job task starts executing, for example, if the job is scheduled to
            run on a new cluster, this is the time the cluster creation call is
            issued.
        state:
          $ref: '#/components/schemas/RunState'
          description: Deprecated. Please use the `status` field instead.
        status:
          $ref: '#/components/schemas/RunStatus'
        timeout_seconds:
          type: integer
          description: >-
            An optional timeout applied to each run of this job task. A value of
            `0` means no timeout.
        webhook_notifications:
          $ref: '#/components/schemas/WebhookNotifications'
          description: >-
            A collection of system notification IDs to notify when the run
            begins or completes. The default behavior is to not send any system
            notifications. Task webhooks respect the task notification settings.
      required:
        - task_key
      description: Used when outputting a child run, in GetRun or ListRuns.
    SparkJarTask:
      type: object
      properties:
        jar_uri:
          type: string
        main_class_name:
          type: string
          description: >-
            The full name of the class containing the main method to be
            executed. This class must be contained in a JAR provided as a
            library. The code must use `SparkContext.getOrCreate` to obtain a
            Spark context; otherwise, runs of the job fail.
        parameters:
          type: array
          items:
            type: string
          description: >-
            Parameters passed to the main method. Use [Task parameter variables]
            to set parameters containing information about job runs. [Task
            parameter variables]:
            https://docs.databricks.com/jobs.html#parameter-variables
        run_as_repl:
          type: boolean
          description: Deprecated. A value of `false` is no longer supported.
    SparkPythonTask:
      type: object
      properties:
        python_file:
          type: string
        parameters:
          type: array
          items:
            type: string
          description: >-
            Command line parameters passed to the Python file. Use [Task
            parameter variables] to set parameters containing information about
            job runs. [Task parameter variables]:
            https://docs.databricks.com/jobs.html#parameter-variables
        source:
          $ref: '#/components/schemas/Source'
          description: >-
            Optional location type of the Python file. When set to `WORKSPACE`
            or not specified, the file will be retrieved from the local
            Databricks workspace or cloud location (if the `python_file` has a
            URI format). When set to `GIT`, the Python file will be retrieved
            from a Git repository defined in `git_source`. * `WORKSPACE`: The
            Python file is located in a Databricks workspace or at a cloud
            filesystem URI. * `GIT`: The Python file is located in a remote Git
            repository.
      required:
        - python_file
    SparkSubmitTask:
      type: object
      properties:
        parameters:
          type: array
          items:
            type: string
    SqlAlertOutput:
      type: object
      properties:
        alert_state:
          $ref: '#/components/schemas/SqlAlertState'
        output_link:
          type: string
          description: The link to find the output results.
        query_text:
          type: string
          description: >-
            The text of the SQL query. Can Run permission of the SQL query
            associated with the SQL alert is required to view this field.
        sql_statements:
          type: array
          items:
            $ref: '#/components/schemas/SqlStatementOutput'
          description: Information about SQL statements executed in the run.
        warehouse_id:
          type: string
          description: The canonical identifier of the SQL warehouse.
    SqlDashboardOutput:
      type: object
      properties:
        warehouse_id:
          type: string
        widgets:
          type: array
          items:
            $ref: '#/components/schemas/SqlDashboardWidgetOutput'
          description: >-
            Widgets executed in the run. Only SQL query based widgets are
            listed.
    SqlDashboardWidgetOutput:
      type: object
      properties:
        end_time:
          type: integer
        error:
          $ref: '#/components/schemas/SqlOutputError'
          description: The information about the error when execution fails.
        output_link:
          type: string
          description: The link to find the output results.
        start_time:
          type: integer
          description: >-
            Time (in epoch milliseconds) when execution of the SQL widget
            starts.
        status:
          $ref: '#/components/schemas/SqlDashboardWidgetOutputStatus'
          description: The execution status of the SQL widget.
        widget_id:
          type: string
          description: The canonical identifier of the SQL widget.
        widget_title:
          type: string
          description: The title of the SQL widget.
    SqlOutput:
      type: object
      properties:
        alert_output:
          $ref: '#/components/schemas/SqlAlertOutput'
        dashboard_output:
          $ref: '#/components/schemas/SqlDashboardOutput'
          description: The output of a SQL dashboard task, if available.
        query_output:
          $ref: '#/components/schemas/SqlQueryOutput'
          description: The output of a SQL query task, if available.
    SqlOutputError:
      type: object
      properties:
        message:
          type: string
    SqlQueryOutput:
      type: object
      properties:
        endpoint_id:
          type: string
        output_link:
          type: string
          description: The link to find the output results.
        query_text:
          type: string
          description: >-
            The text of the SQL query. Can Run permission of the SQL query is
            required to view this field.
        sql_statements:
          type: array
          items:
            $ref: '#/components/schemas/SqlStatementOutput'
          description: Information about SQL statements executed in the run.
        warehouse_id:
          type: string
          description: The canonical identifier of the SQL warehouse.
    SqlStatementOutput:
      type: object
      properties:
        lookup_key:
          type: string
    SqlTask:
      type: object
      properties:
        warehouse_id:
          type: string
        alert:
          $ref: '#/components/schemas/SqlTaskAlert'
          description: If alert, indicates that this job must refresh a SQL alert.
        dashboard:
          $ref: '#/components/schemas/SqlTaskDashboard'
          description: If dashboard, indicates that this job must refresh a SQL dashboard.
        file:
          $ref: '#/components/schemas/SqlTaskFile'
          description: >-
            If file, indicates that this job runs a SQL file in a remote Git
            repository.
        parameters:
          type: object
          description: >-
            Parameters to be used for each run of this job. The SQL alert task
            does not support custom parameters.
        query:
          $ref: '#/components/schemas/SqlTaskQuery'
          description: If query, indicates that this job must execute a SQL query.
      required:
        - warehouse_id
    SqlTaskAlert:
      type: object
      properties:
        alert_id:
          type: string
        pause_subscriptions:
          type: boolean
          description: If true, the alert notifications are not sent to subscribers.
        subscriptions:
          type: array
          items:
            $ref: '#/components/schemas/SqlTaskSubscription'
          description: If specified, alert notifications are sent to subscribers.
      required:
        - alert_id
    SqlTaskDashboard:
      type: object
      properties:
        dashboard_id:
          type: string
        custom_subject:
          type: string
          description: Subject of the email sent to subscribers of this task.
        pause_subscriptions:
          type: boolean
          description: >-
            If true, the dashboard snapshot is not taken, and emails are not
            sent to subscribers.
        subscriptions:
          type: array
          items:
            $ref: '#/components/schemas/SqlTaskSubscription'
          description: If specified, dashboard snapshots are sent to subscriptions.
      required:
        - dashboard_id
    SqlTaskFile:
      type: object
      properties:
        path:
          type: string
        source:
          $ref: '#/components/schemas/Source'
          description: >-
            Optional location type of the SQL file. When set to `WORKSPACE`, the
            SQL file will be retrieved from the local Databricks workspace. When
            set to `GIT`, the SQL file will be retrieved from a Git repository
            defined in `git_source`. If the value is empty, the task will use
            `GIT` if `git_source` is defined and `WORKSPACE` otherwise. *
            `WORKSPACE`: SQL file is located in Databricks workspace. * `GIT`:
            SQL file is located in cloud Git provider.
      required:
        - path
    SqlTaskQuery:
      type: object
      properties:
        query_id:
          type: string
      required:
        - query_id
    SqlTaskSubscription:
      type: object
      properties:
        destination_id:
          type: string
        user_name:
          type: string
          description: >-
            The user name to receive the subscription email. This parameter is
            mutually exclusive with destination_id. You cannot set both
            destination_id and user_name for subscription notifications.
    SubmitRunResponse:
      type: object
      properties:
        run_id:
          type: integer
          description: The canonical identifier for the newly submitted run.
      description: Run was created and started successfully.
    SubmitTask:
      type: object
      properties:
        task_key:
          type: string
        clean_rooms_notebook_task:
          $ref: '#/components/schemas/CleanRoomsNotebookTask'
          description: >-
            The task runs a [clean rooms] notebook when the
            `clean_rooms_notebook_task` field is present. [clean rooms]:
            https://docs.databricks.com/clean-rooms/index.html
        compute:
          $ref: '#/components/schemas/Compute'
          description: Task level compute configuration.
        condition_task:
          $ref: '#/components/schemas/ConditionTask'
          description: >-
            The task evaluates a condition that can be used to control the
            execution of other tasks when the `condition_task` field is present.
            The condition task does not require a cluster to execute and does
            not support retries or notifications.
        dashboard_task:
          $ref: '#/components/schemas/DashboardTask'
          description: The task refreshes a dashboard and sends a snapshot to subscribers.
        dbt_cloud_task:
          $ref: '#/components/schemas/DbtCloudTask'
          description: >-
            Task type for dbt cloud, deprecated in favor of the new name
            dbt_platform_task
        dbt_platform_task:
          $ref: '#/components/schemas/DbtPlatformTask'
        dbt_task:
          $ref: '#/components/schemas/DbtTask'
          description: >-
            The task runs one or more dbt commands when the `dbt_task` field is
            present. The dbt task requires both Databricks SQL and the ability
            to use a serverless or a pro SQL warehouse.
        depends_on:
          type: array
          items:
            $ref: '#/components/schemas/TaskDependency'
          description: >-
            An optional array of objects specifying the dependency graph of the
            task. All tasks specified in this field must complete successfully
            before executing this task. The key is `task_key`, and the value is
            the name assigned to the dependent task.
        description:
          type: string
          description: An optional description for this task.
        email_notifications:
          $ref: '#/components/schemas/JobEmailNotifications'
          description: >-
            An optional set of email addresses notified when the task run begins
            or completes. The default behavior is to not send any emails.
        environment_key:
          type: string
          description: >-
            The key that references an environment spec in a job. This field is
            required for Python script, Python wheel and dbt tasks when using
            serverless compute.
        existing_cluster_id:
          type: string
          description: >-
            If existing_cluster_id, the ID of an existing cluster that is used
            for all runs. When running jobs or tasks on an existing cluster, you
            may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater
            reliability
        for_each_task:
          $ref: '#/components/schemas/ForEachTask'
          description: >-
            The task executes a nested task for every input provided when the
            `for_each_task` field is present.
        gen_ai_compute_task:
          $ref: '#/components/schemas/GenAiComputeTask'
        health:
          $ref: '#/components/schemas/JobsHealthRules'
        libraries:
          type: string
          description: >-
            An optional list of libraries to be installed on the cluster. The
            default value is an empty list.
        new_cluster:
          type: string
          description: >-
            If new_cluster, a description of a new cluster that is created for
            each run.
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
          description: The task runs a notebook when the `notebook_task` field is present.
        notification_settings:
          $ref: '#/components/schemas/TaskNotificationSettings'
          description: >-
            Optional notification settings that are used when sending
            notifications to each of the `email_notifications` and
            `webhook_notifications` for this task run.
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
          description: >-
            The task triggers a pipeline update when the `pipeline_task` field
            is present. Only pipelines configured to use triggered more are
            supported.
        power_bi_task:
          $ref: '#/components/schemas/PowerBiTask'
          description: >-
            The task triggers a Power BI semantic model update when the
            `power_bi_task` field is present.
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
          description: >-
            The task runs a Python wheel when the `python_wheel_task` field is
            present.
        run_if:
          $ref: '#/components/schemas/RunIf'
          description: >-
            An optional value indicating the condition that determines whether
            the task should be run once its dependencies have been completed.
            When omitted, defaults to `ALL_SUCCESS`. See :method:jobs/create for
            a list of possible values.
        run_job_task:
          $ref: '#/components/schemas/RunJobTask'
          description: >-
            The task triggers another job when the `run_job_task` field is
            present.
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
          description: The task runs a JAR when the `spark_jar_task` field is present.
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
          description: >-
            The task runs a Python file when the `spark_python_task` field is
            present.
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
          description: >-
            (Legacy) The task runs the spark-submit script when the
            spark_submit_task field is present. Databricks recommends using the
            spark_jar_task instead; see [Spark Submit task for
            jobs](/jobs/spark-submit).
        sql_task:
          $ref: '#/components/schemas/SqlTask'
          description: >-
            The task runs a SQL query or file, or it refreshes a SQL alert or a
            legacy SQL dashboard when the `sql_task` field is present.
        timeout_seconds:
          type: integer
          description: >-
            An optional timeout applied to each run of this job task. A value of
            `0` means no timeout.
        webhook_notifications:
          $ref: '#/components/schemas/WebhookNotifications'
          description: >-
            A collection of system notification IDs to notify when the run
            begins or completes. The default behavior is to not send any system
            notifications. Task webhooks respect the task notification settings.
      required:
        - task_key
    Subscription:
      type: object
      properties:
        custom_subject:
          type: string
        paused:
          type: boolean
          description: When true, the subscription will not send emails.
        subscribers:
          type: array
          items:
            $ref: '#/components/schemas/SubscriptionSubscriber'
          description: The list of subscribers to send the snapshot of the dashboard to.
    SubscriptionSubscriber:
      type: object
      properties:
        destination_id:
          type: string
        user_name:
          type: string
          description: >-
            A snapshot of the dashboard will be sent to the user's email when
            the `user_name` field is present.
    TableState:
      type: object
      properties:
        has_seen_updates:
          type: boolean
        table_name:
          type: string
          description: >-
            Full table name of the table to monitor, e.g.
            `mycatalog.myschema.mytable`
    TableTriggerState:
      type: object
      properties:
        last_seen_table_states:
          type: array
          items:
            $ref: '#/components/schemas/TableState'
        using_scalable_monitoring:
          type: boolean
          description: Indicates whether the trigger is using scalable monitoring.
    TableUpdateTriggerConfiguration:
      type: object
      properties:
        table_names:
          type: array
          items:
            type: string
        condition:
          $ref: '#/components/schemas/Condition'
          description: The table(s) condition based on which to trigger a job run.
        min_time_between_triggers_seconds:
          type: integer
          description: >-
            If set, the trigger starts a run only after the specified amount of
            time has passed since the last time the trigger fired. The minimum
            allowed value is 60 seconds.
        wait_after_last_change_seconds:
          type: integer
          description: >-
            If set, the trigger starts a run only after no table updates have
            occurred for the specified time and can be used to wait for a series
            of table updates before triggering a run. The minimum allowed value
            is 60 seconds.
      required:
        - table_names
    Task:
      type: object
      properties:
        task_key:
          type: string
        clean_rooms_notebook_task:
          $ref: '#/components/schemas/CleanRoomsNotebookTask'
          description: >-
            The task runs a [clean rooms] notebook when the
            `clean_rooms_notebook_task` field is present. [clean rooms]:
            https://docs.databricks.com/clean-rooms/index.html
        compute:
          $ref: '#/components/schemas/Compute'
          description: Task level compute configuration.
        condition_task:
          $ref: '#/components/schemas/ConditionTask'
          description: >-
            The task evaluates a condition that can be used to control the
            execution of other tasks when the `condition_task` field is present.
            The condition task does not require a cluster to execute and does
            not support retries or notifications.
        dashboard_task:
          $ref: '#/components/schemas/DashboardTask'
          description: The task refreshes a dashboard and sends a snapshot to subscribers.
        dbt_cloud_task:
          $ref: '#/components/schemas/DbtCloudTask'
          description: >-
            Task type for dbt cloud, deprecated in favor of the new name
            dbt_platform_task
        dbt_platform_task:
          $ref: '#/components/schemas/DbtPlatformTask'
        dbt_task:
          $ref: '#/components/schemas/DbtTask'
          description: >-
            The task runs one or more dbt commands when the `dbt_task` field is
            present. The dbt task requires both Databricks SQL and the ability
            to use a serverless or a pro SQL warehouse.
        depends_on:
          type: array
          items:
            $ref: '#/components/schemas/TaskDependency'
          description: >-
            An optional array of objects specifying the dependency graph of the
            task. All tasks specified in this field must complete before
            executing this task. The task will run only if the `run_if`
            condition is true. The key is `task_key`, and the value is the name
            assigned to the dependent task.
        description:
          type: string
          description: An optional description for this task.
        disable_auto_optimization:
          type: boolean
          description: An option to disable auto optimization in serverless
        disabled:
          type: boolean
          description: >-
            An optional flag to disable the task. If set to true, the task will
            not run even if it is part of a job.
        email_notifications:
          $ref: '#/components/schemas/TaskEmailNotifications'
          description: >-
            An optional set of email addresses that is notified when runs of
            this task begin or complete as well as when this task is deleted.
            The default behavior is to not send any emails.
        environment_key:
          type: string
          description: >-
            The key that references an environment spec in a job. This field is
            required for Python script, Python wheel and dbt tasks when using
            serverless compute.
        existing_cluster_id:
          type: string
          description: >-
            If existing_cluster_id, the ID of an existing cluster that is used
            for all runs. When running jobs or tasks on an existing cluster, you
            may need to manually restart the cluster if it stops responding. We
            suggest running jobs and tasks on new clusters for greater
            reliability
        for_each_task:
          $ref: '#/components/schemas/ForEachTask'
          description: >-
            The task executes a nested task for every input provided when the
            `for_each_task` field is present.
        gen_ai_compute_task:
          $ref: '#/components/schemas/GenAiComputeTask'
        health:
          $ref: '#/components/schemas/JobsHealthRules'
        job_cluster_key:
          type: string
          description: >-
            If job_cluster_key, this task is executed reusing the cluster
            specified in `job.settings.job_clusters`.
        libraries:
          type: string
          description: >-
            An optional list of libraries to be installed on the cluster. The
            default value is an empty list.
        max_retries:
          type: integer
          description: >-
            An optional maximum number of times to retry an unsuccessful run. A
            run is considered to be unsuccessful if it completes with the
            `FAILED` result_state or `INTERNAL_ERROR` `life_cycle_state`. The
            value `-1` means to retry indefinitely and the value `0` means to
            never retry.
        min_retry_interval_millis:
          type: integer
          description: >-
            An optional minimal interval in milliseconds between the start of
            the failed run and the subsequent retry run. The default behavior is
            that unsuccessful runs are immediately retried.
        new_cluster:
          type: string
          description: >-
            If new_cluster, a description of a new cluster that is created for
            each run.
        notebook_task:
          $ref: '#/components/schemas/NotebookTask'
          description: The task runs a notebook when the `notebook_task` field is present.
        notification_settings:
          $ref: '#/components/schemas/TaskNotificationSettings'
          description: >-
            Optional notification settings that are used when sending
            notifications to each of the `email_notifications` and
            `webhook_notifications` for this task.
        pipeline_task:
          $ref: '#/components/schemas/PipelineTask'
          description: >-
            The task triggers a pipeline update when the `pipeline_task` field
            is present. Only pipelines configured to use triggered more are
            supported.
        power_bi_task:
          $ref: '#/components/schemas/PowerBiTask'
          description: >-
            The task triggers a Power BI semantic model update when the
            `power_bi_task` field is present.
        python_wheel_task:
          $ref: '#/components/schemas/PythonWheelTask'
          description: >-
            The task runs a Python wheel when the `python_wheel_task` field is
            present.
        retry_on_timeout:
          type: boolean
          description: >-
            An optional policy to specify whether to retry a job when it times
            out. The default behavior is to not retry on timeout.
        run_if:
          $ref: '#/components/schemas/RunIf'
          description: >-
            An optional value specifying the condition determining whether the
            task is run once its dependencies have been completed. *
            `ALL_SUCCESS`: All dependencies have executed and succeeded *
            `AT_LEAST_ONE_SUCCESS`: At least one dependency has succeeded *
            `NONE_FAILED`: None of the dependencies have failed and at least one
            was executed * `ALL_DONE`: All dependencies have been completed *
            `AT_LEAST_ONE_FAILED`: At least one dependency failed *
            `ALL_FAILED`: ALl dependencies have failed
        run_job_task:
          $ref: '#/components/schemas/RunJobTask'
          description: >-
            The task triggers another job when the `run_job_task` field is
            present.
        spark_jar_task:
          $ref: '#/components/schemas/SparkJarTask'
          description: The task runs a JAR when the `spark_jar_task` field is present.
        spark_python_task:
          $ref: '#/components/schemas/SparkPythonTask'
          description: >-
            The task runs a Python file when the `spark_python_task` field is
            present.
        spark_submit_task:
          $ref: '#/components/schemas/SparkSubmitTask'
          description: >-
            (Legacy) The task runs the spark-submit script when the
            spark_submit_task field is present. Databricks recommends using the
            spark_jar_task instead; see [Spark Submit task for
            jobs](/jobs/spark-submit).
        sql_task:
          $ref: '#/components/schemas/SqlTask'
          description: >-
            The task runs a SQL query or file, or it refreshes a SQL alert or a
            legacy SQL dashboard when the `sql_task` field is present.
        timeout_seconds:
          type: integer
          description: >-
            An optional timeout applied to each run of this job task. A value of
            `0` means no timeout.
        webhook_notifications:
          $ref: '#/components/schemas/WebhookNotifications'
          description: >-
            A collection of system notification IDs to notify when runs of this
            task begin or complete. The default behavior is to not send any
            system notifications.
      required:
        - task_key
    TaskDependency:
      type: object
      properties:
        task_key:
          type: string
        outcome:
          type: string
          description: >-
            Can only be specified on condition task dependencies. The outcome of
            the dependent task that must be met for this task to run.
      required:
        - task_key
    TaskEmailNotifications:
      type: object
      properties:
        no_alert_for_skipped_runs:
          type: boolean
        on_duration_warning_threshold_exceeded:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when the duration of a run
            exceeds the threshold specified for the `RUN_DURATION_SECONDS`
            metric in the `health` field. If no rule for the
            `RUN_DURATION_SECONDS` metric is specified in the `health` field for
            the job, notifications are not sent.
        on_failure:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when a run unsuccessfully
            completes. A run is considered to have completed unsuccessfully if
            it ends with an `INTERNAL_ERROR` `life_cycle_state` or a `FAILED`,
            or `TIMED_OUT` result_state. If this is not specified on job
            creation, reset, or update the list is empty, and notifications are
            not sent.
        on_start:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when a run begins. If not
            specified on job creation, reset, or update, the list is empty, and
            notifications are not sent.
        on_streaming_backlog_exceeded:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to notify when any streaming backlog
            thresholds are exceeded for any stream. Streaming backlog thresholds
            can be set in the `health` field using the following metrics:
            `STREAMING_BACKLOG_BYTES`, `STREAMING_BACKLOG_RECORDS`,
            `STREAMING_BACKLOG_SECONDS`, or `STREAMING_BACKLOG_FILES`. Alerting
            is based on the 10-minute average of these metrics. If the issue
            persists, notifications are resent every 30 minutes.
        on_success:
          type: array
          items:
            type: string
          description: >-
            A list of email addresses to be notified when a run successfully
            completes. A run is considered to have completed successfully if it
            ends with a `TERMINATED` `life_cycle_state` and a `SUCCESS`
            result_state. If not specified on job creation, reset, or update,
            the list is empty, and notifications are not sent.
    TaskNotificationSettings:
      type: object
      properties:
        alert_on_last_attempt:
          type: boolean
        no_alert_for_canceled_runs:
          type: boolean
          description: >-
            If true, do not send notifications to recipients specified in
            `on_failure` if the run is canceled.
        no_alert_for_skipped_runs:
          type: boolean
          description: >-
            If true, do not send notifications to recipients specified in
            `on_failure` if the run is skipped.
    TerminationDetails:
      type: object
      properties:
        code:
          $ref: '#/components/schemas/TerminationCodeCode'
        message:
          type: string
          description: >-
            A descriptive message with the termination details. This field is
            unstructured and the format might change.
        type:
          $ref: '#/components/schemas/TerminationTypeType'
    TriggerInfo:
      type: object
      properties:
        run_id:
          type: integer
          description: The run id of the Run Job task run
      description: Additional details about what triggered the run
    TriggerSettings:
      type: object
      properties:
        file_arrival:
          $ref: '#/components/schemas/FileArrivalTriggerConfiguration'
        model:
          $ref: '#/components/schemas/ModelTriggerConfiguration'
        pause_status:
          $ref: '#/components/schemas/PauseStatus'
          description: Whether this trigger is paused or not.
        periodic:
          $ref: '#/components/schemas/PeriodicTriggerConfiguration'
          description: Periodic trigger settings.
        table_update:
          $ref: '#/components/schemas/TableUpdateTriggerConfiguration'
    TriggerStateProto:
      type: object
      properties:
        file_arrival:
          $ref: '#/components/schemas/FileArrivalTriggerState'
        table:
          $ref: '#/components/schemas/TableTriggerState'
    ViewItem:
      type: object
      properties:
        content:
          type: string
        name:
          type: string
          description: >-
            Name of the view item. In the case of code view, it would be the
            notebook’s name. In the case of dashboard view, it would be the
            dashboard’s name.
        type:
          $ref: '#/components/schemas/ViewType'
          description: Type of the view item.
    Webhook:
      type: object
      properties:
        id:
          type: string
      required:
        - id
    WebhookNotifications:
      type: object
      properties:
        on_duration_warning_threshold_exceeded:
          type: array
          items:
            $ref: '#/components/schemas/Webhook'
        on_failure:
          type: array
          items:
            $ref: '#/components/schemas/Webhook'
          description: >-
            An optional list of system notification IDs to call when the run
            fails. A maximum of 3 destinations can be specified for the
            `on_failure` property.
        on_start:
          type: array
          items:
            $ref: '#/components/schemas/Webhook'
          description: >-
            An optional list of system notification IDs to call when the run
            starts. A maximum of 3 destinations can be specified for the
            `on_start` property.
        on_streaming_backlog_exceeded:
          type: array
          items:
            $ref: '#/components/schemas/Webhook'
          description: >-
            An optional list of system notification IDs to call when any
            streaming backlog thresholds are exceeded for any stream. Streaming
            backlog thresholds can be set in the `health` field using the
            following metrics: `STREAMING_BACKLOG_BYTES`,
            `STREAMING_BACKLOG_RECORDS`, `STREAMING_BACKLOG_SECONDS`, or
            `STREAMING_BACKLOG_FILES`. Alerting is based on the 10-minute
            average of these metrics. If the issue persists, notifications are
            resent every 30 minutes. A maximum of 3 destinations can be
            specified for the `on_streaming_backlog_exceeded` property.
        on_success:
          type: array
          items:
            $ref: '#/components/schemas/Webhook'
          description: >-
            An optional list of system notification IDs to call when the run
            completes successfully. A maximum of 3 destinations can be specified
            for the `on_success` property.
    WidgetErrorDetail:
      type: object
      properties:
        message:
          type: string
    AuthenticationMethod:
      type: string
      enum:
        - OAUTH
        - PAT
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    CleanRoomTaskRunLifeCycleState:
      type: string
      enum:
        - BLOCKED
        - INTERNAL_ERROR
        - PENDING
        - QUEUED
        - RUNNING
        - RUN_LIFE_CYCLE_STATE_UNSPECIFIED
        - SKIPPED
        - TERMINATED
        - TERMINATING
        - WAITING_FOR_RETRY
      description: >-
        Copied from elastic-spark-common/api/messages/runs.proto. Using the
        original definition to

        remove coupling with jobs API definition
    CleanRoomTaskRunResultState:
      type: string
      enum:
        - CANCELED
        - DISABLED
        - EVICTED
        - EXCLUDED
        - FAILED
        - MAXIMUM_CONCURRENT_RUNS_REACHED
        - RUN_RESULT_STATE_UNSPECIFIED
        - SUCCESS
        - SUCCESS_WITH_FAILURES
        - TIMEDOUT
        - UPSTREAM_CANCELED
        - UPSTREAM_EVICTED
        - UPSTREAM_FAILED
      description: >-
        Copied from elastic-spark-common/api/messages/runs.proto. Using the
        original definition to avoid

        cyclic dependency.
    Condition:
      type: string
      enum:
        - ALL_UPDATED
        - ANY_UPDATED
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    ConditionTaskOp:
      type: string
      enum:
        - EQUAL_TO
        - GREATER_THAN
        - GREATER_THAN_OR_EQUAL
        - LESS_THAN
        - LESS_THAN_OR_EQUAL
        - NOT_EQUAL
      description: >-
        * `EQUAL_TO`, `NOT_EQUAL` operators perform string comparison of their
        operands. This means that

        `“12.0” == “12”` will evaluate to `false`. * `GREATER_THAN`,
        `GREATER_THAN_OR_EQUAL`,

        `LESS_THAN`, `LESS_THAN_OR_EQUAL` operators perform numeric comparison
        of their operands.

        `“12.0” >= “12”` will evaluate to `true`, `“10.0” >= “12”` will evaluate
        to

        `false`.


        The boolean comparison to task values can be implemented with operators
        `EQUAL_TO`, `NOT_EQUAL`.

        If a task value was set to a boolean value, it will be serialized to
        `“true”` or

        `“false”` for the comparison.
    DbtPlatformRunStatus:
      type: string
      enum:
        - CANCELLED
        - ERROR
        - QUEUED
        - RUNNING
        - STARTING
        - SUCCESS
      description: >-
        Response enumeration from calling the dbt platform API, for inclusion in
        output
    Format:
      type: string
      enum:
        - MULTI_TASK
        - SINGLE_TASK
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    GitProvider:
      type: string
      enum:
        - awsCodeCommit
        - azureDevOpsServices
        - bitbucketCloud
        - bitbucketServer
        - gitHub
        - gitHubEnterprise
        - gitLab
        - gitLabEnterpriseEdition
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    JobDeploymentKind:
      type: string
      enum:
        - BUNDLE
        - SYSTEM_MANAGED
      description: >-
        * `BUNDLE`: The job is managed by Databricks Asset Bundle. *
        `SYSTEM_MANAGED`: The job is

        managed by Databricks and is read-only.
    JobEditMode:
      type: string
      enum:
        - EDITABLE
        - UI_LOCKED
      description: >-
        Edit mode of the job.


        * `UI_LOCKED`: The job is in a locked UI state and cannot be modified. *
        `EDITABLE`: The job is

        in an editable state and can be modified.
    JobPermissionLevel:
      type: string
      enum:
        - CAN_MANAGE
        - CAN_MANAGE_RUN
        - CAN_VIEW
        - IS_OWNER
      description: Permission level
    JobSourceDirtyState:
      type: string
      enum:
        - DISCONNECTED
        - NOT_SYNCED
      description: >-
        Dirty state indicates the job is not fully synced with the job
        specification in the remote

        repository.


        Possible values are: * `NOT_SYNCED`: The job is not yet synced with the
        remote job

        specification. Import the remote job specification from UI to make the
        job fully synced. *

        `DISCONNECTED`: The job is temporary disconnected from the remote job
        specification and is

        allowed for live edit. Import the remote job specification again from UI
        to make the job fully

        synced.
    JobsHealthMetric:
      type: string
      enum:
        - RUN_DURATION_SECONDS
        - STREAMING_BACKLOG_BYTES
        - STREAMING_BACKLOG_FILES
        - STREAMING_BACKLOG_RECORDS
        - STREAMING_BACKLOG_SECONDS
      description: >-
        Specifies the health metric that is being evaluated for a particular
        health rule.


        * `RUN_DURATION_SECONDS`: Expected total time for a run in seconds. *
        `STREAMING_BACKLOG_BYTES`:

        An estimate of the maximum bytes of data waiting to be consumed across
        all streams. This metric

        is in Public Preview. * `STREAMING_BACKLOG_RECORDS`: An estimate of the
        maximum offset lag

        across all streams. This metric is in Public Preview. *
        `STREAMING_BACKLOG_SECONDS`: An estimate

        of the maximum consumer delay across all streams. This metric is in
        Public Preview. *

        `STREAMING_BACKLOG_FILES`: An estimate of the maximum number of
        outstanding files across all

        streams. This metric is in Public Preview.
    JobsHealthOperator:
      type: string
      enum:
        - GREATER_THAN
      description: >-
        Specifies the operator used to compare the health metric value with the
        specified threshold.
    ModelTriggerConfigurationCondition:
      type: string
      enum:
        - MODEL_ALIAS_SET
        - MODEL_CREATED
        - MODEL_VERSION_READY
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    PauseStatus:
      type: string
      enum:
        - PAUSED
        - UNPAUSED
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    PerformanceTarget:
      type: string
      enum:
        - PERFORMANCE_OPTIMIZED
        - STANDARD
      description: >-
        PerformanceTarget defines how performant (lower latency) or cost
        efficient the execution of run

        on serverless compute should be. The performance mode on the job or
        pipeline should map to a

        performance setting that is passed to Cluster Manager (see
        cluster-common PerformanceTarget).
    PeriodicTriggerConfigurationTimeUnit:
      type: string
      enum:
        - DAYS
        - HOURS
        - WEEKS
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    QueueDetailsCodeCode:
      type: string
      enum:
        - ACTIVE_RUNS_LIMIT_REACHED
        - ACTIVE_RUN_JOB_TASKS_LIMIT_REACHED
        - MAX_CONCURRENT_RUNS_REACHED
      description: >-
        The reason for queuing the run. * `ACTIVE_RUNS_LIMIT_REACHED`: The run
        was queued due to

        reaching the workspace limit of active task runs. *
        `MAX_CONCURRENT_RUNS_REACHED`: The run was

        queued due to reaching the per-job limit of concurrent job runs. *

        `ACTIVE_RUN_JOB_TASKS_LIMIT_REACHED`: The run was queued due to reaching
        the workspace limit of

        active run job tasks.
    RepairHistoryItemType:
      type: string
      enum:
        - ORIGINAL
        - REPAIR
      description: >-
        The repair history item type. Indicates whether a run is the original
        run or a repair run.
    RunIf:
      type: string
      enum:
        - ALL_DONE
        - ALL_FAILED
        - ALL_SUCCESS
        - AT_LEAST_ONE_FAILED
        - AT_LEAST_ONE_SUCCESS
        - NONE_FAILED
      description: >-
        An optional value indicating the condition that determines whether the
        task should be run once

        its dependencies have been completed. When omitted, defaults to
        `ALL_SUCCESS`.


        Possible values are: * `ALL_SUCCESS`: All dependencies have executed and
        succeeded *

        `AT_LEAST_ONE_SUCCESS`: At least one dependency has succeeded *
        `NONE_FAILED`: None of the

        dependencies have failed and at least one was executed * `ALL_DONE`: All
        dependencies have been

        completed * `AT_LEAST_ONE_FAILED`: At least one dependency failed *
        `ALL_FAILED`: ALl

        dependencies have failed
    RunLifeCycleState:
      type: string
      enum:
        - BLOCKED
        - INTERNAL_ERROR
        - PENDING
        - QUEUED
        - RUNNING
        - SKIPPED
        - TERMINATED
        - TERMINATING
        - WAITING_FOR_RETRY
      description: >-
        A value indicating the run's lifecycle state. The possible values are: *
        `QUEUED`: The run is

        queued. * `PENDING`: The run is waiting to be executed while the cluster
        and execution context

        are being prepared. * `RUNNING`: The task of this run is being executed.
        * `TERMINATING`: The

        task of this run has completed, and the cluster and execution context
        are being cleaned up. *

        `TERMINATED`: The task of this run has completed, and the cluster and
        execution context have

        been cleaned up. This state is terminal. * `SKIPPED`: This run was
        aborted because a previous

        run of the same job was already active. This state is terminal. *
        `INTERNAL_ERROR`: An

        exceptional state that indicates a failure in the Jobs service, such as
        network failure over a

        long period. If a run on a new cluster ends in the `INTERNAL_ERROR`
        state, the Jobs service

        terminates the cluster as soon as possible. This state is terminal. *
        `BLOCKED`: The run is

        blocked on an upstream dependency. * `WAITING_FOR_RETRY`: The run is
        waiting for a retry.
    RunLifecycleStateV2State:
      type: string
      enum:
        - BLOCKED
        - PENDING
        - QUEUED
        - RUNNING
        - TERMINATED
        - TERMINATING
        - WAITING
      description: The current state of the run.
    RunResultState:
      type: string
      enum:
        - CANCELED
        - DISABLED
        - EXCLUDED
        - FAILED
        - MAXIMUM_CONCURRENT_RUNS_REACHED
        - SUCCESS
        - SUCCESS_WITH_FAILURES
        - TIMEDOUT
        - UPSTREAM_CANCELED
        - UPSTREAM_FAILED
      description: >-
        A value indicating the run's result. The possible values are: *
        `SUCCESS`: The task completed

        successfully. * `FAILED`: The task completed with an error. *
        `TIMEDOUT`: The run was stopped

        after reaching the timeout. * `CANCELED`: The run was canceled at user
        request. *

        `MAXIMUM_CONCURRENT_RUNS_REACHED`: The run was skipped because the
        maximum concurrent runs were

        reached. * `EXCLUDED`: The run was skipped because the necessary
        conditions were not met. *

        `SUCCESS_WITH_FAILURES`: The job run completed successfully with some
        failures; leaf tasks were

        successful. * `UPSTREAM_FAILED`: The run was skipped because of an
        upstream failure. *

        `UPSTREAM_CANCELED`: The run was skipped because an upstream task was
        canceled. * `DISABLED`:

        The run was skipped because it was disabled explicitly by the user.
    RunType:
      type: string
      enum:
        - JOB_RUN
        - SUBMIT_RUN
        - WORKFLOW_RUN
      description: >-
        The type of a run. * `JOB_RUN`: Normal job run. A run created with
        :method:jobs/runNow. *

        `WORKFLOW_RUN`: Workflow run. A run created with [dbutils.notebook.run].
        * `SUBMIT_RUN`: Submit

        run. A run created with :method:jobs/submit.


        [dbutils.notebook.run]:
        https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-workflow
    Source:
      type: string
      enum:
        - GIT
        - WORKSPACE
      description: >-
        Optional location type of the SQL file. When set to `WORKSPACE`, the SQL
        file will be retrieved    from the local Databricks workspace. When set
        to `GIT`, the SQL file will be retrieved from a

        Git repository defined in `git_source`. If the value is empty, the task
        will use `GIT` if

        `git_source` is defined and `WORKSPACE` otherwise.


        * `WORKSPACE`: SQL file is located in Databricks workspace. * `GIT`: SQL
        file is located in

        cloud Git provider.
    SqlAlertState:
      type: string
      enum:
        - OK
        - TRIGGERED
        - UNKNOWN
      description: >-
        The state of the SQL alert.


        * UNKNOWN: alert yet to be evaluated * OK: alert evaluated and did not
        fulfill trigger

        conditions * TRIGGERED: alert evaluated and fulfilled trigger conditions
    SqlDashboardWidgetOutputStatus:
      type: string
      enum:
        - CANCELLED
        - FAILED
        - PENDING
        - RUNNING
        - SUCCESS
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    StorageMode:
      type: string
      enum:
        - DIRECT_QUERY
        - DUAL
        - IMPORT
      description: |-
        Create a collection of name/value pairs.

        Example enumeration:

        >>> class Color(Enum):
        ...     RED = 1
        ...     BLUE = 2
        ...     GREEN = 3

        Access them by:

        - attribute access::

        >>> Color.RED
        <Color.RED: 1>

        - value lookup:

        >>> Color(1)
        <Color.RED: 1>

        - name lookup:

        >>> Color['RED']
        <Color.RED: 1>

        Enumerations can be iterated over, and know how many members they have:

        >>> len(Color)
        3

        >>> list(Color)
        [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]

        Methods can be added to enumerations, and members can have their own
        attributes -- see the documentation for details.
    TaskRetryMode:
      type: string
      enum:
        - NEVER
        - ON_FAILURE
      description: >-
        task retry mode of the continuous job * NEVER: The failed task will not
        be retried. *

        ON_FAILURE: Retry a failed task if at least one other task in the job is
        still running its first

        attempt. When this condition is no longer met or the retry limit is
        reached, the job run is

        cancelled and a new run is started.
    TerminationCodeCode:
      type: string
      enum:
        - BUDGET_POLICY_LIMIT_EXCEEDED
        - CANCELED
        - CLOUD_FAILURE
        - CLUSTER_ERROR
        - CLUSTER_REQUEST_LIMIT_EXCEEDED
        - DISABLED
        - DRIVER_ERROR
        - FEATURE_DISABLED
        - INTERNAL_ERROR
        - INVALID_CLUSTER_REQUEST
        - INVALID_RUN_CONFIGURATION
        - LIBRARY_INSTALLATION_ERROR
        - MAX_CONCURRENT_RUNS_EXCEEDED
        - MAX_JOB_QUEUE_SIZE_EXCEEDED
        - MAX_SPARK_CONTEXTS_EXCEEDED
        - REPOSITORY_CHECKOUT_FAILED
        - RESOURCE_NOT_FOUND
        - RUN_EXECUTION_ERROR
        - SKIPPED
        - STORAGE_ACCESS_ERROR
        - SUCCESS
        - SUCCESS_WITH_FAILURES
        - UNAUTHORIZED_ERROR
        - USER_CANCELED
        - WORKSPACE_RUN_LIMIT_EXCEEDED
      description: >-
        The code indicates why the run was terminated. Additional codes might be
        introduced in future

        releases. * `SUCCESS`: The run was completed successfully. *
        `SUCCESS_WITH_FAILURES`: The run

        was completed successfully but some child runs failed. *
        `USER_CANCELED`: The run was

        successfully canceled during execution by a user. * `CANCELED`: The run
        was canceled during

        execution by the Databricks platform; for example, if the maximum run
        duration was exceeded. *

        `SKIPPED`: Run was never executed, for example, if the upstream task run
        failed, the dependency

        type condition was not met, or there were no material tasks to execute.
        * `INTERNAL_ERROR`: The

        run encountered an unexpected error. Refer to the state message for
        further details. *

        `DRIVER_ERROR`: The run encountered an error while communicating with
        the Spark Driver. *

        `CLUSTER_ERROR`: The run failed due to a cluster error. Refer to the
        state message for further

        details. * `REPOSITORY_CHECKOUT_FAILED`: Failed to complete the checkout
        due to an error when

        communicating with the third party service. * `INVALID_CLUSTER_REQUEST`:
        The run failed because

        it issued an invalid request to start the cluster. *
        `WORKSPACE_RUN_LIMIT_EXCEEDED`: The

        workspace has reached the quota for the maximum number of concurrent
        active runs. Consider

        scheduling the runs over a larger time frame. * `FEATURE_DISABLED`: The
        run failed because it

        tried to access a feature unavailable for the workspace. *
        `CLUSTER_REQUEST_LIMIT_EXCEEDED`: The

        number of cluster creation, start, and upsize requests have exceeded the
        allotted rate limit.

        Consider spreading the run execution over a larger time frame. *
        `STORAGE_ACCESS_ERROR`: The run

        failed due to an error when accessing the customer blob storage. Refer
        to the state message for

        further details. * `RUN_EXECUTION_ERROR`: The run was completed with
        task failures. For more

        details, refer to the state message or run output. *
        `UNAUTHORIZED_ERROR`: The run failed due to

        a permission issue while accessing a resource. Refer to the state
        message for further details. *

        `LIBRARY_INSTALLATION_ERROR`: The run failed while installing the
        user-requested library. Refer

        to the state message for further details. The causes might include, but
        are not limited to: The

        provided library is invalid, there are insufficient permissions to
        install the library, and so

        forth. * `MAX_CONCURRENT_RUNS_EXCEEDED`: The scheduled run exceeds the
        limit of maximum

        concurrent runs set for the job. * `MAX_SPARK_CONTEXTS_EXCEEDED`: The
        run is scheduled on a

        cluster that has already reached the maximum number of contexts it is
        configured to create. See:

        [Link]. * `RESOURCE_NOT_FOUND`: A resource necessary for run execution
        does not exist. Refer to

        the state message for further details. * `INVALID_RUN_CONFIGURATION`:
        The run failed due to an

        invalid configuration. Refer to the state message for further details. *
        `CLOUD_FAILURE`: The

        run failed due to a cloud provider issue. Refer to the state message for
        further details. *

        `MAX_JOB_QUEUE_SIZE_EXCEEDED`: The run was skipped due to reaching the
        job level queue size

        limit. * `DISABLED`: The run was never executed because it was disabled
        explicitly by the user.

        * `BREAKING_CHANGE`: Run failed because of an intentional breaking
        change in Spark, but it will

        be retried with a mitigation config.


        [Link]:
        https://kb.databricks.com/en_US/notebooks/too-many-execution-contexts-are-open-right-now
    TerminationTypeType:
      type: string
      enum:
        - CLIENT_ERROR
        - CLOUD_FAILURE
        - INTERNAL_ERROR
        - SUCCESS
      description: >-
        * `SUCCESS`: The run terminated without any issues * `INTERNAL_ERROR`:
        An error occurred in the

        Databricks platform. Please look at the [status page] or contact support
        if the issue persists.

        * `CLIENT_ERROR`: The run was terminated because of an error caused by
        user input or the job

        configuration. * `CLOUD_FAILURE`: The run was terminated because of an
        issue with your cloud

        provider.


        [status page]: https://status.databricks.com/
    TriggerType:
      type: string
      enum:
        - CONTINUOUS
        - CONTINUOUS_RESTART
        - FILE_ARRIVAL
        - ONE_TIME
        - PERIODIC
        - RETRY
        - RUN_JOB_TASK
        - TABLE
      description: >-
        The type of trigger that fired this run.


        * `PERIODIC`: Schedules that periodically trigger runs, such as a cron
        scheduler. * `ONE_TIME`:

        One time triggers that fire a single run. This occurs you triggered a
        single run on demand

        through the UI or the API. * `RETRY`: Indicates a run that is triggered
        as a retry of a

        previously failed run. This occurs when you request to re-run the job in
        case of failures. *

        `RUN_JOB_TASK`: Indicates a run that is triggered using a Run Job task.
        * `FILE_ARRIVAL`:

        Indicates a run that is triggered by a file arrival. * `CONTINUOUS`:
        Indicates a run that is

        triggered by a continuous job. * `TABLE`: Indicates a run that is
        triggered by a table update. *

        `CONTINUOUS_RESTART`: Indicates a run created by user to manually
        restart a continuous job run.

        * `MODEL`: Indicates a run that is triggered by a model update.
    ViewType:
      type: string
      enum:
        - DASHBOARD
        - NOTEBOOK
      description: '* `NOTEBOOK`: Notebook view item. * `DASHBOARD`: Dashboard view item.'
    ViewsToExport:
      type: string
      enum:
        - ALL
        - CODE
        - DASHBOARDS
      description: >-
        * `CODE`: Code view of the notebook. * `DASHBOARDS`: All dashboard views
        of the notebook. *

        `ALL`: All views of the notebook.
  x-stackQL-resources:
    jobs:
      id: databricks_workspace.jobs.jobs
      name: jobs
      title: Jobs
      methods:
        jobs_cancel_all_runs:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1cancel-all/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_cancel_run:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1cancel/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_create:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1create/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_delete:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1delete/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_delete_run:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1delete/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_export_run:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1export/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_get:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1get/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_get_permission_levels:
          operation:
            $ref: >-
              #/paths/~1api~12.0~1permissions~1jobs~1{job_id}~1permissionLevels/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_get_permissions:
          operation:
            $ref: '#/paths/~1api~12.0~1permissions~1jobs~1{job_id}/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_set_permissions:
          operation:
            $ref: '#/paths/~1api~12.0~1permissions~1jobs~1{job_id}/put'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_update_permissions:
          operation:
            $ref: '#/paths/~1api~12.0~1permissions~1jobs~1{job_id}/patch'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_get_run:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1get/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_get_run_output:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1get-output/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_list:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1list/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_list_runs:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1list/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_repair_run:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1repair/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_reset:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1reset/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_run_now:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1run-now/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_submit:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1runs~1submit/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        jobs_update:
          operation:
            $ref: '#/paths/~1api~12.2~1jobs~1update/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/jobs/methods/jobs_get_permission_levels
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_get_permissions'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_export_run'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_get'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_get_run'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_get_run_output'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_list'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_list_runs'
        insert:
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_cancel_all_runs'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_cancel_run'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_create'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_delete'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_delete_run'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_repair_run'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_reset'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_run_now'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_submit'
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_update'
        update:
          - $ref: >-
              #/components/x-stackQL-resources/jobs/methods/jobs_update_permissions
        delete: []
        replace:
          - $ref: '#/components/x-stackQL-resources/jobs/methods/jobs_set_permissions'
    policy_compliance_for_jobs:
      id: databricks_workspace.jobs.policy_compliance_for_jobs
      name: policy_compliance_for_jobs
      title: Policy Compliance For Jobs
      methods:
        policy_compliance_for_jobs_enforce_compliance:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1jobs~1enforce-compliance/post'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        policy_compliance_for_jobs_get_compliance:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1jobs~1get-compliance/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        policy_compliance_for_jobs_list_compliance:
          operation:
            $ref: '#/paths/~1api~12.0~1policies~1jobs~1list-compliance/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/policy_compliance_for_jobs/methods/policy_compliance_for_jobs_get_compliance
          - $ref: >-
              #/components/x-stackQL-resources/policy_compliance_for_jobs/methods/policy_compliance_for_jobs_list_compliance
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/policy_compliance_for_jobs/methods/policy_compliance_for_jobs_enforce_compliance
        update: []
        delete: []
        replace: []
