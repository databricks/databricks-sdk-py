{
  "openapi": "3.0.0",
  "info": {
    "title": "Databricks Compute API (workspace)",
    "description": "OpenAPI specification for the Databricks compute service (workspace-level APIs), generated from the Databricks Python SDK.",
    "version": "0.1.0"
  },
  "servers": [
    {
      "url": "https://{workspace}.cloud.databricks.com",
      "description": "Databricks workspace",
      "variables": {
        "workspace": {
          "default": "your-workspace",
          "description": "Your Databricks workspace name"
        }
      }
    }
  ],
  "paths": {
    "/api/2.0/policies/clusters/create": {
      "post": {
        "operationId": "cluster_policies_create",
        "summary": "Creates a new policy with prescribed settings.",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Creates a new policy with prescribed settings.\n\n:param definition: str (optional)\n  Policy definition document expressed in [Databricks Cluster Policy Definition Language].\n\n  [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n:param description: str (optional)\n  Additional human-readable description of the cluster policy.\n:param libraries: List[:class:`Library`] (optional)\n  A list of libraries to be installed on the next cluster restart that uses this policy. The maximum\n  number of libraries is 500.\n:param max_clusters_per_user: int (optional)\n  Max number of clusters per user that can be active using this policy. If not present, there is no\n  max limit.\n:param name: str (optional)\n  Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100\n  characters.\n:param policy_family_definition_overrides: str (optional)\n  Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON\n  document must be passed as a string and cannot be embedded in the requests.\n\n  You can use this to customize the policy definition inherited from the policy family. Policy rules\n  specified here are merged into the inherited policy definition.\n\n  [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n:param policy_family_id: str (optional)\n  ID of the policy family. The cluster policy's policy definition inherits the policy family's policy\n  definition.\n\n  Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the\n  policy definition.\n\n:returns: :class:`CreatePolicyResponse`",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "definition": {
                    "type": "string",
                    "description": "Policy definition document expressed in [Databricks Cluster Policy Definition Language]. [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html"
                  },
                  "description": {
                    "type": "string",
                    "description": "Additional human-readable description of the cluster policy."
                  },
                  "libraries": {
                    "type": "string",
                    "description": "A list of libraries to be installed on the next cluster restart that uses this policy. The maximum number of libraries is 500."
                  },
                  "max_clusters_per_user": {
                    "type": "string",
                    "description": "Max number of clusters per user that can be active using this policy. If not present, there is no max limit."
                  },
                  "name": {
                    "type": "string",
                    "description": "Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100 characters."
                  },
                  "policy_family_definition_overrides": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition. [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html"
                  },
                  "policy_family_id": {
                    "type": "string",
                    "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition."
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreatePolicyResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/delete": {
      "post": {
        "operationId": "cluster_policies_delete",
        "summary": "Delete a policy for a cluster. Clusters governed by this policy can still run, but cannot be edited.",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Delete a policy for a cluster. Clusters governed by this policy can still run, but cannot be edited.\n\n:param policy_id: str\n  The ID of the policy to delete.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "policy_id": {
                    "type": "string",
                    "description": "The ID of the policy to delete."
                  }
                },
                "required": [
                  "policy_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/edit": {
      "post": {
        "operationId": "cluster_policies_edit",
        "summary": "Update an existing policy for cluster. This operation may make some clusters governed by the previous",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Update an existing policy for cluster. This operation may make some clusters governed by the previous\npolicy invalid.\n\n:param policy_id: str\n  The ID of the policy to update.\n:param definition: str (optional)\n  Policy definition document expressed in [Databricks Cluster Policy Definition Language].\n\n  [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n:param description: str (optional)\n  Additional human-readable description of the cluster policy.\n:param libraries: List[:class:`Library`] (optional)\n  A list of libraries to be installed on the next cluster restart that uses this policy. The maximum\n  number of libraries is 500.\n:param max_clusters_per_user: int (optional)\n  Max number of clusters per user that can be active using this policy. If not present, there is no\n  max limit.\n:param name: str (optional)\n  Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100\n  characters.\n:param policy_family_definition_overrides: str (optional)\n  Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON\n  document must be passed as a string and cannot be embedded in the requests.\n\n  You can use this to customize the policy definition inherited from the policy family. Policy rules\n  specified here are merged into the inherited policy definition.\n\n  [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html\n:param policy_family_id: str (optional)\n  ID of the policy family. The cluster policy's policy definition inherits the policy family's policy\n  definition.\n\n  Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the\n  policy definition.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "policy_id": {
                    "type": "string",
                    "description": "The ID of the policy to update."
                  },
                  "definition": {
                    "type": "string",
                    "description": "Policy definition document expressed in [Databricks Cluster Policy Definition Language]. [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html"
                  },
                  "description": {
                    "type": "string",
                    "description": "Additional human-readable description of the cluster policy."
                  },
                  "libraries": {
                    "type": "string",
                    "description": "A list of libraries to be installed on the next cluster restart that uses this policy. The maximum number of libraries is 500."
                  },
                  "max_clusters_per_user": {
                    "type": "string",
                    "description": "Max number of clusters per user that can be active using this policy. If not present, there is no max limit."
                  },
                  "name": {
                    "type": "string",
                    "description": "Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100 characters."
                  },
                  "policy_family_definition_overrides": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition. [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html"
                  },
                  "policy_family_id": {
                    "type": "string",
                    "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition."
                  }
                },
                "required": [
                  "policy_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/get": {
      "get": {
        "operationId": "cluster_policies_get",
        "summary": "Get a cluster policy entity. Creation and editing is available to admins only.",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Get a cluster policy entity. Creation and editing is available to admins only.\n\n:param policy_id: str\n  Canonical unique identifier for the Cluster Policy.\n\n:returns: :class:`Policy`",
        "parameters": [
          {
            "name": "policy_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "Canonical unique identifier for the Cluster Policy."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/Policy"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/permissions/cluster-policies/{cluster_policy_id}/permissionLevels": {
      "get": {
        "operationId": "cluster_policies_get_permission_levels",
        "summary": "Gets the permission levels that a user can have on an object.",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Gets the permission levels that a user can have on an object.\n\n:param cluster_policy_id: str\n  The cluster policy for which to get or manage permissions.\n\n:returns: :class:`GetClusterPolicyPermissionLevelsResponse`",
        "parameters": [
          {
            "name": "cluster_policy_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster policy for which to get or manage permissions."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetClusterPolicyPermissionLevelsResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/permissions/cluster-policies/{cluster_policy_id}": {
      "get": {
        "operationId": "cluster_policies_get_permissions",
        "summary": "Gets the permissions of a cluster policy. Cluster policies can inherit permissions from their root",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Gets the permissions of a cluster policy. Cluster policies can inherit permissions from their root\nobject.\n\n:param cluster_policy_id: str\n  The cluster policy for which to get or manage permissions.\n\n:returns: :class:`ClusterPolicyPermissions`",
        "parameters": [
          {
            "name": "cluster_policy_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster policy for which to get or manage permissions."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterPolicyPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "put": {
        "operationId": "cluster_policies_set_permissions",
        "summary": "Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct\npermissions if none are specified. Objects can inherit permissions from their root object.\n\n:param cluster_policy_id: str\n  The cluster policy for which to get or manage permissions.\n:param access_control_list: List[:class:`ClusterPolicyAccessControlRequest`] (optional)\n\n:returns: :class:`ClusterPolicyPermissions`",
        "parameters": [
          {
            "name": "cluster_policy_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster policy for which to get or manage permissions."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "access_control_list": {
                    "type": "string",
                    "description": ":returns: :class:`ClusterPolicyPermissions`"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterPolicyPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "patch": {
        "operationId": "cluster_policies_update_permissions",
        "summary": "Updates the permissions on a cluster policy. Cluster policies can inherit permissions from their root",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Updates the permissions on a cluster policy. Cluster policies can inherit permissions from their root\nobject.\n\n:param cluster_policy_id: str\n  The cluster policy for which to get or manage permissions.\n:param access_control_list: List[:class:`ClusterPolicyAccessControlRequest`] (optional)\n\n:returns: :class:`ClusterPolicyPermissions`",
        "parameters": [
          {
            "name": "cluster_policy_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster policy for which to get or manage permissions."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "access_control_list": {
                    "type": "string",
                    "description": ":returns: :class:`ClusterPolicyPermissions`"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterPolicyPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/list": {
      "get": {
        "operationId": "cluster_policies_list",
        "summary": "Returns a list of policies accessible by the requesting user.",
        "tags": [
          "compute",
          "cluster_policies"
        ],
        "description": "Returns a list of policies accessible by the requesting user.\n\n:param sort_column: :class:`ListSortColumn` (optional)\n  The cluster policy attribute to sort by. * `POLICY_CREATION_TIME` - Sort result list by policy\n  creation time. * `POLICY_NAME` - Sort result list by policy name.\n:param sort_order: :class:`ListSortOrder` (optional)\n  The order in which the policies get listed. * `DESC` - Sort result list in descending order. * `ASC`\n  - Sort result list in ascending order.\n\n:returns: Iterator over :class:`Policy`",
        "parameters": [
          {
            "name": "sort_column",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "The cluster policy attribute to sort by. * `POLICY_CREATION_TIME` - Sort result list by policy creation time. * `POLICY_NAME` - Sort result list by policy name."
          },
          {
            "name": "sort_order",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "The order in which the policies get listed. * `DESC` - Sort result list in descending order. * `ASC` - Sort result list in ascending order."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListPoliciesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/change-owner": {
      "post": {
        "operationId": "clusters_change_owner",
        "summary": "Change the owner of the cluster. You must be an admin and the cluster must be terminated to perform",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Change the owner of the cluster. You must be an admin and the cluster must be terminated to perform\nthis operation. The service principal application ID can be supplied as an argument to\n`owner_username`.\n\n:param cluster_id: str\n:param owner_username: str\n  New owner of the cluster_id after this RPC.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": ":param owner_username: str New owner of the cluster_id after this RPC."
                  },
                  "owner_username": {
                    "type": "string"
                  }
                },
                "required": [
                  "cluster_id",
                  "owner_username"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/create": {
      "post": {
        "operationId": "clusters_create",
        "summary": "Creates a new Spark cluster. This method will acquire new instances from the cloud provider if",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Creates a new Spark cluster. This method will acquire new instances from the cloud provider if\nnecessary. This method is asynchronous; the returned ``cluster_id`` can be used to poll the cluster\nstatus. When this method returns, the cluster will be in a ``PENDING`` state. The cluster will be\nusable once it enters a ``RUNNING`` state. Note: Databricks may not be able to acquire some of the\nrequested nodes, due to cloud provider limitations (account limits, spot price, etc.) or transient\nnetwork issues.\n\nIf Databricks acquires at least 85% of the requested on-demand nodes, cluster creation will succeed.\nOtherwise the cluster will terminate with an informative error message.\n\nRather than authoring the cluster's JSON definition from scratch, Databricks recommends filling out\nthe [create compute UI] and then copying the generated JSON definition from the UI.\n\n[create compute UI]: https://docs.databricks.com/compute/configure.html\n\n:param spark_version: str\n  The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be\n  retrieved by using the :method:clusters/sparkVersions API call.\n:param apply_policy_default_values: bool (optional)\n  When set to true, fixed and default values from the policy will be used for fields that are omitted.\n  When set to false, only fixed values from the policy will be applied.\n:param autoscale: :class:`AutoScale` (optional)\n  Parameters needed in order to automatically scale clusters up and down based on load. Note:\n  autoscaling works best with DB runtime versions 3.0 or later.\n:param autotermination_minutes: int (optional)\n  Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this\n  cluster will not be automatically terminated. If specified, the threshold must be between 10 and\n  10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination.\n:param aws_attributes: :class:`AwsAttributes` (optional)\n  Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation,\n  a set of default values will be used.\n:param azure_attributes: :class:`AzureAttributes` (optional)\n  Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a\n  set of default values will be used.\n:param clone_from: :class:`CloneCluster` (optional)\n  When specified, this clones libraries from a source cluster during the creation of a new cluster.\n:param cluster_log_conf: :class:`ClusterLogConf` (optional)\n  The configuration for delivering spark logs to a long-term storage destination. Three kinds of\n  destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be\n  specified for one cluster. If the conf is given, the logs will be delivered to the destination every\n  `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination\n  of executor logs is `$destination/$clusterId/executor`.\n:param cluster_name: str (optional)\n  Cluster name requested by the user. This doesn't have to be unique. If not specified at creation,\n  the cluster name will be an empty string. For job clusters, the cluster name is automatically set\n  based on the job and job run IDs.\n:param custom_tags: Dict[str,str] (optional)\n  Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\n  instances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n  - Currently, Databricks allows at most 45 custom tags\n\n  - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags\n:param data_security_mode: :class:`DataSecurityMode` (optional)\n:param docker_image: :class:`DockerImage` (optional)\n  Custom docker image BYOC\n:param driver_instance_pool_id: str (optional)\n  The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses\n  the instance pool with id (instance_pool_id) if the driver pool is not assigned.\n:param driver_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)\n  Flexible node type configuration for the driver node.\n:param driver_node_type_id: str (optional)\n  The node type of the Spark driver. Note that this field is optional; if unset, the driver node type\n  will be set as the same value as `node_type_id` defined above.\n\n  This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both\n  driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and\n  node_type_id take precedence.\n:param enable_elastic_disk: bool (optional)\n  Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space\n  when its Spark workers are running low on disk space.\n:param enable_local_disk_encryption: bool (optional)\n  Whether to enable LUKS on cluster VMs' local disks\n:param gcp_attributes: :class:`GcpAttributes` (optional)\n  Attributes related to clusters running on Google Cloud Platform. If not specified at cluster\n  creation, a set of default values will be used.\n:param init_scripts: List[:class:`InitScriptInfo`] (optional)\n  The configuration for storing init scripts. Any number of destinations can be specified. The scripts\n  are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script\n  logs are sent to `<destination>/<cluster-ID>/init_scripts`.\n:param instance_pool_id: str (optional)\n  The optional ID of the instance pool to which the cluster belongs.\n:param is_single_node: bool (optional)\n  This field can only be used when `kind = CLASSIC_PREVIEW`.\n\n  When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`,\n  and `num_workers`\n:param kind: :class:`Kind` (optional)\n:param node_type_id: str (optional)\n  This field encodes, through a single value, the resources available to each of the Spark nodes in\n  this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute\n  intensive workloads. A list of available node types can be retrieved by using the\n  :method:clusters/listNodeTypes API call.\n:param num_workers: int (optional)\n  Number of worker nodes that this cluster should have. A cluster has one Spark Driver and\n  `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\n  Note: When reading the properties of a cluster, this field reflects the desired number of workers\n  rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10\n  workers, this field will immediately be updated to reflect the target size of 10 workers, whereas\n  the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are\n  provisioned.\n:param policy_id: str (optional)\n  The ID of the cluster policy used to create the cluster if applicable.\n:param remote_disk_throughput: int (optional)\n  If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported\n  for GCP HYPERDISK_BALANCED disks.\n:param runtime_engine: :class:`RuntimeEngine` (optional)\n  Determines the cluster's runtime engine, either standard or Photon.\n\n  This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove\n  `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`.\n\n  If left unspecified, the runtime engine defaults to standard unless the spark_version contains\n  -photon-, in which case Photon will be used.\n:param single_user_name: str (optional)\n  Single user name if data_security_mode is `SINGLE_USER`\n:param spark_conf: Dict[str,str] (optional)\n  An object containing a set of optional, user-specified Spark configuration key-value pairs. Users\n  can also pass in a string of extra JVM options to the driver and the executors via\n  `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n:param spark_env_vars: Dict[str,str] (optional)\n  An object containing a set of optional, user-specified environment variable key-value pairs. Please\n  note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while\n  launching the driver and workers.\n\n  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to\n  `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks\n  managed environmental variables are included as well.\n\n  Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\":\n  \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS\n  -Dspark.shuffle.service.enabled=true\"}`\n:param ssh_public_keys: List[str] (optional)\n  SSH public key contents that will be added to each Spark node in this cluster. The corresponding\n  private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be\n  specified.\n:param total_initial_remote_disk_size: int (optional)\n  If set, what the total initial volume size (in GB) of the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED disks.\n:param use_ml_runtime: bool (optional)\n  This field can only be used when `kind = CLASSIC_PREVIEW`.\n\n  `effective_spark_version` is determined by `spark_version` (DBR release), this field\n  `use_ml_runtime`, and whether `node_type_id` is gpu node or not.\n:param worker_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)\n  Flexible node type configuration for worker nodes.\n:param workload_type: :class:`WorkloadType` (optional)\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "spark_version": {
                    "type": "string",
                    "description": "The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
                  },
                  "apply_policy_default_values": {
                    "type": "string",
                    "description": "When set to true, fixed and default values from the policy will be used for fields that are omitted. When set to false, only fixed values from the policy will be applied."
                  },
                  "autoscale": {
                    "type": "string",
                    "description": "Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later."
                  },
                  "autotermination_minutes": {
                    "type": "string",
                    "description": "Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination."
                  },
                  "aws_attributes": {
                    "type": "string",
                    "description": "Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used."
                  },
                  "azure_attributes": {
                    "type": "string",
                    "description": "Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used."
                  },
                  "clone_from": {
                    "type": "string",
                    "description": "When specified, this clones libraries from a source cluster during the creation of a new cluster."
                  },
                  "cluster_log_conf": {
                    "type": "string",
                    "description": "The configuration for delivering spark logs to a long-term storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`."
                  },
                  "cluster_name": {
                    "type": "string",
                    "description": "Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. For job clusters, the cluster name is automatically set based on the job and job run IDs."
                  },
                  "custom_tags": {
                    "type": "string",
                    "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags"
                  },
                  "data_security_mode": {
                    "type": "string",
                    "description": ":param docker_image: :class:`DockerImage` (optional) Custom docker image BYOC"
                  },
                  "docker_image": {
                    "type": "string"
                  },
                  "driver_instance_pool_id": {
                    "type": "string",
                    "description": "The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned."
                  },
                  "driver_node_type_flexibility": {
                    "type": "string",
                    "description": "Flexible node type configuration for the driver node."
                  },
                  "driver_node_type_id": {
                    "type": "string",
                    "description": "The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and node_type_id take precedence."
                  },
                  "enable_elastic_disk": {
                    "type": "string",
                    "description": "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space."
                  },
                  "enable_local_disk_encryption": {
                    "type": "string",
                    "description": "Whether to enable LUKS on cluster VMs' local disks"
                  },
                  "gcp_attributes": {
                    "type": "string",
                    "description": "Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used."
                  },
                  "init_scripts": {
                    "type": "string",
                    "description": "The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`."
                  },
                  "instance_pool_id": {
                    "type": "string",
                    "description": "The optional ID of the instance pool to which the cluster belongs."
                  },
                  "is_single_node": {
                    "type": "string",
                    "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`"
                  },
                  "kind": {
                    "type": "string",
                    "description": ":param node_type_id: str (optional) This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
                  },
                  "node_type_id": {
                    "type": "string"
                  },
                  "num_workers": {
                    "type": "string",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
                  },
                  "policy_id": {
                    "type": "string",
                    "description": "The ID of the cluster policy used to create the cluster if applicable."
                  },
                  "remote_disk_throughput": {
                    "type": "string",
                    "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks."
                  },
                  "runtime_engine": {
                    "type": "string",
                    "description": "Determines the cluster's runtime engine, either standard or Photon. This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`. If left unspecified, the runtime engine defaults to standard unless the spark_version contains -photon-, in which case Photon will be used."
                  },
                  "single_user_name": {
                    "type": "string",
                    "description": "Single user name if data_security_mode is `SINGLE_USER`"
                  },
                  "spark_conf": {
                    "type": "string",
                    "description": "An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively."
                  },
                  "spark_env_vars": {
                    "type": "string",
                    "description": "An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while launching the driver and workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`"
                  },
                  "ssh_public_keys": {
                    "type": "string",
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified."
                  },
                  "total_initial_remote_disk_size": {
                    "type": "string",
                    "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks."
                  },
                  "use_ml_runtime": {
                    "type": "string",
                    "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. `effective_spark_version` is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is gpu node or not."
                  },
                  "worker_node_type_flexibility": {
                    "type": "string",
                    "description": "Flexible node type configuration for worker nodes."
                  },
                  "workload_type": {
                    "type": "string",
                    "description": ":returns: Long-running operation waiter for :class:`ClusterDetails`. See :method:wait_get_cluster_running for more details."
                  }
                },
                "required": [
                  "spark_version"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/delete": {
      "post": {
        "operationId": "clusters_delete",
        "summary": "Terminates the Spark cluster with the specified ID. The cluster is removed asynchronously. Once the",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Terminates the Spark cluster with the specified ID. The cluster is removed asynchronously. Once the\ntermination has completed, the cluster will be in a `TERMINATED` state. If the cluster is already in a\n`TERMINATING` or `TERMINATED` state, nothing will happen.\n\n:param cluster_id: str\n  The cluster to be terminated.\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_terminated for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The cluster to be terminated."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/edit": {
      "post": {
        "operationId": "clusters_edit",
        "summary": "Updates the configuration of a cluster to match the provided attributes and size. A cluster can be",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Updates the configuration of a cluster to match the provided attributes and size. A cluster can be\nupdated if it is in a `RUNNING` or `TERMINATED` state.\n\nIf a cluster is updated while in a `RUNNING` state, it will be restarted so that the new attributes\ncan take effect.\n\nIf a cluster is updated while in a `TERMINATED` state, it will remain `TERMINATED`. The next time it\nis started using the `clusters/start` API, the new attributes will take effect. Any attempt to update\na cluster in any other state will be rejected with an `INVALID_STATE` error code.\n\nClusters created by the Databricks Jobs service cannot be edited.\n\n:param cluster_id: str\n  ID of the cluster\n:param spark_version: str\n  The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be\n  retrieved by using the :method:clusters/sparkVersions API call.\n:param apply_policy_default_values: bool (optional)\n  When set to true, fixed and default values from the policy will be used for fields that are omitted.\n  When set to false, only fixed values from the policy will be applied.\n:param autoscale: :class:`AutoScale` (optional)\n  Parameters needed in order to automatically scale clusters up and down based on load. Note:\n  autoscaling works best with DB runtime versions 3.0 or later.\n:param autotermination_minutes: int (optional)\n  Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this\n  cluster will not be automatically terminated. If specified, the threshold must be between 10 and\n  10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination.\n:param aws_attributes: :class:`AwsAttributes` (optional)\n  Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation,\n  a set of default values will be used.\n:param azure_attributes: :class:`AzureAttributes` (optional)\n  Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a\n  set of default values will be used.\n:param cluster_log_conf: :class:`ClusterLogConf` (optional)\n  The configuration for delivering spark logs to a long-term storage destination. Three kinds of\n  destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be\n  specified for one cluster. If the conf is given, the logs will be delivered to the destination every\n  `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination\n  of executor logs is `$destination/$clusterId/executor`.\n:param cluster_name: str (optional)\n  Cluster name requested by the user. This doesn't have to be unique. If not specified at creation,\n  the cluster name will be an empty string. For job clusters, the cluster name is automatically set\n  based on the job and job run IDs.\n:param custom_tags: Dict[str,str] (optional)\n  Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\n  instances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n  - Currently, Databricks allows at most 45 custom tags\n\n  - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags\n:param data_security_mode: :class:`DataSecurityMode` (optional)\n:param docker_image: :class:`DockerImage` (optional)\n  Custom docker image BYOC\n:param driver_instance_pool_id: str (optional)\n  The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses\n  the instance pool with id (instance_pool_id) if the driver pool is not assigned.\n:param driver_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)\n  Flexible node type configuration for the driver node.\n:param driver_node_type_id: str (optional)\n  The node type of the Spark driver. Note that this field is optional; if unset, the driver node type\n  will be set as the same value as `node_type_id` defined above.\n\n  This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both\n  driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and\n  node_type_id take precedence.\n:param enable_elastic_disk: bool (optional)\n  Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space\n  when its Spark workers are running low on disk space.\n:param enable_local_disk_encryption: bool (optional)\n  Whether to enable LUKS on cluster VMs' local disks\n:param gcp_attributes: :class:`GcpAttributes` (optional)\n  Attributes related to clusters running on Google Cloud Platform. If not specified at cluster\n  creation, a set of default values will be used.\n:param init_scripts: List[:class:`InitScriptInfo`] (optional)\n  The configuration for storing init scripts. Any number of destinations can be specified. The scripts\n  are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script\n  logs are sent to `<destination>/<cluster-ID>/init_scripts`.\n:param instance_pool_id: str (optional)\n  The optional ID of the instance pool to which the cluster belongs.\n:param is_single_node: bool (optional)\n  This field can only be used when `kind = CLASSIC_PREVIEW`.\n\n  When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`,\n  and `num_workers`\n:param kind: :class:`Kind` (optional)\n:param node_type_id: str (optional)\n  This field encodes, through a single value, the resources available to each of the Spark nodes in\n  this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute\n  intensive workloads. A list of available node types can be retrieved by using the\n  :method:clusters/listNodeTypes API call.\n:param num_workers: int (optional)\n  Number of worker nodes that this cluster should have. A cluster has one Spark Driver and\n  `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\n  Note: When reading the properties of a cluster, this field reflects the desired number of workers\n  rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10\n  workers, this field will immediately be updated to reflect the target size of 10 workers, whereas\n  the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are\n  provisioned.\n:param policy_id: str (optional)\n  The ID of the cluster policy used to create the cluster if applicable.\n:param remote_disk_throughput: int (optional)\n  If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported\n  for GCP HYPERDISK_BALANCED disks.\n:param runtime_engine: :class:`RuntimeEngine` (optional)\n  Determines the cluster's runtime engine, either standard or Photon.\n\n  This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove\n  `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`.\n\n  If left unspecified, the runtime engine defaults to standard unless the spark_version contains\n  -photon-, in which case Photon will be used.\n:param single_user_name: str (optional)\n  Single user name if data_security_mode is `SINGLE_USER`\n:param spark_conf: Dict[str,str] (optional)\n  An object containing a set of optional, user-specified Spark configuration key-value pairs. Users\n  can also pass in a string of extra JVM options to the driver and the executors via\n  `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n:param spark_env_vars: Dict[str,str] (optional)\n  An object containing a set of optional, user-specified environment variable key-value pairs. Please\n  note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while\n  launching the driver and workers.\n\n  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to\n  `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks\n  managed environmental variables are included as well.\n\n  Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\":\n  \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS\n  -Dspark.shuffle.service.enabled=true\"}`\n:param ssh_public_keys: List[str] (optional)\n  SSH public key contents that will be added to each Spark node in this cluster. The corresponding\n  private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be\n  specified.\n:param total_initial_remote_disk_size: int (optional)\n  If set, what the total initial volume size (in GB) of the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED disks.\n:param use_ml_runtime: bool (optional)\n  This field can only be used when `kind = CLASSIC_PREVIEW`.\n\n  `effective_spark_version` is determined by `spark_version` (DBR release), this field\n  `use_ml_runtime`, and whether `node_type_id` is gpu node or not.\n:param worker_node_type_flexibility: :class:`NodeTypeFlexibility` (optional)\n  Flexible node type configuration for worker nodes.\n:param workload_type: :class:`WorkloadType` (optional)\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "ID of the cluster"
                  },
                  "spark_version": {
                    "type": "string",
                    "description": "The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
                  },
                  "apply_policy_default_values": {
                    "type": "string",
                    "description": "When set to true, fixed and default values from the policy will be used for fields that are omitted. When set to false, only fixed values from the policy will be applied."
                  },
                  "autoscale": {
                    "type": "string",
                    "description": "Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later."
                  },
                  "autotermination_minutes": {
                    "type": "string",
                    "description": "Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination."
                  },
                  "aws_attributes": {
                    "type": "string",
                    "description": "Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used."
                  },
                  "azure_attributes": {
                    "type": "string",
                    "description": "Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used."
                  },
                  "cluster_log_conf": {
                    "type": "string",
                    "description": "The configuration for delivering spark logs to a long-term storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`."
                  },
                  "cluster_name": {
                    "type": "string",
                    "description": "Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. For job clusters, the cluster name is automatically set based on the job and job run IDs."
                  },
                  "custom_tags": {
                    "type": "string",
                    "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags"
                  },
                  "data_security_mode": {
                    "type": "string",
                    "description": ":param docker_image: :class:`DockerImage` (optional) Custom docker image BYOC"
                  },
                  "docker_image": {
                    "type": "string"
                  },
                  "driver_instance_pool_id": {
                    "type": "string",
                    "description": "The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned."
                  },
                  "driver_node_type_flexibility": {
                    "type": "string",
                    "description": "Flexible node type configuration for the driver node."
                  },
                  "driver_node_type_id": {
                    "type": "string",
                    "description": "The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and node_type_id take precedence."
                  },
                  "enable_elastic_disk": {
                    "type": "string",
                    "description": "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space."
                  },
                  "enable_local_disk_encryption": {
                    "type": "string",
                    "description": "Whether to enable LUKS on cluster VMs' local disks"
                  },
                  "gcp_attributes": {
                    "type": "string",
                    "description": "Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used."
                  },
                  "init_scripts": {
                    "type": "string",
                    "description": "The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`."
                  },
                  "instance_pool_id": {
                    "type": "string",
                    "description": "The optional ID of the instance pool to which the cluster belongs."
                  },
                  "is_single_node": {
                    "type": "string",
                    "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`"
                  },
                  "kind": {
                    "type": "string",
                    "description": ":param node_type_id: str (optional) This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
                  },
                  "node_type_id": {
                    "type": "string"
                  },
                  "num_workers": {
                    "type": "string",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
                  },
                  "policy_id": {
                    "type": "string",
                    "description": "The ID of the cluster policy used to create the cluster if applicable."
                  },
                  "remote_disk_throughput": {
                    "type": "string",
                    "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks."
                  },
                  "runtime_engine": {
                    "type": "string",
                    "description": "Determines the cluster's runtime engine, either standard or Photon. This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`. If left unspecified, the runtime engine defaults to standard unless the spark_version contains -photon-, in which case Photon will be used."
                  },
                  "single_user_name": {
                    "type": "string",
                    "description": "Single user name if data_security_mode is `SINGLE_USER`"
                  },
                  "spark_conf": {
                    "type": "string",
                    "description": "An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively."
                  },
                  "spark_env_vars": {
                    "type": "string",
                    "description": "An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while launching the driver and workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`"
                  },
                  "ssh_public_keys": {
                    "type": "string",
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified."
                  },
                  "total_initial_remote_disk_size": {
                    "type": "string",
                    "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks."
                  },
                  "use_ml_runtime": {
                    "type": "string",
                    "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. `effective_spark_version` is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is gpu node or not."
                  },
                  "worker_node_type_flexibility": {
                    "type": "string",
                    "description": "Flexible node type configuration for worker nodes."
                  },
                  "workload_type": {
                    "type": "string",
                    "description": ":returns: Long-running operation waiter for :class:`ClusterDetails`. See :method:wait_get_cluster_running for more details."
                  }
                },
                "required": [
                  "cluster_id",
                  "spark_version"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/events": {
      "post": {
        "operationId": "clusters_events",
        "summary": "Retrieves a list of events about the activity of a cluster. This API is paginated. If there are more",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Retrieves a list of events about the activity of a cluster. This API is paginated. If there are more\nevents to read, the response includes all the parameters necessary to request the next page of events.\n\n:param cluster_id: str\n  The ID of the cluster to retrieve events about.\n:param end_time: int (optional)\n  The end time in epoch milliseconds. If empty, returns events up to the current time.\n:param event_types: List[:class:`EventType`] (optional)\n  An optional set of event types to filter on. If empty, all event types are returned.\n:param limit: int (optional)\n  Deprecated: use page_token in combination with page_size instead.\n\n  The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed\n  value is 500.\n:param offset: int (optional)\n  Deprecated: use page_token in combination with page_size instead.\n\n  The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results\n  are requested in descending order, the end_time field is required.\n:param order: :class:`GetEventsOrder` (optional)\n  The order to list events in; either \"ASC\" or \"DESC\". Defaults to \"DESC\".\n:param page_size: int (optional)\n  The maximum number of events to include in a page of events. The server may further constrain the\n  maximum number of results returned in a single page. If the page_size is empty or 0, the server will\n  decide the number of results to be returned. The field has to be in the range [0,500]. If the value\n  is outside the range, the server enforces 0 or 500.\n:param page_token: str (optional)\n  Use next_page_token or prev_page_token returned from the previous request to list the next or\n  previous page of events respectively. If page_token is empty, the first page is returned.\n:param start_time: int (optional)\n  The start time in epoch milliseconds. If empty, returns events starting from the beginning of time.\n\n:returns: Iterator over :class:`ClusterEvent`",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The ID of the cluster to retrieve events about."
                  },
                  "end_time": {
                    "type": "string",
                    "description": "The end time in epoch milliseconds. If empty, returns events up to the current time."
                  },
                  "event_types": {
                    "type": "string",
                    "description": "An optional set of event types to filter on. If empty, all event types are returned."
                  },
                  "limit": {
                    "type": "string",
                    "description": "Deprecated: use page_token in combination with page_size instead. The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed value is 500."
                  },
                  "offset": {
                    "type": "string",
                    "description": "Deprecated: use page_token in combination with page_size instead. The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results are requested in descending order, the end_time field is required."
                  },
                  "order": {
                    "type": "string",
                    "description": "The order to list events in; either \"ASC\" or \"DESC\". Defaults to \"DESC\"."
                  },
                  "page_size": {
                    "type": "string",
                    "description": "The maximum number of events to include in a page of events. The server may further constrain the maximum number of results returned in a single page. If the page_size is empty or 0, the server will decide the number of results to be returned. The field has to be in the range [0,500]. If the value is outside the range, the server enforces 0 or 500."
                  },
                  "page_token": {
                    "type": "string",
                    "description": "Use next_page_token or prev_page_token returned from the previous request to list the next or previous page of events respectively. If page_token is empty, the first page is returned."
                  },
                  "start_time": {
                    "type": "string",
                    "description": "The start time in epoch milliseconds. If empty, returns events starting from the beginning of time."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetEventsResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/get": {
      "get": {
        "operationId": "clusters_get",
        "summary": "Retrieves the information for a cluster given its identifier. Clusters can be described while they are",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Retrieves the information for a cluster given its identifier. Clusters can be described while they are\nrunning, or up to 60 days after they are terminated.\n\n:param cluster_id: str\n  The cluster about which to retrieve information.\n\n:returns: :class:`ClusterDetails`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster about which to retrieve information."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/permissions/clusters/{cluster_id}/permissionLevels": {
      "get": {
        "operationId": "clusters_get_permission_levels",
        "summary": "Gets the permission levels that a user can have on an object.",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Gets the permission levels that a user can have on an object.\n\n:param cluster_id: str\n  The cluster for which to get or manage permissions.\n\n:returns: :class:`GetClusterPermissionLevelsResponse`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster for which to get or manage permissions."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetClusterPermissionLevelsResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/permissions/clusters/{cluster_id}": {
      "get": {
        "operationId": "clusters_get_permissions",
        "summary": "Gets the permissions of a cluster. Clusters can inherit permissions from their root object.",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Gets the permissions of a cluster. Clusters can inherit permissions from their root object.\n\n:param cluster_id: str\n  The cluster for which to get or manage permissions.\n\n:returns: :class:`ClusterPermissions`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster for which to get or manage permissions."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "put": {
        "operationId": "clusters_set_permissions",
        "summary": "Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct\npermissions if none are specified. Objects can inherit permissions from their root object.\n\n:param cluster_id: str\n  The cluster for which to get or manage permissions.\n:param access_control_list: List[:class:`ClusterAccessControlRequest`] (optional)\n\n:returns: :class:`ClusterPermissions`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster for which to get or manage permissions."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "access_control_list": {
                    "type": "string",
                    "description": ":returns: :class:`ClusterPermissions`"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "patch": {
        "operationId": "clusters_update_permissions",
        "summary": "Updates the permissions on a cluster. Clusters can inherit permissions from their root object.",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Updates the permissions on a cluster. Clusters can inherit permissions from their root object.\n\n:param cluster_id: str\n  The cluster for which to get or manage permissions.\n:param access_control_list: List[:class:`ClusterAccessControlRequest`] (optional)\n\n:returns: :class:`ClusterPermissions`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The cluster for which to get or manage permissions."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "access_control_list": {
                    "type": "string",
                    "description": ":returns: :class:`ClusterPermissions`"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/list": {
      "get": {
        "operationId": "clusters_list",
        "summary": "Return information about all pinned and active clusters, and all clusters terminated within the last",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Return information about all pinned and active clusters, and all clusters terminated within the last\n30 days. Clusters terminated prior to this period are not included.\n\n:param filter_by: :class:`ListClustersFilterBy` (optional)\n  Filters to apply to the list of clusters.\n:param page_size: int (optional)\n  Use this field to specify the maximum number of results to be returned by the server. The server may\n  further constrain the maximum number of results returned in a single page.\n:param page_token: str (optional)\n  Use next_page_token or prev_page_token returned from the previous request to list the next or\n  previous page of clusters respectively.\n:param sort_by: :class:`ListClustersSortBy` (optional)\n  Sort the list of clusters by a specific criteria.\n\n:returns: Iterator over :class:`ClusterDetails`",
        "parameters": [
          {
            "name": "filter_by",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "Filters to apply to the list of clusters."
          },
          {
            "name": "page_size",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "Use this field to specify the maximum number of results to be returned by the server. The server may further constrain the maximum number of results returned in a single page."
          },
          {
            "name": "page_token",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "Use next_page_token or prev_page_token returned from the previous request to list the next or previous page of clusters respectively."
          },
          {
            "name": "sort_by",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "Sort the list of clusters by a specific criteria."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListClustersResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/list-node-types": {
      "get": {
        "operationId": "clusters_list_node_types",
        "summary": "Returns a list of supported Spark node types. These node types can be used to launch a cluster.",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Returns a list of supported Spark node types. These node types can be used to launch a cluster.\n\n\n:returns: :class:`ListNodeTypesResponse`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListNodeTypesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/list-zones": {
      "get": {
        "operationId": "clusters_list_zones",
        "summary": "Returns a list of availability zones where clusters can be created in (For example, us-west-2a). These",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Returns a list of availability zones where clusters can be created in (For example, us-west-2a). These\nzones can be used to launch a cluster.\n\n\n:returns: :class:`ListAvailableZonesResponse`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListAvailableZonesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/permanent-delete": {
      "post": {
        "operationId": "clusters_permanent_delete",
        "summary": "Permanently deletes a Spark cluster. This cluster is terminated and resources are asynchronously",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Permanently deletes a Spark cluster. This cluster is terminated and resources are asynchronously\nremoved.\n\nIn addition, users will no longer see permanently deleted clusters in the cluster list, and API users\ncan no longer perform any action on permanently deleted clusters.\n\n:param cluster_id: str\n  The cluster to be deleted.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The cluster to be deleted."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/pin": {
      "post": {
        "operationId": "clusters_pin",
        "summary": "Pinning a cluster ensures that the cluster will always be returned by the ListClusters API. Pinning a",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Pinning a cluster ensures that the cluster will always be returned by the ListClusters API. Pinning a\ncluster that is already pinned will have no effect. This API can only be called by workspace admins.\n\n:param cluster_id: str",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "str"
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/resize": {
      "post": {
        "operationId": "clusters_resize",
        "summary": "Resizes a cluster to have a desired number of workers. This will fail unless the cluster is in a",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Resizes a cluster to have a desired number of workers. This will fail unless the cluster is in a\n`RUNNING` state.\n\n:param cluster_id: str\n  The cluster to be resized.\n:param autoscale: :class:`AutoScale` (optional)\n  Parameters needed in order to automatically scale clusters up and down based on load. Note:\n  autoscaling works best with DB runtime versions 3.0 or later.\n:param num_workers: int (optional)\n  Number of worker nodes that this cluster should have. A cluster has one Spark Driver and\n  `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\n  Note: When reading the properties of a cluster, this field reflects the desired number of workers\n  rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10\n  workers, this field will immediately be updated to reflect the target size of 10 workers, whereas\n  the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are\n  provisioned.\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The cluster to be resized."
                  },
                  "autoscale": {
                    "type": "string",
                    "description": "Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later."
                  },
                  "num_workers": {
                    "type": "string",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/restart": {
      "post": {
        "operationId": "clusters_restart",
        "summary": "Restarts a Spark cluster with the supplied ID. If the cluster is not currently in a `RUNNING` state,",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Restarts a Spark cluster with the supplied ID. If the cluster is not currently in a `RUNNING` state,\nnothing will happen.\n\n:param cluster_id: str\n  The cluster to be started.\n:param restart_user: str (optional)\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The cluster to be started."
                  },
                  "restart_user": {
                    "type": "string",
                    "description": ":returns: Long-running operation waiter for :class:`ClusterDetails`. See :method:wait_get_cluster_running for more details."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/spark-versions": {
      "get": {
        "operationId": "clusters_spark_versions",
        "summary": "Returns the list of available Spark versions. These versions can be used to launch a cluster.",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Returns the list of available Spark versions. These versions can be used to launch a cluster.\n\n\n:returns: :class:`GetSparkVersionsResponse`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetSparkVersionsResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/start": {
      "post": {
        "operationId": "clusters_start",
        "summary": "Starts a terminated Spark cluster with the supplied ID. This works similar to `createCluster` except:",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Starts a terminated Spark cluster with the supplied ID. This works similar to `createCluster` except:\n- The previous cluster id and attributes are preserved. - The cluster starts with the last specified\ncluster size. - If the previous cluster was an autoscaling cluster, the current cluster starts with\nthe minimum number of nodes. - If the cluster is not currently in a ``TERMINATED`` state, nothing will\nhappen. - Clusters launched to run a job cannot be started.\n\n:param cluster_id: str\n  The cluster to be started.\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The cluster to be started."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/unpin": {
      "post": {
        "operationId": "clusters_unpin",
        "summary": "Unpinning a cluster will allow the cluster to eventually be removed from the ListClusters API.",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Unpinning a cluster will allow the cluster to eventually be removed from the ListClusters API.\nUnpinning a cluster that is not pinned will have no effect. This API can only be called by workspace\nadmins.\n\n:param cluster_id: str",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "str"
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.1/clusters/update": {
      "post": {
        "operationId": "clusters_update",
        "summary": "Updates the configuration of a cluster to match the partial set of attributes and size. Denote which",
        "tags": [
          "compute",
          "clusters"
        ],
        "description": "Updates the configuration of a cluster to match the partial set of attributes and size. Denote which\nfields to update using the `update_mask` field in the request body. A cluster can be updated if it is\nin a `RUNNING` or `TERMINATED` state. If a cluster is updated while in a `RUNNING` state, it will be\nrestarted so that the new attributes can take effect. If a cluster is updated while in a `TERMINATED`\nstate, it will remain `TERMINATED`. The updated attributes will take effect the next time the cluster\nis started using the `clusters/start` API. Attempts to update a cluster in any other state will be\nrejected with an `INVALID_STATE` error code. Clusters created by the Databricks Jobs service cannot be\nupdated.\n\n:param cluster_id: str\n  ID of the cluster.\n:param update_mask: str\n  Used to specify which cluster attributes and size fields to update. See https://google.aip.dev/161\n  for more details.\n\n  The field mask must be a single string, with multiple fields separated by commas (no spaces). The\n  field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g.,\n  `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only\n  the entire collection field can be specified. Field names must exactly match the resource field\n  names.\n\n  A field mask of `*` indicates full replacement. It\u2019s recommended to always explicitly list the\n  fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API\n  changes in the future.\n:param cluster: :class:`UpdateClusterResource` (optional)\n  The cluster to be updated.\n\n:returns:\n  Long-running operation waiter for :class:`ClusterDetails`.\n  See :method:wait_get_cluster_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "ID of the cluster."
                  },
                  "update_mask": {
                    "type": "string",
                    "description": "Used to specify which cluster attributes and size fields to update. See https://google.aip.dev/161 for more details. The field mask must be a single string, with multiple fields separated by commas (no spaces). The field path is relative to the resource object, using a dot (`.`) to navigate sub-fields (e.g., `author.given_name`). Specification of elements in sequence or map fields is not allowed, as only the entire collection field can be specified. Field names must exactly match the resource field names. A field mask of `*` indicates full replacement. It\u2019s recommended to always explicitly list the fields being updated and avoid using `*` wildcards, as it can lead to unintended results if the API changes in the future."
                  },
                  "cluster": {
                    "type": "string",
                    "description": "The cluster to be updated."
                  }
                },
                "required": [
                  "cluster_id",
                  "update_mask"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterDetails"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/1.2/commands/cancel": {
      "post": {
        "operationId": "command_execution_cancel",
        "summary": "Cancels a currently running command within an execution context.",
        "tags": [
          "compute",
          "command_execution"
        ],
        "description": "Cancels a currently running command within an execution context.\n\nThe command ID is obtained from a prior successful call to __execute__.\n\n:param cluster_id: str (optional)\n:param command_id: str (optional)\n:param context_id: str (optional)\n\n:returns:\n  Long-running operation waiter for :class:`CommandStatusResponse`.\n  See :method:wait_command_status_command_execution_cancelled for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": ":param command_id: str (optional)"
                  },
                  "command_id": {
                    "type": "string"
                  },
                  "context_id": {
                    "type": "string",
                    "description": ":returns: Long-running operation waiter for :class:`CommandStatusResponse`. See :method:wait_command_status_command_execution_cancelled for more details."
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CommandStatusResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/1.2/commands/status": {
      "get": {
        "operationId": "command_execution_command_status",
        "summary": "Gets the status of and, if available, the results from a currently executing command.",
        "tags": [
          "compute",
          "command_execution"
        ],
        "description": "Gets the status of and, if available, the results from a currently executing command.\n\nThe command ID is obtained from a prior successful call to __execute__.\n\n:param cluster_id: str\n:param context_id: str\n:param command_id: str\n\n:returns: :class:`CommandStatusResponse`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": ":param context_id: str"
          },
          {
            "name": "context_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            }
          },
          {
            "name": "command_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": ":returns: :class:`CommandStatusResponse`"
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CommandStatusResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/1.2/contexts/status": {
      "get": {
        "operationId": "command_execution_context_status",
        "summary": "Gets the status for an execution context.",
        "tags": [
          "compute",
          "command_execution"
        ],
        "description": "Gets the status for an execution context.\n\n:param cluster_id: str\n:param context_id: str\n\n:returns: :class:`ContextStatusResponse`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": ":param context_id: str"
          },
          {
            "name": "context_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ContextStatusResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/1.2/contexts/create": {
      "post": {
        "operationId": "command_execution_create",
        "summary": "Creates an execution context for running cluster commands.",
        "tags": [
          "compute",
          "command_execution"
        ],
        "description": "Creates an execution context for running cluster commands.\n\nIf successful, this method returns the ID of the new execution context.\n\n:param cluster_id: str (optional)\n  Running cluster id\n:param language: :class:`Language` (optional)\n\n:returns:\n  Long-running operation waiter for :class:`ContextStatusResponse`.\n  See :method:wait_context_status_command_execution_running for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "Running cluster id"
                  },
                  "language": {
                    "type": "string",
                    "description": ":returns: Long-running operation waiter for :class:`ContextStatusResponse`. See :method:wait_context_status_command_execution_running for more details."
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ContextStatusResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/1.2/contexts/destroy": {
      "post": {
        "operationId": "command_execution_destroy",
        "summary": "Deletes an execution context.",
        "tags": [
          "compute",
          "command_execution"
        ],
        "description": "Deletes an execution context.\n\n:param cluster_id: str\n:param context_id: str",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": ":param context_id: str"
                  },
                  "context_id": {
                    "type": "string"
                  }
                },
                "required": [
                  "cluster_id",
                  "context_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/1.2/commands/execute": {
      "post": {
        "operationId": "command_execution_execute",
        "summary": "Runs a cluster command in the given execution context, using the provided language.",
        "tags": [
          "compute",
          "command_execution"
        ],
        "description": "Runs a cluster command in the given execution context, using the provided language.\n\nIf successful, it returns an ID for tracking the status of the command's execution.\n\n:param cluster_id: str (optional)\n  Running cluster id\n:param command: str (optional)\n  Executable code\n:param context_id: str (optional)\n  Running context id\n:param language: :class:`Language` (optional)\n\n:returns:\n  Long-running operation waiter for :class:`CommandStatusResponse`.\n  See :method:wait_command_status_command_execution_finished_or_error for more details.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "Running cluster id"
                  },
                  "command": {
                    "type": "string",
                    "description": "Executable code"
                  },
                  "context_id": {
                    "type": "string",
                    "description": "Running context id"
                  },
                  "language": {
                    "type": "string",
                    "description": ":returns: Long-running operation waiter for :class:`CommandStatusResponse`. See :method:wait_command_status_command_execution_finished_or_error for more details."
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CommandStatusResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/global-init-scripts": {
      "post": {
        "operationId": "global_init_scripts_create",
        "summary": "Creates a new global init script in this workspace.",
        "tags": [
          "compute",
          "global_init_scripts"
        ],
        "description": "Creates a new global init script in this workspace.\n\n:param name: str\n  The name of the script\n:param script: str\n  The Base64-encoded content of the script.\n:param enabled: bool (optional)\n  Specifies whether the script is enabled. The script runs only if enabled.\n:param position: int (optional)\n  The position of a global init script, where 0 represents the first script to run, 1 is the second\n  script to run, in ascending order.\n\n  If you omit the numeric position for a new global init script, it defaults to last position. It will\n  run after all current scripts. Setting any value greater than the position of the last script is\n  equivalent to the last position. Example: Take three existing scripts with positions 0, 1, and 2.\n  Any position of (3) or greater puts the script in the last position. If an explicit position value\n  conflicts with an existing script value, your request succeeds, but the original script at that\n  position and all later scripts have their positions incremented by 1.\n\n:returns: :class:`CreateResponse`",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "The name of the script"
                  },
                  "script": {
                    "type": "string",
                    "description": "The Base64-encoded content of the script."
                  },
                  "enabled": {
                    "type": "string",
                    "description": "Specifies whether the script is enabled. The script runs only if enabled."
                  },
                  "position": {
                    "type": "string",
                    "description": "The position of a global init script, where 0 represents the first script to run, 1 is the second script to run, in ascending order. If you omit the numeric position for a new global init script, it defaults to last position. It will run after all current scripts. Setting any value greater than the position of the last script is equivalent to the last position. Example: Take three existing scripts with positions 0, 1, and 2. Any position of (3) or greater puts the script in the last position. If an explicit position value conflicts with an existing script value, your request succeeds, but the original script at that position and all later scripts have their positions incremented by 1."
                  }
                },
                "required": [
                  "name",
                  "script"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "get": {
        "operationId": "global_init_scripts_list",
        "summary": "Get a list of all global init scripts for this workspace. This returns all properties for each script",
        "tags": [
          "compute",
          "global_init_scripts"
        ],
        "description": "Get a list of all global init scripts for this workspace. This returns all properties for each script\nbut **not** the script contents. To retrieve the contents of a script, use the [get a global init\nscript](:method:globalinitscripts/get) operation.\n\n\n:returns: Iterator over :class:`GlobalInitScriptDetails`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListGlobalInitScriptsResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/global-init-scripts/{script_id}": {
      "delete": {
        "operationId": "global_init_scripts_delete",
        "summary": "Deletes a global init script.",
        "tags": [
          "compute",
          "global_init_scripts"
        ],
        "description": "Deletes a global init script.\n\n:param script_id: str\n  The ID of the global init script.",
        "parameters": [
          {
            "name": "script_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The ID of the global init script."
          }
        ],
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "get": {
        "operationId": "global_init_scripts_get",
        "summary": "Gets all the details of a script, including its Base64-encoded contents.",
        "tags": [
          "compute",
          "global_init_scripts"
        ],
        "description": "Gets all the details of a script, including its Base64-encoded contents.\n\n:param script_id: str\n  The ID of the global init script.\n\n:returns: :class:`GlobalInitScriptDetailsWithContent`",
        "parameters": [
          {
            "name": "script_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The ID of the global init script."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GlobalInitScriptDetailsWithContent"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "patch": {
        "operationId": "global_init_scripts_update",
        "summary": "Updates a global init script, specifying only the fields to change. All fields are optional.",
        "tags": [
          "compute",
          "global_init_scripts"
        ],
        "description": "Updates a global init script, specifying only the fields to change. All fields are optional.\nUnspecified fields retain their current value.\n\n:param script_id: str\n  The ID of the global init script.\n:param name: str\n  The name of the script\n:param script: str\n  The Base64-encoded content of the script.\n:param enabled: bool (optional)\n  Specifies whether the script is enabled. The script runs only if enabled.\n:param position: int (optional)\n  The position of a script, where 0 represents the first script to run, 1 is the second script to run,\n  in ascending order. To move the script to run first, set its position to 0.\n\n  To move the script to the end, set its position to any value greater or equal to the position of the\n  last script. Example, three existing scripts with positions 0, 1, and 2. Any position value of 2 or\n  greater puts the script in the last position (2).\n\n  If an explicit position value conflicts with an existing script, your request succeeds, but the\n  original script at that position and all later scripts have their positions incremented by 1.",
        "parameters": [
          {
            "name": "script_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The ID of the global init script."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "The name of the script"
                  },
                  "script": {
                    "type": "string",
                    "description": "The Base64-encoded content of the script."
                  },
                  "enabled": {
                    "type": "string",
                    "description": "Specifies whether the script is enabled. The script runs only if enabled."
                  },
                  "position": {
                    "type": "string",
                    "description": "The position of a script, where 0 represents the first script to run, 1 is the second script to run, in ascending order. To move the script to run first, set its position to 0. To move the script to the end, set its position to any value greater or equal to the position of the last script. Example, three existing scripts with positions 0, 1, and 2. Any position value of 2 or greater puts the script in the last position (2). If an explicit position value conflicts with an existing script, your request succeeds, but the original script at that position and all later scripts have their positions incremented by 1."
                  }
                },
                "required": [
                  "name",
                  "script"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-pools/create": {
      "post": {
        "operationId": "instance_pools_create",
        "summary": "Creates a new instance pool using idle and ready-to-use cloud instances.",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Creates a new instance pool using idle and ready-to-use cloud instances.\n\n:param instance_pool_name: str\n  Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100\n  characters.\n:param node_type_id: str\n  This field encodes, through a single value, the resources available to each of the Spark nodes in\n  this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute\n  intensive workloads. A list of available node types can be retrieved by using the\n  :method:clusters/listNodeTypes API call.\n:param aws_attributes: :class:`InstancePoolAwsAttributes` (optional)\n  Attributes related to instance pools running on Amazon Web Services. If not specified at pool\n  creation, a set of default values will be used.\n:param azure_attributes: :class:`InstancePoolAzureAttributes` (optional)\n  Attributes related to instance pools running on Azure. If not specified at pool creation, a set of\n  default values will be used.\n:param custom_tags: Dict[str,str] (optional)\n  Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and\n  EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n  - Currently, Databricks allows at most 45 custom tags\n:param disk_spec: :class:`DiskSpec` (optional)\n  Defines the specification of the disks that will be attached to all spark containers.\n:param enable_elastic_disk: bool (optional)\n  Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire\n  additional disk space when its Spark workers are running low on disk space. In AWS, this feature\n  requires specific AWS permissions to function correctly - refer to the User Guide for more details.\n:param gcp_attributes: :class:`InstancePoolGcpAttributes` (optional)\n  Attributes related to instance pools running on Google Cloud Platform. If not specified at pool\n  creation, a set of default values will be used.\n:param idle_instance_autotermination_minutes: int (optional)\n  Automatically terminates the extra instances in the pool cache after they are inactive for this time\n  in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances\n  will be automatically terminated after a default timeout. If specified, the threshold must be\n  between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances\n  from the cache if min cache size could still hold.\n:param max_capacity: int (optional)\n  Maximum number of outstanding instances to keep in the pool, including both instances used by\n  clusters and idle instances. Clusters that require further instance provisioning will fail during\n  upsize requests.\n:param min_idle_instances: int (optional)\n  Minimum number of idle instances to keep in the instance pool\n:param node_type_flexibility: :class:`NodeTypeFlexibility` (optional)\n  Flexible node type configuration for the pool.\n:param preloaded_docker_images: List[:class:`DockerImage`] (optional)\n  Custom Docker Image BYOC\n:param preloaded_spark_versions: List[str] (optional)\n  A list containing at most one preloaded Spark image version for the pool. Pool-backed clusters\n  started with the preloaded Spark version will start faster. A list of available Spark versions can\n  be retrieved by using the :method:clusters/sparkVersions API call.\n:param remote_disk_throughput: int (optional)\n  If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported\n  for GCP HYPERDISK_BALANCED types.\n:param total_initial_remote_disk_size: int (optional)\n  If set, what the total initial volume size (in GB) of the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED types.\n\n:returns: :class:`CreateInstancePoolResponse`",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "instance_pool_name": {
                    "type": "string",
                    "description": "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100 characters."
                  },
                  "node_type_id": {
                    "type": "string",
                    "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
                  },
                  "aws_attributes": {
                    "type": "string",
                    "description": "Attributes related to instance pools running on Amazon Web Services. If not specified at pool creation, a set of default values will be used."
                  },
                  "azure_attributes": {
                    "type": "string",
                    "description": "Attributes related to instance pools running on Azure. If not specified at pool creation, a set of default values will be used."
                  },
                  "custom_tags": {
                    "type": "string",
                    "description": "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags"
                  },
                  "disk_spec": {
                    "type": "string",
                    "description": "Defines the specification of the disks that will be attached to all spark containers."
                  },
                  "enable_elastic_disk": {
                    "type": "string",
                    "description": "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire additional disk space when its Spark workers are running low on disk space. In AWS, this feature requires specific AWS permissions to function correctly - refer to the User Guide for more details."
                  },
                  "gcp_attributes": {
                    "type": "string",
                    "description": "Attributes related to instance pools running on Google Cloud Platform. If not specified at pool creation, a set of default values will be used."
                  },
                  "idle_instance_autotermination_minutes": {
                    "type": "string",
                    "description": "Automatically terminates the extra instances in the pool cache after they are inactive for this time in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances will be automatically terminated after a default timeout. If specified, the threshold must be between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances from the cache if min cache size could still hold."
                  },
                  "max_capacity": {
                    "type": "string",
                    "description": "Maximum number of outstanding instances to keep in the pool, including both instances used by clusters and idle instances. Clusters that require further instance provisioning will fail during upsize requests."
                  },
                  "min_idle_instances": {
                    "type": "string",
                    "description": "Minimum number of idle instances to keep in the instance pool"
                  },
                  "node_type_flexibility": {
                    "type": "string",
                    "description": "Flexible node type configuration for the pool."
                  },
                  "preloaded_docker_images": {
                    "type": "string",
                    "description": "Custom Docker Image BYOC"
                  },
                  "preloaded_spark_versions": {
                    "type": "string",
                    "description": "A list containing at most one preloaded Spark image version for the pool. Pool-backed clusters started with the preloaded Spark version will start faster. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
                  },
                  "remote_disk_throughput": {
                    "type": "string",
                    "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED types."
                  },
                  "total_initial_remote_disk_size": {
                    "type": "string",
                    "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED types."
                  }
                },
                "required": [
                  "instance_pool_name",
                  "node_type_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateInstancePoolResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-pools/delete": {
      "post": {
        "operationId": "instance_pools_delete",
        "summary": "Deletes the instance pool permanently. The idle instances in the pool are terminated asynchronously.",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Deletes the instance pool permanently. The idle instances in the pool are terminated asynchronously.\n\n:param instance_pool_id: str\n  The instance pool to be terminated.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "instance_pool_id": {
                    "type": "string",
                    "description": "The instance pool to be terminated."
                  }
                },
                "required": [
                  "instance_pool_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-pools/edit": {
      "post": {
        "operationId": "instance_pools_edit",
        "summary": "Modifies the configuration of an existing instance pool.",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Modifies the configuration of an existing instance pool.\n\n:param instance_pool_id: str\n  Instance pool ID\n:param instance_pool_name: str\n  Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100\n  characters.\n:param node_type_id: str\n  This field encodes, through a single value, the resources available to each of the Spark nodes in\n  this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute\n  intensive workloads. A list of available node types can be retrieved by using the\n  :method:clusters/listNodeTypes API call.\n:param custom_tags: Dict[str,str] (optional)\n  Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and\n  EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n  - Currently, Databricks allows at most 45 custom tags\n:param idle_instance_autotermination_minutes: int (optional)\n  Automatically terminates the extra instances in the pool cache after they are inactive for this time\n  in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances\n  will be automatically terminated after a default timeout. If specified, the threshold must be\n  between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances\n  from the cache if min cache size could still hold.\n:param max_capacity: int (optional)\n  Maximum number of outstanding instances to keep in the pool, including both instances used by\n  clusters and idle instances. Clusters that require further instance provisioning will fail during\n  upsize requests.\n:param min_idle_instances: int (optional)\n  Minimum number of idle instances to keep in the instance pool\n:param node_type_flexibility: :class:`NodeTypeFlexibility` (optional)\n  Flexible node type configuration for the pool.\n:param remote_disk_throughput: int (optional)\n  If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported\n  for GCP HYPERDISK_BALANCED types.\n:param total_initial_remote_disk_size: int (optional)\n  If set, what the total initial volume size (in GB) of the remote disks should be. Currently only\n  supported for GCP HYPERDISK_BALANCED types.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "instance_pool_id": {
                    "type": "string",
                    "description": "Instance pool ID"
                  },
                  "instance_pool_name": {
                    "type": "string",
                    "description": "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100 characters."
                  },
                  "node_type_id": {
                    "type": "string",
                    "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
                  },
                  "custom_tags": {
                    "type": "string",
                    "description": "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags"
                  },
                  "idle_instance_autotermination_minutes": {
                    "type": "string",
                    "description": "Automatically terminates the extra instances in the pool cache after they are inactive for this time in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances will be automatically terminated after a default timeout. If specified, the threshold must be between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances from the cache if min cache size could still hold."
                  },
                  "max_capacity": {
                    "type": "string",
                    "description": "Maximum number of outstanding instances to keep in the pool, including both instances used by clusters and idle instances. Clusters that require further instance provisioning will fail during upsize requests."
                  },
                  "min_idle_instances": {
                    "type": "string",
                    "description": "Minimum number of idle instances to keep in the instance pool"
                  },
                  "node_type_flexibility": {
                    "type": "string",
                    "description": "Flexible node type configuration for the pool."
                  },
                  "remote_disk_throughput": {
                    "type": "string",
                    "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED types."
                  },
                  "total_initial_remote_disk_size": {
                    "type": "string",
                    "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED types."
                  }
                },
                "required": [
                  "instance_pool_id",
                  "instance_pool_name",
                  "node_type_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-pools/get": {
      "get": {
        "operationId": "instance_pools_get",
        "summary": "Retrieve the information for an instance pool based on its identifier.",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Retrieve the information for an instance pool based on its identifier.\n\n:param instance_pool_id: str\n  The canonical unique identifier for the instance pool.\n\n:returns: :class:`GetInstancePool`",
        "parameters": [
          {
            "name": "instance_pool_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The canonical unique identifier for the instance pool."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetInstancePool"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/permissions/instance-pools/{instance_pool_id}/permissionLevels": {
      "get": {
        "operationId": "instance_pools_get_permission_levels",
        "summary": "Gets the permission levels that a user can have on an object.",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Gets the permission levels that a user can have on an object.\n\n:param instance_pool_id: str\n  The instance pool for which to get or manage permissions.\n\n:returns: :class:`GetInstancePoolPermissionLevelsResponse`",
        "parameters": [
          {
            "name": "instance_pool_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The instance pool for which to get or manage permissions."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetInstancePoolPermissionLevelsResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/permissions/instance-pools/{instance_pool_id}": {
      "get": {
        "operationId": "instance_pools_get_permissions",
        "summary": "Gets the permissions of an instance pool. Instance pools can inherit permissions from their root",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Gets the permissions of an instance pool. Instance pools can inherit permissions from their root\nobject.\n\n:param instance_pool_id: str\n  The instance pool for which to get or manage permissions.\n\n:returns: :class:`InstancePoolPermissions`",
        "parameters": [
          {
            "name": "instance_pool_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The instance pool for which to get or manage permissions."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/InstancePoolPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "put": {
        "operationId": "instance_pools_set_permissions",
        "summary": "Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Sets permissions on an object, replacing existing permissions if they exist. Deletes all direct\npermissions if none are specified. Objects can inherit permissions from their root object.\n\n:param instance_pool_id: str\n  The instance pool for which to get or manage permissions.\n:param access_control_list: List[:class:`InstancePoolAccessControlRequest`] (optional)\n\n:returns: :class:`InstancePoolPermissions`",
        "parameters": [
          {
            "name": "instance_pool_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The instance pool for which to get or manage permissions."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "access_control_list": {
                    "type": "string",
                    "description": ":returns: :class:`InstancePoolPermissions`"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/InstancePoolPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      },
      "patch": {
        "operationId": "instance_pools_update_permissions",
        "summary": "Updates the permissions on an instance pool. Instance pools can inherit permissions from their root",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Updates the permissions on an instance pool. Instance pools can inherit permissions from their root\nobject.\n\n:param instance_pool_id: str\n  The instance pool for which to get or manage permissions.\n:param access_control_list: List[:class:`InstancePoolAccessControlRequest`] (optional)\n\n:returns: :class:`InstancePoolPermissions`",
        "parameters": [
          {
            "name": "instance_pool_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The instance pool for which to get or manage permissions."
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "access_control_list": {
                    "type": "string",
                    "description": ":returns: :class:`InstancePoolPermissions`"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/InstancePoolPermissions"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-pools/list": {
      "get": {
        "operationId": "instance_pools_list",
        "summary": "Gets a list of instance pools with their statistics.",
        "tags": [
          "compute",
          "instance_pools"
        ],
        "description": "Gets a list of instance pools with their statistics.\n\n\n:returns: Iterator over :class:`InstancePoolAndStats`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListInstancePools"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-profiles/add": {
      "post": {
        "operationId": "instance_profiles_add",
        "summary": "Registers an instance profile in Databricks. In the UI, you can then give users the permission to use",
        "tags": [
          "compute",
          "instance_profiles"
        ],
        "description": "Registers an instance profile in Databricks. In the UI, you can then give users the permission to use\nthis instance profile when launching clusters.\n\nThis API is only available to admin users.\n\n:param instance_profile_arn: str\n  The AWS ARN of the instance profile to register with Databricks. This field is required.\n:param iam_role_arn: str (optional)\n  The AWS IAM role ARN of the role associated with the instance profile. This field is required if\n  your role name and instance profile name do not match and you want to use the instance profile with\n  [Databricks SQL Serverless].\n\n  Otherwise, this field is optional.\n\n  [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html\n:param is_meta_instance_profile: bool (optional)\n  Boolean flag indicating whether the instance profile should only be used in credential passthrough\n  scenarios. If true, it means the instance profile contains an meta IAM role which could assume a\n  wide range of roles. Therefore it should always be used with authorization. This field is optional,\n  the default value is `false`.\n:param skip_validation: bool (optional)\n  By default, Databricks validates that it has sufficient permissions to launch instances with the\n  instance profile. This validation uses AWS dry-run mode for the RunInstances API. If validation\n  fails with an error message that does not indicate an IAM related permission issue, (e.g. \u201cYour\n  requested instance type is not supported in your requested availability zone\u201d), you can pass this\n  flag to skip the validation and forcibly add the instance profile.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "instance_profile_arn": {
                    "type": "string",
                    "description": "The AWS ARN of the instance profile to register with Databricks. This field is required."
                  },
                  "iam_role_arn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. This field is required if your role name and instance profile name do not match and you want to use the instance profile with [Databricks SQL Serverless]. Otherwise, this field is optional. [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html"
                  },
                  "is_meta_instance_profile": {
                    "type": "string",
                    "description": "Boolean flag indicating whether the instance profile should only be used in credential passthrough scenarios. If true, it means the instance profile contains an meta IAM role which could assume a wide range of roles. Therefore it should always be used with authorization. This field is optional, the default value is `false`."
                  },
                  "skip_validation": {
                    "type": "string",
                    "description": "By default, Databricks validates that it has sufficient permissions to launch instances with the instance profile. This validation uses AWS dry-run mode for the RunInstances API. If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \u201cYour requested instance type is not supported in your requested availability zone\u201d), you can pass this flag to skip the validation and forcibly add the instance profile."
                  }
                },
                "required": [
                  "instance_profile_arn"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-profiles/edit": {
      "post": {
        "operationId": "instance_profiles_edit",
        "summary": "The only supported field to change is the optional IAM role ARN associated with the instance profile.",
        "tags": [
          "compute",
          "instance_profiles"
        ],
        "description": "The only supported field to change is the optional IAM role ARN associated with the instance profile.\nIt is required to specify the IAM role ARN if both of the following are true:\n\n* Your role name and instance profile name do not match. The name is the part after the last slash in\neach ARN. * You want to use the instance profile with [Databricks SQL Serverless].\n\nTo understand where these fields are in the AWS console, see [Enable serverless SQL warehouses].\n\nThis API is only available to admin users.\n\n[Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html\n[Enable serverless SQL warehouses]: https://docs.databricks.com/sql/admin/serverless.html\n\n:param instance_profile_arn: str\n  The AWS ARN of the instance profile to register with Databricks. This field is required.\n:param iam_role_arn: str (optional)\n  The AWS IAM role ARN of the role associated with the instance profile. This field is required if\n  your role name and instance profile name do not match and you want to use the instance profile with\n  [Databricks SQL Serverless].\n\n  Otherwise, this field is optional.\n\n  [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html\n:param is_meta_instance_profile: bool (optional)\n  Boolean flag indicating whether the instance profile should only be used in credential passthrough\n  scenarios. If true, it means the instance profile contains an meta IAM role which could assume a\n  wide range of roles. Therefore it should always be used with authorization. This field is optional,\n  the default value is `false`.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "instance_profile_arn": {
                    "type": "string",
                    "description": "The AWS ARN of the instance profile to register with Databricks. This field is required."
                  },
                  "iam_role_arn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. This field is required if your role name and instance profile name do not match and you want to use the instance profile with [Databricks SQL Serverless]. Otherwise, this field is optional. [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html"
                  },
                  "is_meta_instance_profile": {
                    "type": "string",
                    "description": "Boolean flag indicating whether the instance profile should only be used in credential passthrough scenarios. If true, it means the instance profile contains an meta IAM role which could assume a wide range of roles. Therefore it should always be used with authorization. This field is optional, the default value is `false`."
                  }
                },
                "required": [
                  "instance_profile_arn"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-profiles/list": {
      "get": {
        "operationId": "instance_profiles_list",
        "summary": "List the instance profiles that the calling user can use to launch a cluster.",
        "tags": [
          "compute",
          "instance_profiles"
        ],
        "description": "List the instance profiles that the calling user can use to launch a cluster.\n\nThis API is available to all users.\n\n\n:returns: Iterator over :class:`InstanceProfile`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListInstanceProfilesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/instance-profiles/remove": {
      "post": {
        "operationId": "instance_profiles_remove",
        "summary": "Remove the instance profile with the provided ARN. Existing clusters with this instance profile will",
        "tags": [
          "compute",
          "instance_profiles"
        ],
        "description": "Remove the instance profile with the provided ARN. Existing clusters with this instance profile will\ncontinue to function.\n\nThis API is only accessible to admin users.\n\n:param instance_profile_arn: str\n  The ARN of the instance profile to remove. This field is required.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "instance_profile_arn": {
                    "type": "string",
                    "description": "The ARN of the instance profile to remove. This field is required."
                  }
                },
                "required": [
                  "instance_profile_arn"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/libraries/all-cluster-statuses": {
      "get": {
        "operationId": "libraries_all_cluster_statuses",
        "summary": "Get the status of all libraries on all clusters. A status is returned for all libraries installed on",
        "tags": [
          "compute",
          "libraries"
        ],
        "description": "Get the status of all libraries on all clusters. A status is returned for all libraries installed on\nthis cluster via the API or the libraries UI.\n\n\n:returns: Iterator over :class:`ClusterLibraryStatuses`",
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListAllClusterLibraryStatusesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/libraries/cluster-status": {
      "get": {
        "operationId": "libraries_cluster_status",
        "summary": "Get the status of libraries on a cluster. A status is returned for all libraries installed on this",
        "tags": [
          "compute",
          "libraries"
        ],
        "description": "Get the status of libraries on a cluster. A status is returned for all libraries installed on this\ncluster via the API or the libraries UI. The order of returned libraries is as follows: 1. Libraries\nset to be installed on this cluster, in the order that the libraries were added to the cluster, are\nreturned first. 2. Libraries that were previously requested to be installed on this cluster or, but\nare now marked for removal, in no particular order, are returned last.\n\n:param cluster_id: str\n  Unique identifier of the cluster whose status should be retrieved.\n\n:returns: Iterator over :class:`LibraryFullStatus`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "Unique identifier of the cluster whose status should be retrieved."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ClusterLibraryStatuses"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/libraries/install": {
      "post": {
        "operationId": "libraries_install",
        "summary": "Add libraries to install on a cluster. The installation is asynchronous; it happens in the background",
        "tags": [
          "compute",
          "libraries"
        ],
        "description": "Add libraries to install on a cluster. The installation is asynchronous; it happens in the background\nafter the completion of this request.\n\n:param cluster_id: str\n  Unique identifier for the cluster on which to install these libraries.\n:param libraries: List[:class:`Library`]\n  The libraries to install.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "Unique identifier for the cluster on which to install these libraries."
                  },
                  "libraries": {
                    "type": "string",
                    "description": "The libraries to install."
                  }
                },
                "required": [
                  "cluster_id",
                  "libraries"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/libraries/uninstall": {
      "post": {
        "operationId": "libraries_uninstall",
        "summary": "Set libraries to uninstall from a cluster. The libraries won't be uninstalled until the cluster is",
        "tags": [
          "compute",
          "libraries"
        ],
        "description": "Set libraries to uninstall from a cluster. The libraries won't be uninstalled until the cluster is\nrestarted. A request to uninstall a library that is not currently installed is ignored.\n\n:param cluster_id: str\n  Unique identifier for the cluster on which to uninstall these libraries.\n:param libraries: List[:class:`Library`]\n  The libraries to uninstall.",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "Unique identifier for the cluster on which to uninstall these libraries."
                  },
                  "libraries": {
                    "type": "string",
                    "description": "The libraries to uninstall."
                  }
                },
                "required": [
                  "cluster_id",
                  "libraries"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success"
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/enforce-compliance": {
      "post": {
        "operationId": "policy_compliance_for_clusters_enforce_compliance",
        "summary": "Updates a cluster to be compliant with the current version of its policy. A cluster can be updated if",
        "tags": [
          "compute",
          "policy_compliance_for_clusters"
        ],
        "description": "Updates a cluster to be compliant with the current version of its policy. A cluster can be updated if\nit is in a `RUNNING` or `TERMINATED` state.\n\nIf a cluster is updated while in a `RUNNING` state, it will be restarted so that the new attributes\ncan take effect.\n\nIf a cluster is updated while in a `TERMINATED` state, it will remain `TERMINATED`. The next time the\ncluster is started, the new attributes will take effect.\n\nClusters created by the Databricks Jobs, DLT, or Models services cannot be enforced by this API.\nInstead, use the \"Enforce job policy compliance\" API to enforce policy compliance on jobs.\n\n:param cluster_id: str\n  The ID of the cluster you want to enforce policy compliance on.\n:param validate_only: bool (optional)\n  If set, previews the changes that would be made to a cluster to enforce compliance but does not\n  update the cluster.\n\n:returns: :class:`EnforceClusterComplianceResponse`",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "cluster_id": {
                    "type": "string",
                    "description": "The ID of the cluster you want to enforce policy compliance on."
                  },
                  "validate_only": {
                    "type": "string",
                    "description": "If set, previews the changes that would be made to a cluster to enforce compliance but does not update the cluster."
                  }
                },
                "required": [
                  "cluster_id"
                ]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/EnforceClusterComplianceResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/get-compliance": {
      "get": {
        "operationId": "policy_compliance_for_clusters_get_compliance",
        "summary": "Returns the policy compliance status of a cluster. Clusters could be out of compliance if their policy",
        "tags": [
          "compute",
          "policy_compliance_for_clusters"
        ],
        "description": "Returns the policy compliance status of a cluster. Clusters could be out of compliance if their policy\nwas updated after the cluster was last edited.\n\n:param cluster_id: str\n  The ID of the cluster to get the compliance status\n\n:returns: :class:`GetClusterComplianceResponse`",
        "parameters": [
          {
            "name": "cluster_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The ID of the cluster to get the compliance status"
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetClusterComplianceResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policies/clusters/list-compliance": {
      "get": {
        "operationId": "policy_compliance_for_clusters_list_compliance",
        "summary": "Returns the policy compliance status of all clusters that use a given policy. Clusters could be out of",
        "tags": [
          "compute",
          "policy_compliance_for_clusters"
        ],
        "description": "Returns the policy compliance status of all clusters that use a given policy. Clusters could be out of\ncompliance if their policy was updated after the cluster was last edited.\n\n:param policy_id: str\n  Canonical unique identifier for the cluster policy.\n:param page_size: int (optional)\n  Use this field to specify the maximum number of results to be returned by the server. The server may\n  further constrain the maximum number of results returned in a single page.\n:param page_token: str (optional)\n  A page token that can be used to navigate to the next page or previous page as returned by\n  `next_page_token` or `prev_page_token`.\n\n:returns: Iterator over :class:`ClusterCompliance`",
        "parameters": [
          {
            "name": "policy_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "Canonical unique identifier for the cluster policy."
          },
          {
            "name": "page_size",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "Use this field to specify the maximum number of results to be returned by the server. The server may further constrain the maximum number of results returned in a single page."
          },
          {
            "name": "page_token",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "A page token that can be used to navigate to the next page or previous page as returned by `next_page_token` or `prev_page_token`."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListClusterCompliancesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policy-families/{policy_family_id}": {
      "get": {
        "operationId": "policy_families_get",
        "summary": "Retrieve the information for an policy family based on its identifier and version",
        "tags": [
          "compute",
          "policy_families"
        ],
        "description": "Retrieve the information for an policy family based on its identifier and version\n\n:param policy_family_id: str\n  The family ID about which to retrieve information.\n:param version: int (optional)\n  The version number for the family to fetch. Defaults to the latest version.\n\n:returns: :class:`PolicyFamily`",
        "parameters": [
          {
            "name": "policy_family_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string"
            },
            "description": "The family ID about which to retrieve information."
          },
          {
            "name": "version",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "The version number for the family to fetch. Defaults to the latest version."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/PolicyFamily"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/api/2.0/policy-families": {
      "get": {
        "operationId": "policy_families_list",
        "summary": "Returns the list of policy definition types available to use at their latest version. This API is",
        "tags": [
          "compute",
          "policy_families"
        ],
        "description": "Returns the list of policy definition types available to use at their latest version. This API is\npaginated.\n\n:param max_results: int (optional)\n  Maximum number of policy families to return.\n:param page_token: str (optional)\n  A token that can be used to get the next page of results.\n\n:returns: Iterator over :class:`PolicyFamily`",
        "parameters": [
          {
            "name": "max_results",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "Maximum number of policy families to return."
          },
          {
            "name": "page_token",
            "in": "query",
            "required": false,
            "schema": {
              "type": "string"
            },
            "description": "A token that can be used to get the next page of results."
          }
        ],
        "responses": {
          "200": {
            "description": "Success",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListPolicyFamiliesResponse"
                }
              }
            }
          },
          "default": {
            "description": "Error response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string"
                    },
                    "message": {
                      "type": "string"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "AddResponse": {
        "type": "object",
        "properties": {}
      },
      "Adlsgen2Info": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string",
            "description": "abfss destination, e.g. `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`."
          }
        },
        "required": [
          "destination"
        ],
        "description": "A storage location in Adls Gen2"
      },
      "AutoScale": {
        "type": "object",
        "properties": {
          "max_workers": {
            "type": "integer"
          },
          "min_workers": {
            "type": "integer",
            "description": "The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation."
          }
        }
      },
      "AwsAttributes": {
        "type": "object",
        "properties": {
          "availability": {
            "$ref": "#/components/schemas/AwsAvailability"
          },
          "ebs_volume_count": {
            "type": "integer",
            "description": "The number of volumes launched for each instance. Users can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at `/ebs0`, `/ebs1`, and etc. Instance store volumes will be mounted at `/local_disk0`, `/local_disk1`, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogenously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. Please note that if EBS volumes are specified, then the Spark configuration `spark.local.dir` will be overridden."
          },
          "ebs_volume_iops": {
            "type": "integer",
            "description": "If using gp3 volumes, what IOPS to use for the disk. If this is not set, the maximum performance of a gp2 volume with the same volume size will be used."
          },
          "ebs_volume_size": {
            "type": "integer",
            "description": "The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096."
          },
          "ebs_volume_throughput": {
            "type": "integer",
            "description": "If using gp3 volumes, what throughput to use for the disk. If this is not set, the maximum performance of a gp2 volume with the same volume size will be used."
          },
          "ebs_volume_type": {
            "$ref": "#/components/schemas/EbsVolumeType",
            "description": "The type of EBS volumes that will be launched with this cluster."
          },
          "first_on_demand": {
            "type": "integer",
            "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node in particular will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances and the remainder will be placed on `availability` instances. Note that this value does not affect cluster size and cannot currently be mutated over the lifetime of a cluster."
          },
          "instance_profile_arn": {
            "type": "string",
            "description": "Nodes for this cluster will only be placed on AWS instances with this instance profile. If ommitted, nodes will be placed on instances without an IAM instance profile. The instance profile must have previously been added to the Databricks environment by an account administrator. This feature may only be available to certain customer plans."
          },
          "spot_bid_price_percent": {
            "type": "integer",
            "description": "The bid price for AWS spot instances, as a percentage of the corresponding instance type's on-demand price. For example, if this field is set to 50, and the cluster needs a new `r3.xlarge` spot instance, then the bid price is half of the price of on-demand `r3.xlarge` instances. Similarly, if this field is set to 200, the bid price is twice the price of on-demand `r3.xlarge` instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose bid price percentage matches this field will be considered. Note that, for safety, we enforce this field to be no more than 10000."
          },
          "zone_id": {
            "type": "string",
            "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like \"us-west-2a\". The provided availability zone must be in the same region as the Databricks deployment. For example, \"us-west-2a\" is not a valid zone id if the Databricks deployment resides in the \"us-east-1\" region. This is an optional field at cluster creation, and if not specified, the zone \"auto\" will be used. If the zone specified is \"auto\", will try to place cluster in a zone with high availability, and will retry placement in a different AZ if there is not enough capacity. The list of available zones as well as the default value can be found by using the `List Zones` method."
          }
        },
        "description": "Attributes set during cluster creation which are related to Amazon Web Services."
      },
      "AzureAttributes": {
        "type": "object",
        "properties": {
          "availability": {
            "$ref": "#/components/schemas/AzureAvailability",
            "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones. Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster."
          },
          "first_on_demand": {
            "type": "integer",
            "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. This value should be greater than 0, to make sure the cluster driver node is placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances and the remainder will be placed on `availability` instances. Note that this value does not affect cluster size and cannot currently be mutated over the lifetime of a cluster."
          },
          "log_analytics_info": {
            "$ref": "#/components/schemas/LogAnalyticsInfo",
            "description": "Defines values necessary to configure and run Azure Log Analytics agent"
          },
          "spot_bid_max_price": {
            "type": "number",
            "description": "The max bid price to be used for Azure spot instances. The Max price for the bid cannot be higher than the on-demand price of the instance. If not specified, the default value is -1, which specifies that the instance cannot be evicted on the basis of price, and only on the basis of availability. Further, the value should > 0 or -1."
          }
        },
        "description": "Attributes set during cluster creation which are related to Microsoft Azure."
      },
      "CancelResponse": {
        "type": "object",
        "properties": {}
      },
      "ChangeClusterOwnerResponse": {
        "type": "object",
        "properties": {}
      },
      "ClientsTypes": {
        "type": "object",
        "properties": {
          "jobs": {
            "type": "boolean"
          },
          "notebooks": {
            "type": "boolean",
            "description": "With notebooks set, this cluster can be used for notebooks"
          }
        }
      },
      "CloneCluster": {
        "type": "object",
        "properties": {
          "source_cluster_id": {
            "type": "string"
          }
        },
        "required": [
          "source_cluster_id"
        ]
      },
      "CloudProviderNodeInfo": {
        "type": "object",
        "properties": {
          "status": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/CloudProviderNodeStatus"
            }
          }
        }
      },
      "ClusterAccessControlRequest": {
        "type": "object",
        "properties": {
          "group_name": {
            "type": "string"
          },
          "permission_level": {
            "$ref": "#/components/schemas/ClusterPermissionLevel"
          },
          "service_principal_name": {
            "type": "string",
            "description": "application ID of a service principal"
          },
          "user_name": {
            "type": "string",
            "description": "name of the user"
          }
        }
      },
      "ClusterAccessControlResponse": {
        "type": "object",
        "properties": {
          "all_permissions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterPermission"
            }
          },
          "display_name": {
            "type": "string",
            "description": "Display name of the user or service principal."
          },
          "group_name": {
            "type": "string",
            "description": "name of the group"
          },
          "service_principal_name": {
            "type": "string",
            "description": "Name of the service principal."
          },
          "user_name": {
            "type": "string",
            "description": "name of the user"
          }
        }
      },
      "ClusterAttributes": {
        "type": "object",
        "properties": {
          "spark_version": {
            "type": "string",
            "description": "The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
          },
          "autotermination_minutes": {
            "type": "integer",
            "description": "Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination."
          },
          "aws_attributes": {
            "$ref": "#/components/schemas/AwsAttributes",
            "description": "Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used."
          },
          "azure_attributes": {
            "$ref": "#/components/schemas/AzureAttributes",
            "description": "Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used."
          },
          "cluster_log_conf": {
            "$ref": "#/components/schemas/ClusterLogConf",
            "description": "The configuration for delivering spark logs to a long-term storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`."
          },
          "cluster_name": {
            "type": "string",
            "description": "Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. For job clusters, the cluster name is automatically set based on the job and job run IDs."
          },
          "custom_tags": {
            "type": "object",
            "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags"
          },
          "data_security_mode": {
            "$ref": "#/components/schemas/DataSecurityMode"
          },
          "docker_image": {
            "$ref": "#/components/schemas/DockerImage",
            "description": "Custom docker image BYOC"
          },
          "driver_instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned."
          },
          "driver_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for the driver node."
          },
          "driver_node_type_id": {
            "type": "string",
            "description": "The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and node_type_id take precedence."
          },
          "enable_elastic_disk": {
            "type": "boolean",
            "description": "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space."
          },
          "enable_local_disk_encryption": {
            "type": "boolean",
            "description": "Whether to enable LUKS on cluster VMs' local disks"
          },
          "gcp_attributes": {
            "$ref": "#/components/schemas/GcpAttributes",
            "description": "Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used."
          },
          "init_scripts": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InitScriptInfo"
            },
            "description": "The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`."
          },
          "instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool to which the cluster belongs."
          },
          "is_single_node": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`"
          },
          "kind": {
            "$ref": "#/components/schemas/Kind"
          },
          "node_type_id": {
            "type": "string",
            "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
          },
          "policy_id": {
            "type": "string",
            "description": "The ID of the cluster policy used to create the cluster if applicable."
          },
          "remote_disk_throughput": {
            "type": "integer",
            "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "runtime_engine": {
            "$ref": "#/components/schemas/RuntimeEngine",
            "description": "Determines the cluster's runtime engine, either standard or Photon. This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`. If left unspecified, the runtime engine defaults to standard unless the spark_version contains -photon-, in which case Photon will be used."
          },
          "single_user_name": {
            "type": "string",
            "description": "Single user name if data_security_mode is `SINGLE_USER`"
          },
          "spark_conf": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively."
          },
          "spark_env_vars": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while launching the driver and workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`"
          },
          "ssh_public_keys": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified."
          },
          "total_initial_remote_disk_size": {
            "type": "integer",
            "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "use_ml_runtime": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. `effective_spark_version` is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is gpu node or not."
          },
          "worker_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for worker nodes."
          },
          "workload_type": {
            "$ref": "#/components/schemas/WorkloadType"
          }
        },
        "required": [
          "spark_version"
        ],
        "description": "Common set of attributes set during cluster creation. These attributes cannot be changed over\n    the lifetime of a cluster."
      },
      "ClusterCompliance": {
        "type": "object",
        "properties": {
          "cluster_id": {
            "type": "string"
          },
          "is_compliant": {
            "type": "boolean",
            "description": "Whether this cluster is in compliance with the latest version of its policy."
          },
          "violations": {
            "type": "object",
            "description": "An object containing key-value mappings representing the first 200 policy validation errors. The keys indicate the path where the policy validation error is occurring. The values indicate an error message describing the policy validation error."
          }
        },
        "required": [
          "cluster_id"
        ]
      },
      "ClusterDetails": {
        "type": "object",
        "properties": {
          "autoscale": {
            "$ref": "#/components/schemas/AutoScale",
            "description": "Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later."
          },
          "autotermination_minutes": {
            "type": "integer",
            "description": "Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination."
          },
          "aws_attributes": {
            "$ref": "#/components/schemas/AwsAttributes",
            "description": "Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used."
          },
          "azure_attributes": {
            "$ref": "#/components/schemas/AzureAttributes",
            "description": "Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used."
          },
          "cluster_cores": {
            "type": "number",
            "description": "Number of CPU cores available for this cluster. Note that this can be fractional, e.g. 7.5 cores, since certain node types are configured to share cores between Spark nodes on the same instance."
          },
          "cluster_id": {
            "type": "string",
            "description": "Canonical identifier for the cluster. This id is retained during cluster restarts and resizes, while each new cluster has a globally unique id."
          },
          "cluster_log_conf": {
            "$ref": "#/components/schemas/ClusterLogConf",
            "description": "The configuration for delivering spark logs to a long-term storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`."
          },
          "cluster_log_status": {
            "$ref": "#/components/schemas/LogSyncStatus",
            "description": "Cluster log delivery status."
          },
          "cluster_memory_mb": {
            "type": "integer",
            "description": "Total amount of cluster memory, in megabytes"
          },
          "cluster_name": {
            "type": "string",
            "description": "Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. For job clusters, the cluster name is automatically set based on the job and job run IDs."
          },
          "cluster_source": {
            "$ref": "#/components/schemas/ClusterSource",
            "description": "Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs Scheduler, or through an API request."
          },
          "creator_user_name": {
            "type": "string",
            "description": "Creator user name. The field won't be included in the response if the user has already been deleted."
          },
          "custom_tags": {
            "type": "object",
            "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags"
          },
          "data_security_mode": {
            "$ref": "#/components/schemas/DataSecurityMode"
          },
          "default_tags": {
            "type": "object",
            "description": "Tags that are added by Databricks regardless of any `custom_tags`, including: - Vendor: Databricks - Creator: <username_of_creator> - ClusterName: <name_of_cluster> - ClusterId: <id_of_cluster> - Name: <Databricks internal use>"
          },
          "docker_image": {
            "$ref": "#/components/schemas/DockerImage",
            "description": "Custom docker image BYOC"
          },
          "driver": {
            "$ref": "#/components/schemas/SparkNode",
            "description": "Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs."
          },
          "driver_instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned."
          },
          "driver_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for the driver node."
          },
          "driver_node_type_id": {
            "type": "string",
            "description": "The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and node_type_id take precedence."
          },
          "enable_elastic_disk": {
            "type": "boolean",
            "description": "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space."
          },
          "enable_local_disk_encryption": {
            "type": "boolean",
            "description": "Whether to enable LUKS on cluster VMs' local disks"
          },
          "executors": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/SparkNode"
            },
            "description": "Nodes on which the Spark executors reside."
          },
          "gcp_attributes": {
            "$ref": "#/components/schemas/GcpAttributes",
            "description": "Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used."
          },
          "init_scripts": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InitScriptInfo"
            },
            "description": "The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`."
          },
          "instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool to which the cluster belongs."
          },
          "is_single_node": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`"
          },
          "jdbc_port": {
            "type": "integer",
            "description": "Port on which Spark JDBC server is listening, in the driver nod. No service will be listeningon on this port in executor nodes."
          },
          "kind": {
            "$ref": "#/components/schemas/Kind"
          },
          "last_restarted_time": {
            "type": "integer",
            "description": "the timestamp that the cluster was started/restarted"
          },
          "last_state_loss_time": {
            "type": "integer",
            "description": "Time when the cluster driver last lost its state (due to a restart or driver failure)."
          },
          "node_type_id": {
            "type": "string",
            "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
          },
          "num_workers": {
            "type": "integer",
            "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
          },
          "policy_id": {
            "type": "string",
            "description": "The ID of the cluster policy used to create the cluster if applicable."
          },
          "remote_disk_throughput": {
            "type": "integer",
            "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "runtime_engine": {
            "$ref": "#/components/schemas/RuntimeEngine",
            "description": "Determines the cluster's runtime engine, either standard or Photon. This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`. If left unspecified, the runtime engine defaults to standard unless the spark_version contains -photon-, in which case Photon will be used."
          },
          "single_user_name": {
            "type": "string",
            "description": "Single user name if data_security_mode is `SINGLE_USER`"
          },
          "spark_conf": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively."
          },
          "spark_context_id": {
            "type": "integer",
            "description": "A canonical SparkContext identifier. This value *does* change when the Spark driver restarts. The pair `(cluster_id, spark_context_id)` is a globally unique identifier over all Spark contexts."
          },
          "spark_env_vars": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while launching the driver and workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`"
          },
          "spark_version": {
            "type": "string",
            "description": "The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
          },
          "spec": {
            "$ref": "#/components/schemas/ClusterSpec",
            "description": "The spec contains a snapshot of the latest user specified settings that were used to create/edit the cluster. Note: not included in the response of the ListClusters API."
          },
          "ssh_public_keys": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified."
          },
          "start_time": {
            "type": "integer",
            "description": "Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a `PENDING` state)."
          },
          "state": {
            "$ref": "#/components/schemas/State",
            "description": "Current state of the cluster."
          },
          "state_message": {
            "type": "string",
            "description": "A message associated with the most recent state transition (e.g., the reason why the cluster entered a `TERMINATED` state)."
          },
          "terminated_time": {
            "type": "integer",
            "description": "Time (in epoch milliseconds) when the cluster was terminated, if applicable."
          },
          "termination_reason": {
            "$ref": "#/components/schemas/TerminationReason",
            "description": "Information about why the cluster was terminated. This field only appears when the cluster is in a `TERMINATING` or `TERMINATED` state."
          },
          "total_initial_remote_disk_size": {
            "type": "integer",
            "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "use_ml_runtime": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. `effective_spark_version` is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is gpu node or not."
          },
          "worker_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for worker nodes."
          },
          "workload_type": {
            "$ref": "#/components/schemas/WorkloadType"
          }
        },
        "description": "Describes all of the metadata about a single Spark cluster in Databricks."
      },
      "ClusterEvent": {
        "type": "object",
        "properties": {
          "cluster_id": {
            "type": "string"
          },
          "data_plane_event_details": {
            "$ref": "#/components/schemas/DataPlaneEventDetails"
          },
          "details": {
            "$ref": "#/components/schemas/EventDetails"
          },
          "timestamp": {
            "type": "integer",
            "description": "The timestamp when the event occurred, stored as the number of milliseconds since the Unix epoch. If not provided, this will be assigned by the Timeline service."
          },
          "type": {
            "$ref": "#/components/schemas/EventType"
          }
        },
        "required": [
          "cluster_id"
        ]
      },
      "ClusterLibraryStatuses": {
        "type": "object",
        "properties": {
          "cluster_id": {
            "type": "string"
          },
          "library_statuses": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/LibraryFullStatus"
            },
            "description": "Status of all libraries on the cluster."
          }
        }
      },
      "ClusterLogConf": {
        "type": "object",
        "properties": {
          "dbfs": {
            "$ref": "#/components/schemas/DbfsStorageInfo",
            "description": "destination needs to be provided. e.g. `{ \"dbfs\" : { \"destination\" : \"dbfs:/home/cluster_log\" } }`"
          },
          "s3": {
            "$ref": "#/components/schemas/S3StorageInfo",
            "description": "destination and either the region or endpoint need to be provided. e.g. `{ \"s3\": { \"destination\" : \"s3://cluster_log_bucket/prefix\", \"region\" : \"us-west-2\" } }` Cluster iam role is used to access s3, please make sure the cluster iam role in `instance_profile_arn` has permission to write data to the s3 destination."
          },
          "volumes": {
            "$ref": "#/components/schemas/VolumesStorageInfo",
            "description": "destination needs to be provided, e.g. `{ \"volumes\": { \"destination\": \"/Volumes/catalog/schema/volume/cluster_log\" } }`"
          }
        },
        "description": "Cluster log delivery config"
      },
      "ClusterPermission": {
        "type": "object",
        "properties": {
          "inherited": {
            "type": "boolean"
          },
          "inherited_from_object": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "permission_level": {
            "$ref": "#/components/schemas/ClusterPermissionLevel"
          }
        }
      },
      "ClusterPermissions": {
        "type": "object",
        "properties": {
          "access_control_list": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterAccessControlResponse"
            }
          },
          "object_id": {
            "type": "string"
          },
          "object_type": {
            "type": "string"
          }
        }
      },
      "ClusterPermissionsDescription": {
        "type": "object",
        "properties": {
          "description": {
            "type": "string"
          },
          "permission_level": {
            "$ref": "#/components/schemas/ClusterPermissionLevel"
          }
        }
      },
      "ClusterPolicyAccessControlRequest": {
        "type": "object",
        "properties": {
          "group_name": {
            "type": "string"
          },
          "permission_level": {
            "$ref": "#/components/schemas/ClusterPolicyPermissionLevel"
          },
          "service_principal_name": {
            "type": "string",
            "description": "application ID of a service principal"
          },
          "user_name": {
            "type": "string",
            "description": "name of the user"
          }
        }
      },
      "ClusterPolicyAccessControlResponse": {
        "type": "object",
        "properties": {
          "all_permissions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterPolicyPermission"
            }
          },
          "display_name": {
            "type": "string",
            "description": "Display name of the user or service principal."
          },
          "group_name": {
            "type": "string",
            "description": "name of the group"
          },
          "service_principal_name": {
            "type": "string",
            "description": "Name of the service principal."
          },
          "user_name": {
            "type": "string",
            "description": "name of the user"
          }
        }
      },
      "ClusterPolicyPermission": {
        "type": "object",
        "properties": {
          "inherited": {
            "type": "boolean"
          },
          "inherited_from_object": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "permission_level": {
            "$ref": "#/components/schemas/ClusterPolicyPermissionLevel"
          }
        }
      },
      "ClusterPolicyPermissions": {
        "type": "object",
        "properties": {
          "access_control_list": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterPolicyAccessControlResponse"
            }
          },
          "object_id": {
            "type": "string"
          },
          "object_type": {
            "type": "string"
          }
        }
      },
      "ClusterPolicyPermissionsDescription": {
        "type": "object",
        "properties": {
          "description": {
            "type": "string"
          },
          "permission_level": {
            "$ref": "#/components/schemas/ClusterPolicyPermissionLevel"
          }
        }
      },
      "ClusterSettingsChange": {
        "type": "object",
        "properties": {
          "field": {
            "type": "string",
            "description": "The field where this change would be made."
          },
          "new_value": {
            "type": "string",
            "description": "The new value of this field after enforcing policy compliance (either a number, a boolean, or a string) converted to a string. This is intended to be read by a human. The typed new value of this field can be retrieved by reading the settings field in the API response."
          },
          "previous_value": {
            "type": "string",
            "description": "The previous value of this field before enforcing policy compliance (either a number, a boolean, or a string) converted to a string. This is intended to be read by a human. The type of the field can be retrieved by reading the settings field in the API response."
          }
        },
        "description": "Represents a change to the cluster settings required for the cluster to become compliant with\n    its policy."
      },
      "ClusterSize": {
        "type": "object",
        "properties": {
          "autoscale": {
            "$ref": "#/components/schemas/AutoScale"
          },
          "num_workers": {
            "type": "integer",
            "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
          }
        }
      },
      "ClusterSpec": {
        "type": "object",
        "properties": {
          "apply_policy_default_values": {
            "type": "boolean",
            "description": "When set to true, fixed and default values from the policy will be used for fields that are omitted. When set to false, only fixed values from the policy will be applied."
          },
          "autoscale": {
            "$ref": "#/components/schemas/AutoScale",
            "description": "Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later."
          },
          "autotermination_minutes": {
            "type": "integer",
            "description": "Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination."
          },
          "aws_attributes": {
            "$ref": "#/components/schemas/AwsAttributes",
            "description": "Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used."
          },
          "azure_attributes": {
            "$ref": "#/components/schemas/AzureAttributes",
            "description": "Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used."
          },
          "cluster_log_conf": {
            "$ref": "#/components/schemas/ClusterLogConf",
            "description": "The configuration for delivering spark logs to a long-term storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`."
          },
          "cluster_name": {
            "type": "string",
            "description": "Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. For job clusters, the cluster name is automatically set based on the job and job run IDs."
          },
          "custom_tags": {
            "type": "object",
            "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags"
          },
          "data_security_mode": {
            "$ref": "#/components/schemas/DataSecurityMode"
          },
          "docker_image": {
            "$ref": "#/components/schemas/DockerImage",
            "description": "Custom docker image BYOC"
          },
          "driver_instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned."
          },
          "driver_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for the driver node."
          },
          "driver_node_type_id": {
            "type": "string",
            "description": "The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and node_type_id take precedence."
          },
          "enable_elastic_disk": {
            "type": "boolean",
            "description": "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space."
          },
          "enable_local_disk_encryption": {
            "type": "boolean",
            "description": "Whether to enable LUKS on cluster VMs' local disks"
          },
          "gcp_attributes": {
            "$ref": "#/components/schemas/GcpAttributes",
            "description": "Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used."
          },
          "init_scripts": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InitScriptInfo"
            },
            "description": "The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`."
          },
          "instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool to which the cluster belongs."
          },
          "is_single_node": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`"
          },
          "kind": {
            "$ref": "#/components/schemas/Kind"
          },
          "node_type_id": {
            "type": "string",
            "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
          },
          "num_workers": {
            "type": "integer",
            "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
          },
          "policy_id": {
            "type": "string",
            "description": "The ID of the cluster policy used to create the cluster if applicable."
          },
          "remote_disk_throughput": {
            "type": "integer",
            "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "runtime_engine": {
            "$ref": "#/components/schemas/RuntimeEngine",
            "description": "Determines the cluster's runtime engine, either standard or Photon. This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`. If left unspecified, the runtime engine defaults to standard unless the spark_version contains -photon-, in which case Photon will be used."
          },
          "single_user_name": {
            "type": "string",
            "description": "Single user name if data_security_mode is `SINGLE_USER`"
          },
          "spark_conf": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively."
          },
          "spark_env_vars": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while launching the driver and workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`"
          },
          "spark_version": {
            "type": "string",
            "description": "The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
          },
          "ssh_public_keys": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified."
          },
          "total_initial_remote_disk_size": {
            "type": "integer",
            "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "use_ml_runtime": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. `effective_spark_version` is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is gpu node or not."
          },
          "worker_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for worker nodes."
          },
          "workload_type": {
            "$ref": "#/components/schemas/WorkloadType"
          }
        },
        "description": "Contains a snapshot of the latest user specified settings that were used to create/edit the\n    cluster."
      },
      "CommandStatusResponse": {
        "type": "object",
        "properties": {
          "id": {
            "type": "string"
          },
          "results": {
            "$ref": "#/components/schemas/Results"
          },
          "status": {
            "$ref": "#/components/schemas/CommandStatus"
          }
        }
      },
      "ContextStatusResponse": {
        "type": "object",
        "properties": {
          "id": {
            "type": "string"
          },
          "status": {
            "$ref": "#/components/schemas/ContextStatus"
          }
        }
      },
      "CreateClusterResponse": {
        "type": "object",
        "properties": {
          "cluster_id": {
            "type": "string"
          }
        }
      },
      "CreateInstancePoolResponse": {
        "type": "object",
        "properties": {
          "instance_pool_id": {
            "type": "string"
          }
        }
      },
      "CreatePolicyResponse": {
        "type": "object",
        "properties": {
          "policy_id": {
            "type": "string"
          }
        }
      },
      "CreateResponse": {
        "type": "object",
        "properties": {
          "script_id": {
            "type": "string"
          }
        }
      },
      "Created": {
        "type": "object",
        "properties": {
          "id": {
            "type": "string"
          }
        }
      },
      "CustomPolicyTag": {
        "type": "object",
        "properties": {
          "key": {
            "type": "string"
          },
          "value": {
            "type": "string",
            "description": "The value of the tag."
          }
        },
        "required": [
          "key"
        ]
      },
      "DataPlaneEventDetails": {
        "type": "object",
        "properties": {
          "event_type": {
            "$ref": "#/components/schemas/DataPlaneEventDetailsEventType"
          },
          "executor_failures": {
            "type": "integer"
          },
          "host_id": {
            "type": "string"
          },
          "timestamp": {
            "type": "integer"
          }
        }
      },
      "DbfsStorageInfo": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string",
            "description": "dbfs destination, e.g. `dbfs:/my/path`"
          }
        },
        "required": [
          "destination"
        ],
        "description": "A storage location in DBFS"
      },
      "DeleteClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "DeleteInstancePoolResponse": {
        "type": "object",
        "properties": {}
      },
      "DeletePolicyResponse": {
        "type": "object",
        "properties": {}
      },
      "DeleteResponse": {
        "type": "object",
        "properties": {}
      },
      "DestroyResponse": {
        "type": "object",
        "properties": {}
      },
      "DiskSpec": {
        "type": "object",
        "properties": {
          "disk_count": {
            "type": "integer",
            "description": "The number of disks launched for each instance: - This feature is only enabled for supported node types. - Users can choose up to the limit of the disks supported by the node type. - For node types with no OS disk, at least one disk must be specified; otherwise, cluster creation will fail. If disks are attached, Databricks will configure Spark to use only the disks for scratch storage, because heterogenously sized scratch devices can lead to inefficient disk utilization. If no disks are attached, Databricks will configure Spark to use instance store disks. Note: If disks are specified, then the Spark configuration `spark.local.dir` will be overridden. Disks will be mounted at: - For AWS: `/ebs0`, `/ebs1`, and etc. - For Azure: `/remote_volume0`, `/remote_volume1`, and etc."
          },
          "disk_iops": {
            "type": "integer"
          },
          "disk_size": {
            "type": "integer",
            "description": "The size of each disk (in GiB) launched for each instance. Values must fall into the supported range for a particular instance type. For AWS: - General Purpose SSD: 100 - 4096 GiB - Throughput Optimized HDD: 500 - 4096 GiB For Azure: - Premium LRS (SSD): 1 - 1023 GiB - Standard LRS (HDD): 1- 1023 GiB"
          },
          "disk_throughput": {
            "type": "integer"
          },
          "disk_type": {
            "$ref": "#/components/schemas/DiskType",
            "description": "The type of disks that will be launched with this cluster."
          }
        },
        "description": "Describes the disks that are launched for each instance in the spark cluster. For example, if\n    the cluster has 3 instances, each instance is configured to launch 2 disks, 100 GiB each, then\n    Databricks will launch a total of 6 disks, 100 GiB each, for this cluster."
      },
      "DiskType": {
        "type": "object",
        "properties": {
          "azure_disk_volume_type": {
            "$ref": "#/components/schemas/DiskTypeAzureDiskVolumeType"
          },
          "ebs_volume_type": {
            "$ref": "#/components/schemas/DiskTypeEbsVolumeType"
          }
        },
        "description": "Describes the disk type."
      },
      "DockerBasicAuth": {
        "type": "object",
        "properties": {
          "password": {
            "type": "string"
          },
          "username": {
            "type": "string",
            "description": "Name of the user"
          }
        }
      },
      "DockerImage": {
        "type": "object",
        "properties": {
          "basic_auth": {
            "$ref": "#/components/schemas/DockerBasicAuth"
          },
          "url": {
            "type": "string",
            "description": "URL of the docker image."
          }
        }
      },
      "EditClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "EditInstancePoolResponse": {
        "type": "object",
        "properties": {}
      },
      "EditPolicyResponse": {
        "type": "object",
        "properties": {}
      },
      "EditResponse": {
        "type": "object",
        "properties": {}
      },
      "EnforceClusterComplianceResponse": {
        "type": "object",
        "properties": {
          "changes": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterSettingsChange"
            }
          },
          "has_changes": {
            "type": "boolean",
            "description": "Whether any changes have been made to the cluster settings for the cluster to become compliant with its policy."
          }
        }
      },
      "Environment": {
        "type": "object",
        "properties": {
          "base_environment": {
            "type": "string",
            "description": "The `base_environment` key refers to an `env.yaml` file that specifies an environment version and a collection of dependencies required for the environment setup. This `env.yaml` file may itself include a `base_environment` reference pointing to another `env_1.yaml` file. However, when used as a base environment, `env_1.yaml` (or further nested references) will not be processed or included in the final environment, meaning that the resolution of `base_environment` references is not recursive."
          },
          "client": {
            "type": "string",
            "description": "Use `environment_version` instead."
          },
          "dependencies": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of pip dependencies, as supported by the version of pip in this environment. Each dependency is a valid pip requirements file line per https://pip.pypa.io/en/stable/reference/requirements-file-format/. Allowed dependencies include a requirement specifier, an archive URL, a local project path (such as WSFS or UC Volumes in Databricks), or a VCS project URL."
          },
          "environment_version": {
            "type": "string",
            "description": "Required. Environment version used by the environment. Each version comes with a specific Python version and a set of Python packages. The version is a string, consisting of an integer."
          },
          "java_dependencies": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of java dependencies. Each dependency is a string representing a java library path. For example: `/Volumes/path/to/test.jar`."
          }
        },
        "description": "The environment entity used to preserve serverless environment side panel, jobs' environment for\n    non-notebook task, and DLT's environment for classic and serverless pipelines. In this minimal\n    environment spec, only pip dependencies are supported."
      },
      "EventDetails": {
        "type": "object",
        "properties": {
          "attributes": {
            "$ref": "#/components/schemas/ClusterAttributes"
          },
          "cause": {
            "$ref": "#/components/schemas/EventDetailsCause",
            "description": "The cause of a change in target size."
          },
          "cluster_size": {
            "$ref": "#/components/schemas/ClusterSize",
            "description": "The actual cluster size that was set in the cluster creation or edit."
          },
          "current_num_vcpus": {
            "type": "integer",
            "description": "The current number of vCPUs in the cluster."
          },
          "current_num_workers": {
            "type": "integer",
            "description": "The current number of nodes in the cluster."
          },
          "did_not_expand_reason": {
            "type": "string"
          },
          "disk_size": {
            "type": "integer",
            "description": "Current disk size in bytes"
          },
          "driver_state_message": {
            "type": "string",
            "description": "More details about the change in driver's state"
          },
          "enable_termination_for_node_blocklisted": {
            "type": "boolean",
            "description": "Whether or not a blocklisted node should be terminated. For ClusterEventType NODE_BLACKLISTED."
          },
          "free_space": {
            "type": "integer"
          },
          "init_scripts": {
            "$ref": "#/components/schemas/InitScriptEventDetails",
            "description": "List of global and cluster init scripts associated with this cluster event."
          },
          "instance_id": {
            "type": "string",
            "description": "Instance Id where the event originated from"
          },
          "job_run_name": {
            "type": "string",
            "description": "Unique identifier of the specific job run associated with this cluster event * For clusters created for jobs, this will be the same as the cluster name"
          },
          "previous_attributes": {
            "$ref": "#/components/schemas/ClusterAttributes",
            "description": "The cluster attributes before a cluster was edited."
          },
          "previous_cluster_size": {
            "$ref": "#/components/schemas/ClusterSize",
            "description": "The size of the cluster before an edit or resize."
          },
          "previous_disk_size": {
            "type": "integer",
            "description": "Previous disk size in bytes"
          },
          "reason": {
            "$ref": "#/components/schemas/TerminationReason",
            "description": "A termination reason: * On a TERMINATED event, this is the reason of the termination. * On a RESIZE_COMPLETE event, this indicates the reason that we failed to acquire some nodes."
          },
          "target_num_vcpus": {
            "type": "integer",
            "description": "The targeted number of vCPUs in the cluster."
          },
          "target_num_workers": {
            "type": "integer",
            "description": "The targeted number of nodes in the cluster."
          },
          "user": {
            "type": "string",
            "description": "The user that caused the event to occur. (Empty if it was done by the control plane.)"
          }
        }
      },
      "GcpAttributes": {
        "type": "object",
        "properties": {
          "availability": {
            "$ref": "#/components/schemas/GcpAvailability",
            "description": "This field determines whether the spark executors will be scheduled to run on preemptible VMs, on-demand VMs, or preemptible VMs with a fallback to on-demand VMs if the former is unavailable."
          },
          "boot_disk_size": {
            "type": "integer",
            "description": "Boot disk size in GB"
          },
          "first_on_demand": {
            "type": "integer",
            "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. This value should be greater than 0, to make sure the cluster driver node is placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances and the remainder will be placed on `availability` instances. Note that this value does not affect cluster size and cannot currently be mutated over the lifetime of a cluster."
          },
          "google_service_account": {
            "type": "string",
            "description": "If provided, the cluster will impersonate the google service account when accessing gcloud services (like GCS). The google service account must have previously been added to the Databricks environment by an account administrator."
          },
          "local_ssd_count": {
            "type": "integer",
            "description": "If provided, each node (workers and driver) in the cluster will have this number of local SSDs attached. Each local SSD is 375GB in size. Refer to [GCP documentation] for the supported number of local SSDs for each instance type. [GCP documentation]: https://cloud.google.com/compute/docs/disks/local-ssd#choose_number_local_ssds"
          },
          "use_preemptible_executors": {
            "type": "boolean",
            "description": "This field determines whether the spark executors will be scheduled to run on preemptible VMs (when set to true) versus standard compute engine VMs (when set to false; default). Note: Soon to be deprecated, use the 'availability' field instead."
          },
          "zone_id": {
            "type": "string",
            "description": "Identifier for the availability zone in which the cluster resides. This can be one of the following: - \"HA\" => High availability, spread nodes across availability zones for a Databricks deployment region [default]. - \"AUTO\" => Databricks picks an availability zone to schedule the cluster on. - A GCP availability zone => Pick One of the available zones for (machine type + region) from https://cloud.google.com/compute/docs/regions-zones."
          }
        },
        "description": "Attributes set during cluster creation which are related to GCP."
      },
      "GcsStorageInfo": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string",
            "description": "GCS destination/URI, e.g. `gs://my-bucket/some-prefix`"
          }
        },
        "required": [
          "destination"
        ],
        "description": "A storage location in Google Cloud Platform's GCS"
      },
      "GetClusterComplianceResponse": {
        "type": "object",
        "properties": {
          "is_compliant": {
            "type": "boolean"
          },
          "violations": {
            "type": "object",
            "description": "An object containing key-value mappings representing the first 200 policy validation errors. The keys indicate the path where the policy validation error is occurring. The values indicate an error message describing the policy validation error."
          }
        }
      },
      "GetClusterPermissionLevelsResponse": {
        "type": "object",
        "properties": {
          "permission_levels": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterPermissionsDescription"
            }
          }
        }
      },
      "GetClusterPolicyPermissionLevelsResponse": {
        "type": "object",
        "properties": {
          "permission_levels": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterPolicyPermissionsDescription"
            }
          }
        }
      },
      "GetEvents": {
        "type": "object",
        "properties": {
          "cluster_id": {
            "type": "string"
          },
          "end_time": {
            "type": "integer",
            "description": "The end time in epoch milliseconds. If empty, returns events up to the current time."
          },
          "event_types": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/EventType"
            },
            "description": "An optional set of event types to filter on. If empty, all event types are returned."
          },
          "limit": {
            "type": "integer",
            "description": "Deprecated: use page_token in combination with page_size instead. The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed value is 500."
          },
          "offset": {
            "type": "integer",
            "description": "Deprecated: use page_token in combination with page_size instead. The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results are requested in descending order, the end_time field is required."
          },
          "order": {
            "$ref": "#/components/schemas/GetEventsOrder",
            "description": "The order to list events in; either \"ASC\" or \"DESC\". Defaults to \"DESC\"."
          },
          "page_size": {
            "type": "integer",
            "description": "The maximum number of events to include in a page of events. The server may further constrain the maximum number of results returned in a single page. If the page_size is empty or 0, the server will decide the number of results to be returned. The field has to be in the range [0,500]. If the value is outside the range, the server enforces 0 or 500."
          },
          "page_token": {
            "type": "string",
            "description": "Use next_page_token or prev_page_token returned from the previous request to list the next or previous page of events respectively. If page_token is empty, the first page is returned."
          },
          "start_time": {
            "type": "integer",
            "description": "The start time in epoch milliseconds. If empty, returns events starting from the beginning of time."
          }
        },
        "required": [
          "cluster_id"
        ]
      },
      "GetEventsResponse": {
        "type": "object",
        "properties": {
          "events": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterEvent"
            }
          },
          "next_page": {
            "$ref": "#/components/schemas/GetEvents",
            "description": "Deprecated: use next_page_token or prev_page_token instead. The parameters required to retrieve the next page of events. Omitted if there are no more events to read."
          },
          "next_page_token": {
            "type": "string",
            "description": "This field represents the pagination token to retrieve the next page of results. If the value is \"\", it means no further results for the request."
          },
          "prev_page_token": {
            "type": "string",
            "description": "This field represents the pagination token to retrieve the previous page of results. If the value is \"\", it means no further results for the request."
          },
          "total_count": {
            "type": "integer",
            "description": "Deprecated: Returns 0 when request uses page_token. Will start returning zero when request uses offset/limit soon. The total number of events filtered by the start_time, end_time, and event_types."
          }
        }
      },
      "GetInstancePool": {
        "type": "object",
        "properties": {
          "instance_pool_id": {
            "type": "string"
          },
          "aws_attributes": {
            "$ref": "#/components/schemas/InstancePoolAwsAttributes",
            "description": "Attributes related to instance pools running on Amazon Web Services. If not specified at pool creation, a set of default values will be used."
          },
          "azure_attributes": {
            "$ref": "#/components/schemas/InstancePoolAzureAttributes",
            "description": "Attributes related to instance pools running on Azure. If not specified at pool creation, a set of default values will be used."
          },
          "custom_tags": {
            "type": "object",
            "description": "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags"
          },
          "default_tags": {
            "type": "object",
            "description": "Tags that are added by Databricks regardless of any ``custom_tags``, including: - Vendor: Databricks - InstancePoolCreator: <user_id_of_creator> - InstancePoolName: <name_of_pool> - InstancePoolId: <id_of_pool>"
          },
          "disk_spec": {
            "$ref": "#/components/schemas/DiskSpec",
            "description": "Defines the specification of the disks that will be attached to all spark containers."
          },
          "enable_elastic_disk": {
            "type": "boolean",
            "description": "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire additional disk space when its Spark workers are running low on disk space. In AWS, this feature requires specific AWS permissions to function correctly - refer to the User Guide for more details."
          },
          "gcp_attributes": {
            "$ref": "#/components/schemas/InstancePoolGcpAttributes",
            "description": "Attributes related to instance pools running on Google Cloud Platform. If not specified at pool creation, a set of default values will be used."
          },
          "idle_instance_autotermination_minutes": {
            "type": "integer",
            "description": "Automatically terminates the extra instances in the pool cache after they are inactive for this time in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances will be automatically terminated after a default timeout. If specified, the threshold must be between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances from the cache if min cache size could still hold."
          },
          "instance_pool_name": {
            "type": "string",
            "description": "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100 characters."
          },
          "max_capacity": {
            "type": "integer",
            "description": "Maximum number of outstanding instances to keep in the pool, including both instances used by clusters and idle instances. Clusters that require further instance provisioning will fail during upsize requests."
          },
          "min_idle_instances": {
            "type": "integer",
            "description": "Minimum number of idle instances to keep in the instance pool"
          },
          "node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for the pool."
          },
          "node_type_id": {
            "type": "string",
            "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
          },
          "preloaded_docker_images": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/DockerImage"
            },
            "description": "Custom Docker Image BYOC"
          },
          "preloaded_spark_versions": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "A list containing at most one preloaded Spark image version for the pool. Pool-backed clusters started with the preloaded Spark version will start faster. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
          },
          "remote_disk_throughput": {
            "type": "integer",
            "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED types."
          },
          "state": {
            "$ref": "#/components/schemas/InstancePoolState",
            "description": "Current state of the instance pool."
          },
          "stats": {
            "$ref": "#/components/schemas/InstancePoolStats",
            "description": "Usage statistics about the instance pool."
          },
          "status": {
            "$ref": "#/components/schemas/InstancePoolStatus",
            "description": "Status of failed pending instances in the pool."
          },
          "total_initial_remote_disk_size": {
            "type": "integer",
            "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED types."
          }
        },
        "required": [
          "instance_pool_id"
        ]
      },
      "GetInstancePoolPermissionLevelsResponse": {
        "type": "object",
        "properties": {
          "permission_levels": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InstancePoolPermissionsDescription"
            }
          }
        }
      },
      "GetSparkVersionsResponse": {
        "type": "object",
        "properties": {
          "versions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/SparkVersion"
            }
          }
        }
      },
      "GlobalInitScriptDetails": {
        "type": "object",
        "properties": {
          "created_at": {
            "type": "integer"
          },
          "created_by": {
            "type": "string",
            "description": "The username of the user who created the script."
          },
          "enabled": {
            "type": "boolean",
            "description": "Specifies whether the script is enabled. The script runs only if enabled."
          },
          "name": {
            "type": "string",
            "description": "The name of the script"
          },
          "position": {
            "type": "integer",
            "description": "The position of a script, where 0 represents the first script to run, 1 is the second script to run, in ascending order."
          },
          "script_id": {
            "type": "string",
            "description": "The global init script ID."
          },
          "updated_at": {
            "type": "integer",
            "description": "Time when the script was updated, represented as a Unix timestamp in milliseconds."
          },
          "updated_by": {
            "type": "string",
            "description": "The username of the user who last updated the script"
          }
        }
      },
      "GlobalInitScriptDetailsWithContent": {
        "type": "object",
        "properties": {
          "created_at": {
            "type": "integer"
          },
          "created_by": {
            "type": "string",
            "description": "The username of the user who created the script."
          },
          "enabled": {
            "type": "boolean",
            "description": "Specifies whether the script is enabled. The script runs only if enabled."
          },
          "name": {
            "type": "string",
            "description": "The name of the script"
          },
          "position": {
            "type": "integer",
            "description": "The position of a script, where 0 represents the first script to run, 1 is the second script to run, in ascending order."
          },
          "script": {
            "type": "string",
            "description": "The Base64-encoded content of the script."
          },
          "script_id": {
            "type": "string",
            "description": "The global init script ID."
          },
          "updated_at": {
            "type": "integer",
            "description": "Time when the script was updated, represented as a Unix timestamp in milliseconds."
          },
          "updated_by": {
            "type": "string",
            "description": "The username of the user who last updated the script"
          }
        }
      },
      "InitScriptEventDetails": {
        "type": "object",
        "properties": {
          "cluster": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InitScriptInfoAndExecutionDetails"
            }
          },
          "global": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InitScriptInfoAndExecutionDetails"
            },
            "description": "The global init scripts associated with this cluster event."
          },
          "reported_for_node": {
            "type": "string",
            "description": "The private ip of the node we are reporting init script execution details for (we will select the execution details from only one node rather than reporting the execution details from every node to keep these event details small) This should only be defined for the INIT_SCRIPTS_FINISHED event"
          }
        }
      },
      "InitScriptInfo": {
        "type": "object",
        "properties": {
          "abfss": {
            "$ref": "#/components/schemas/Adlsgen2Info",
            "description": "destination needs to be provided, e.g. `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>`"
          },
          "dbfs": {
            "$ref": "#/components/schemas/DbfsStorageInfo",
            "description": "destination needs to be provided. e.g. `{ \"dbfs\": { \"destination\" : \"dbfs:/home/cluster_log\" } }`"
          },
          "file": {
            "$ref": "#/components/schemas/LocalFileInfo",
            "description": "destination needs to be provided, e.g. `{ \"file\": { \"destination\": \"file:/my/local/file.sh\" } }`"
          },
          "gcs": {
            "$ref": "#/components/schemas/GcsStorageInfo",
            "description": "destination needs to be provided, e.g. `{ \"gcs\": { \"destination\": \"gs://my-bucket/file.sh\" } }`"
          },
          "s3": {
            "$ref": "#/components/schemas/S3StorageInfo",
            "description": "destination and either the region or endpoint need to be provided. e.g. `{ \\\"s3\\\": { \\\"destination\\\": \\\"s3://cluster_log_bucket/prefix\\\", \\\"region\\\": \\\"us-west-2\\\" } }` Cluster iam role is used to access s3, please make sure the cluster iam role in `instance_profile_arn` has permission to write data to the s3 destination."
          },
          "volumes": {
            "$ref": "#/components/schemas/VolumesStorageInfo",
            "description": "destination needs to be provided. e.g. `{ \\\"volumes\\\" : { \\\"destination\\\" : \\\"/Volumes/my-init.sh\\\" } }`"
          },
          "workspace": {
            "$ref": "#/components/schemas/WorkspaceStorageInfo",
            "description": "destination needs to be provided, e.g. `{ \"workspace\": { \"destination\": \"/cluster-init-scripts/setup-datadog.sh\" } }`"
          }
        },
        "description": "Config for an individual init script Next ID: 11"
      },
      "InitScriptInfoAndExecutionDetails": {
        "type": "object",
        "properties": {
          "abfss": {
            "$ref": "#/components/schemas/Adlsgen2Info"
          },
          "dbfs": {
            "$ref": "#/components/schemas/DbfsStorageInfo",
            "description": "destination needs to be provided. e.g. `{ \"dbfs\": { \"destination\" : \"dbfs:/home/cluster_log\" } }`"
          },
          "error_message": {
            "type": "string",
            "description": "Additional details regarding errors (such as a file not found message if the status is FAILED_FETCH). This field should only be used to provide *additional* information to the status field, not duplicate it."
          },
          "execution_duration_seconds": {
            "type": "integer",
            "description": "The number duration of the script execution in seconds"
          },
          "file": {
            "$ref": "#/components/schemas/LocalFileInfo",
            "description": "destination needs to be provided, e.g. `{ \"file\": { \"destination\": \"file:/my/local/file.sh\" } }`"
          },
          "gcs": {
            "$ref": "#/components/schemas/GcsStorageInfo",
            "description": "destination needs to be provided, e.g. `{ \"gcs\": { \"destination\": \"gs://my-bucket/file.sh\" } }`"
          },
          "s3": {
            "$ref": "#/components/schemas/S3StorageInfo",
            "description": "destination and either the region or endpoint need to be provided. e.g. `{ \\\"s3\\\": { \\\"destination\\\": \\\"s3://cluster_log_bucket/prefix\\\", \\\"region\\\": \\\"us-west-2\\\" } }` Cluster iam role is used to access s3, please make sure the cluster iam role in `instance_profile_arn` has permission to write data to the s3 destination."
          },
          "status": {
            "$ref": "#/components/schemas/InitScriptExecutionDetailsInitScriptExecutionStatus",
            "description": "The current status of the script"
          },
          "stderr": {
            "type": "string",
            "description": "The stderr output from the init script execution. Only populated when init scripts debug is enabled and script execution fails."
          },
          "volumes": {
            "$ref": "#/components/schemas/VolumesStorageInfo",
            "description": "destination needs to be provided. e.g. `{ \\\"volumes\\\" : { \\\"destination\\\" : \\\"/Volumes/my-init.sh\\\" } }`"
          },
          "workspace": {
            "$ref": "#/components/schemas/WorkspaceStorageInfo",
            "description": "destination needs to be provided, e.g. `{ \"workspace\": { \"destination\": \"/cluster-init-scripts/setup-datadog.sh\" } }`"
          }
        }
      },
      "InstallLibrariesResponse": {
        "type": "object",
        "properties": {}
      },
      "InstancePoolAccessControlRequest": {
        "type": "object",
        "properties": {
          "group_name": {
            "type": "string"
          },
          "permission_level": {
            "$ref": "#/components/schemas/InstancePoolPermissionLevel"
          },
          "service_principal_name": {
            "type": "string",
            "description": "application ID of a service principal"
          },
          "user_name": {
            "type": "string",
            "description": "name of the user"
          }
        }
      },
      "InstancePoolAccessControlResponse": {
        "type": "object",
        "properties": {
          "all_permissions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InstancePoolPermission"
            }
          },
          "display_name": {
            "type": "string",
            "description": "Display name of the user or service principal."
          },
          "group_name": {
            "type": "string",
            "description": "name of the group"
          },
          "service_principal_name": {
            "type": "string",
            "description": "Name of the service principal."
          },
          "user_name": {
            "type": "string",
            "description": "name of the user"
          }
        }
      },
      "InstancePoolAndStats": {
        "type": "object",
        "properties": {
          "aws_attributes": {
            "$ref": "#/components/schemas/InstancePoolAwsAttributes"
          },
          "azure_attributes": {
            "$ref": "#/components/schemas/InstancePoolAzureAttributes",
            "description": "Attributes related to instance pools running on Azure. If not specified at pool creation, a set of default values will be used."
          },
          "custom_tags": {
            "type": "object",
            "description": "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags"
          },
          "default_tags": {
            "type": "object",
            "description": "Tags that are added by Databricks regardless of any ``custom_tags``, including: - Vendor: Databricks - InstancePoolCreator: <user_id_of_creator> - InstancePoolName: <name_of_pool> - InstancePoolId: <id_of_pool>"
          },
          "disk_spec": {
            "$ref": "#/components/schemas/DiskSpec",
            "description": "Defines the specification of the disks that will be attached to all spark containers."
          },
          "enable_elastic_disk": {
            "type": "boolean",
            "description": "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire additional disk space when its Spark workers are running low on disk space. In AWS, this feature requires specific AWS permissions to function correctly - refer to the User Guide for more details."
          },
          "gcp_attributes": {
            "$ref": "#/components/schemas/InstancePoolGcpAttributes",
            "description": "Attributes related to instance pools running on Google Cloud Platform. If not specified at pool creation, a set of default values will be used."
          },
          "idle_instance_autotermination_minutes": {
            "type": "integer",
            "description": "Automatically terminates the extra instances in the pool cache after they are inactive for this time in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances will be automatically terminated after a default timeout. If specified, the threshold must be between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances from the cache if min cache size could still hold."
          },
          "instance_pool_id": {
            "type": "string",
            "description": "Canonical unique identifier for the pool."
          },
          "instance_pool_name": {
            "type": "string",
            "description": "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100 characters."
          },
          "max_capacity": {
            "type": "integer",
            "description": "Maximum number of outstanding instances to keep in the pool, including both instances used by clusters and idle instances. Clusters that require further instance provisioning will fail during upsize requests."
          },
          "min_idle_instances": {
            "type": "integer",
            "description": "Minimum number of idle instances to keep in the instance pool"
          },
          "node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for the pool."
          },
          "node_type_id": {
            "type": "string",
            "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
          },
          "preloaded_docker_images": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/DockerImage"
            },
            "description": "Custom Docker Image BYOC"
          },
          "preloaded_spark_versions": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "A list containing at most one preloaded Spark image version for the pool. Pool-backed clusters started with the preloaded Spark version will start faster. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
          },
          "remote_disk_throughput": {
            "type": "integer",
            "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED types."
          },
          "state": {
            "$ref": "#/components/schemas/InstancePoolState",
            "description": "Current state of the instance pool."
          },
          "stats": {
            "$ref": "#/components/schemas/InstancePoolStats",
            "description": "Usage statistics about the instance pool."
          },
          "status": {
            "$ref": "#/components/schemas/InstancePoolStatus",
            "description": "Status of failed pending instances in the pool."
          },
          "total_initial_remote_disk_size": {
            "type": "integer",
            "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED types."
          }
        }
      },
      "InstancePoolAwsAttributes": {
        "type": "object",
        "properties": {
          "availability": {
            "$ref": "#/components/schemas/InstancePoolAwsAttributesAvailability",
            "description": "Availability type used for the spot nodes."
          },
          "instance_profile_arn": {
            "type": "string",
            "description": "All AWS instances belonging to the instance pool will have this instance profile. If omitted, instances will initially be launched with the workspace's default instance profile. If defined, clusters that use the pool will inherit the instance profile, and must not specify their own instance profile on cluster creation or update. If the pool does not specify an instance profile, clusters using the pool may specify any instance profile. The instance profile must have previously been added to the Databricks environment by an account administrator. This feature may only be available to certain customer plans."
          },
          "spot_bid_price_percent": {
            "type": "integer",
            "description": "Calculates the bid price for AWS spot instances, as a percentage of the corresponding instance type's on-demand price. For example, if this field is set to 50, and the cluster needs a new `r3.xlarge` spot instance, then the bid price is half of the price of on-demand `r3.xlarge` instances. Similarly, if this field is set to 200, the bid price is twice the price of on-demand `r3.xlarge` instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose bid price percentage matches this field will be considered. Note that, for safety, we enforce this field to be no more than 10000."
          },
          "zone_id": {
            "type": "string",
            "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like \"us-west-2a\". The provided availability zone must be in the same region as the Databricks deployment. For example, \"us-west-2a\" is not a valid zone id if the Databricks deployment resides in the \"us-east-1\" region. This is an optional field at cluster creation, and if not specified, a default zone will be used. The list of available zones as well as the default value can be found by using the `List Zones` method."
          }
        },
        "description": "Attributes set during instance pool creation which are related to Amazon Web Services."
      },
      "InstancePoolAzureAttributes": {
        "type": "object",
        "properties": {
          "availability": {
            "$ref": "#/components/schemas/InstancePoolAzureAttributesAvailability",
            "description": "Availability type used for the spot nodes."
          },
          "spot_bid_max_price": {
            "type": "number",
            "description": "With variable pricing, you have option to set a max price, in US dollars (USD) For example, the value 2 would be a max price of $2.00 USD per hour. If you set the max price to be -1, the VM won't be evicted based on price. The price for the VM will be the current price for spot or the price for a standard VM, which ever is less, as long as there is capacity and quota available."
          }
        },
        "description": "Attributes set during instance pool creation which are related to Azure."
      },
      "InstancePoolGcpAttributes": {
        "type": "object",
        "properties": {
          "gcp_availability": {
            "$ref": "#/components/schemas/GcpAvailability"
          },
          "local_ssd_count": {
            "type": "integer",
            "description": "If provided, each node in the instance pool will have this number of local SSDs attached. Each local SSD is 375GB in size. Refer to [GCP documentation] for the supported number of local SSDs for each instance type. [GCP documentation]: https://cloud.google.com/compute/docs/disks/local-ssd#choose_number_local_ssds"
          },
          "zone_id": {
            "type": "string",
            "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like \"us-west1-a\". The provided availability zone must be in the same region as the Databricks workspace. For example, \"us-west1-a\" is not a valid zone id if the Databricks workspace resides in the \"us-east1\" region. This is an optional field at instance pool creation, and if not specified, a default zone will be used. This field can be one of the following: - \"HA\" => High availability, spread nodes across availability zones for a Databricks deployment region - A GCP availability zone => Pick One of the available zones for (machine type + region) from https://cloud.google.com/compute/docs/regions-zones (e.g. \"us-west1-a\"). If empty, Databricks picks an availability zone to schedule the cluster on."
          }
        },
        "description": "Attributes set during instance pool creation which are related to GCP."
      },
      "InstancePoolPermission": {
        "type": "object",
        "properties": {
          "inherited": {
            "type": "boolean"
          },
          "inherited_from_object": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "permission_level": {
            "$ref": "#/components/schemas/InstancePoolPermissionLevel"
          }
        }
      },
      "InstancePoolPermissions": {
        "type": "object",
        "properties": {
          "access_control_list": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InstancePoolAccessControlResponse"
            }
          },
          "object_id": {
            "type": "string"
          },
          "object_type": {
            "type": "string"
          }
        }
      },
      "InstancePoolPermissionsDescription": {
        "type": "object",
        "properties": {
          "description": {
            "type": "string"
          },
          "permission_level": {
            "$ref": "#/components/schemas/InstancePoolPermissionLevel"
          }
        }
      },
      "InstancePoolStats": {
        "type": "object",
        "properties": {
          "idle_count": {
            "type": "integer"
          },
          "pending_idle_count": {
            "type": "integer",
            "description": "Number of pending instances in the pool that are NOT part of a cluster."
          },
          "pending_used_count": {
            "type": "integer",
            "description": "Number of pending instances in the pool that are part of a cluster."
          },
          "used_count": {
            "type": "integer",
            "description": "Number of active instances in the pool that are part of a cluster."
          }
        }
      },
      "InstancePoolStatus": {
        "type": "object",
        "properties": {
          "pending_instance_errors": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/PendingInstanceError"
            }
          }
        }
      },
      "InstanceProfile": {
        "type": "object",
        "properties": {
          "instance_profile_arn": {
            "type": "string"
          },
          "iam_role_arn": {
            "type": "string",
            "description": "The AWS IAM role ARN of the role associated with the instance profile. This field is required if your role name and instance profile name do not match and you want to use the instance profile with [Databricks SQL Serverless]. Otherwise, this field is optional. [Databricks SQL Serverless]: https://docs.databricks.com/sql/admin/serverless.html"
          },
          "is_meta_instance_profile": {
            "type": "boolean",
            "description": "Boolean flag indicating whether the instance profile should only be used in credential passthrough scenarios. If true, it means the instance profile contains an meta IAM role which could assume a wide range of roles. Therefore it should always be used with authorization. This field is optional, the default value is `false`."
          }
        },
        "required": [
          "instance_profile_arn"
        ]
      },
      "Library": {
        "type": "object",
        "properties": {
          "cran": {
            "$ref": "#/components/schemas/RCranLibrary"
          },
          "egg": {
            "type": "string",
            "description": "Deprecated. URI of the egg library to install. Installing Python egg files is deprecated and is not supported in Databricks Runtime 14.0 and above."
          },
          "jar": {
            "type": "string",
            "description": "URI of the JAR library to install. Supported URIs include Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example: `{ \"jar\": \"/Workspace/path/to/library.jar\" }`, `{ \"jar\" : \"/Volumes/path/to/library.jar\" }` or `{ \"jar\": \"s3://my-bucket/library.jar\" }`. If S3 is used, please make sure the cluster has read access on the library. You may need to launch the cluster with an IAM role to access the S3 URI."
          },
          "maven": {
            "$ref": "#/components/schemas/MavenLibrary",
            "description": "Specification of a maven library to be installed. For example: `{ \"coordinates\": \"org.jsoup:jsoup:1.7.2\" }`"
          },
          "pypi": {
            "$ref": "#/components/schemas/PythonPyPiLibrary",
            "description": "Specification of a PyPi library to be installed. For example: `{ \"package\": \"simplejson\" }`"
          },
          "requirements": {
            "type": "string",
            "description": "URI of the requirements.txt file to install. Only Workspace paths and Unity Catalog Volumes paths are supported. For example: `{ \"requirements\": \"/Workspace/path/to/requirements.txt\" }` or `{ \"requirements\" : \"/Volumes/path/to/requirements.txt\" }`"
          },
          "whl": {
            "type": "string",
            "description": "URI of the wheel library to install. Supported URIs include Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example: `{ \"whl\": \"/Workspace/path/to/library.whl\" }`, `{ \"whl\" : \"/Volumes/path/to/library.whl\" }` or `{ \"whl\": \"s3://my-bucket/library.whl\" }`. If S3 is used, please make sure the cluster has read access on the library. You may need to launch the cluster with an IAM role to access the S3 URI."
          }
        }
      },
      "LibraryFullStatus": {
        "type": "object",
        "properties": {
          "is_library_for_all_clusters": {
            "type": "boolean",
            "description": "Whether the library was set to be installed on all clusters via the libraries UI."
          },
          "library": {
            "$ref": "#/components/schemas/Library",
            "description": "Unique identifier for the library."
          },
          "messages": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "All the info and warning messages that have occurred so far for this library."
          },
          "status": {
            "$ref": "#/components/schemas/LibraryInstallStatus",
            "description": "Status of installing the library on the cluster."
          }
        },
        "description": "The status of the library on a specific cluster."
      },
      "ListAllClusterLibraryStatusesResponse": {
        "type": "object",
        "properties": {
          "statuses": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterLibraryStatuses"
            }
          }
        }
      },
      "ListAvailableZonesResponse": {
        "type": "object",
        "properties": {
          "default_zone": {
            "type": "string"
          },
          "zones": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "The list of available zones (e.g., ['us-west-2c', 'us-east-2'])."
          }
        }
      },
      "ListClusterCompliancesResponse": {
        "type": "object",
        "properties": {
          "clusters": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterCompliance"
            }
          },
          "next_page_token": {
            "type": "string",
            "description": "This field represents the pagination token to retrieve the next page of results. If the value is \"\", it means no further results for the request."
          },
          "prev_page_token": {
            "type": "string",
            "description": "This field represents the pagination token to retrieve the previous page of results. If the value is \"\", it means no further results for the request."
          }
        }
      },
      "ListClustersFilterBy": {
        "type": "object",
        "properties": {
          "cluster_sources": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterSource"
            }
          },
          "cluster_states": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/State"
            },
            "description": "The current state of the clusters."
          },
          "is_pinned": {
            "type": "boolean",
            "description": "Whether the clusters are pinned or not."
          },
          "policy_id": {
            "type": "string",
            "description": "The ID of the cluster policy used to create the cluster if applicable."
          }
        }
      },
      "ListClustersResponse": {
        "type": "object",
        "properties": {
          "clusters": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/ClusterDetails"
            }
          },
          "next_page_token": {
            "type": "string",
            "description": "This field represents the pagination token to retrieve the next page of results. If the value is \"\", it means no further results for the request."
          },
          "prev_page_token": {
            "type": "string",
            "description": "This field represents the pagination token to retrieve the previous page of results. If the value is \"\", it means no further results for the request."
          }
        }
      },
      "ListClustersSortBy": {
        "type": "object",
        "properties": {
          "direction": {
            "$ref": "#/components/schemas/ListClustersSortByDirection"
          },
          "field": {
            "$ref": "#/components/schemas/ListClustersSortByField",
            "description": "The sorting criteria. By default, clusters are sorted by 3 columns from highest to lowest precedence: cluster state, pinned or unpinned, then cluster name."
          }
        }
      },
      "ListGlobalInitScriptsResponse": {
        "type": "object",
        "properties": {
          "scripts": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/GlobalInitScriptDetails"
            }
          }
        }
      },
      "ListInstancePools": {
        "type": "object",
        "properties": {
          "instance_pools": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InstancePoolAndStats"
            }
          }
        }
      },
      "ListInstanceProfilesResponse": {
        "type": "object",
        "properties": {
          "instance_profiles": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InstanceProfile"
            }
          }
        }
      },
      "ListNodeTypesResponse": {
        "type": "object",
        "properties": {
          "node_types": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/NodeType"
            }
          }
        }
      },
      "ListPoliciesResponse": {
        "type": "object",
        "properties": {
          "policies": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/Policy"
            }
          }
        }
      },
      "ListPolicyFamiliesResponse": {
        "type": "object",
        "properties": {
          "next_page_token": {
            "type": "string"
          },
          "policy_families": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/PolicyFamily"
            },
            "description": "List of policy families."
          }
        }
      },
      "LocalFileInfo": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string"
          }
        },
        "required": [
          "destination"
        ]
      },
      "LogAnalyticsInfo": {
        "type": "object",
        "properties": {
          "log_analytics_primary_key": {
            "type": "string"
          },
          "log_analytics_workspace_id": {
            "type": "string"
          }
        }
      },
      "LogSyncStatus": {
        "type": "object",
        "properties": {
          "last_attempted": {
            "type": "integer",
            "description": "The timestamp of last attempt. If the last attempt fails, `last_exception` will contain the exception in the last attempt."
          },
          "last_exception": {
            "type": "string",
            "description": "The exception thrown in the last attempt, it would be null (omitted in the response) if there is no exception in last attempted."
          }
        },
        "description": "The log delivery status"
      },
      "MavenLibrary": {
        "type": "object",
        "properties": {
          "coordinates": {
            "type": "string"
          },
          "exclusions": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of dependences to exclude. For example: `[\"slf4j:slf4j\", \"*:hadoop-client\"]`. Maven dependency exclusions: https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html."
          },
          "repo": {
            "type": "string",
            "description": "Maven repo to install the Maven package from. If omitted, both Maven Central Repository and Spark Packages are searched."
          }
        },
        "required": [
          "coordinates"
        ]
      },
      "NodeInstanceType": {
        "type": "object",
        "properties": {
          "instance_type_id": {
            "type": "string",
            "description": "Unique identifier across instance types"
          },
          "local_disk_size_gb": {
            "type": "integer",
            "description": "Size of the individual local disks attached to this instance (i.e. per local disk)."
          },
          "local_disks": {
            "type": "integer",
            "description": "Number of local disks that are present on this instance."
          },
          "local_nvme_disk_size_gb": {
            "type": "integer",
            "description": "Size of the individual local nvme disks attached to this instance (i.e. per local disk)."
          },
          "local_nvme_disks": {
            "type": "integer",
            "description": "Number of local nvme disks that are present on this instance."
          }
        },
        "required": [
          "instance_type_id"
        ],
        "description": "This structure embodies the machine type that hosts spark containers Note: this should be an\n    internal data structure for now It is defined in proto in case we want to send it over the wire\n    in the future (which is likely)"
      },
      "NodeType": {
        "type": "object",
        "properties": {
          "node_type_id": {
            "type": "string",
            "description": "Unique identifier for this node type."
          },
          "memory_mb": {
            "type": "integer",
            "description": "Memory (in MB) available for this node type."
          },
          "num_cores": {
            "type": "number",
            "description": "Number of CPU cores available for this node type. Note that this can be fractional, e.g., 2.5 cores, if the the number of cores on a machine instance is not divisible by the number of Spark nodes on that machine."
          },
          "description": {
            "type": "string",
            "description": "A string description associated with this node type, e.g., \"r3.xlarge\"."
          },
          "instance_type_id": {
            "type": "string",
            "description": "An identifier for the type of hardware that this node runs on, e.g., \"r3.2xlarge\" in AWS."
          },
          "category": {
            "type": "string",
            "description": "A descriptive category for this node type. Examples include \"Memory Optimized\" and \"Compute Optimized\"."
          },
          "display_order": {
            "type": "integer",
            "description": "An optional hint at the display order of node types in the UI. Within a node type category, lowest numbers come first."
          },
          "is_deprecated": {
            "type": "boolean",
            "description": "Whether the node type is deprecated. Non-deprecated node types offer greater performance."
          },
          "is_encrypted_in_transit": {
            "type": "boolean",
            "description": "AWS specific, whether this instance supports encryption in transit, used for hipaa and pci workloads."
          },
          "is_graviton": {
            "type": "boolean",
            "description": "Whether this is an Arm-based instance."
          },
          "is_hidden": {
            "type": "boolean",
            "description": "Whether this node is hidden from presentation in the UI."
          },
          "is_io_cache_enabled": {
            "type": "boolean",
            "description": "Whether this node comes with IO cache enabled by default."
          },
          "node_info": {
            "$ref": "#/components/schemas/CloudProviderNodeInfo",
            "description": "A collection of node type info reported by the cloud provider"
          },
          "node_instance_type": {
            "$ref": "#/components/schemas/NodeInstanceType",
            "description": "The NodeInstanceType object corresponding to instance_type_id"
          },
          "num_gpus": {
            "type": "integer",
            "description": "Number of GPUs available for this node type."
          },
          "photon_driver_capable": {
            "type": "boolean"
          },
          "photon_worker_capable": {
            "type": "boolean"
          },
          "support_cluster_tags": {
            "type": "boolean",
            "description": "Whether this node type support cluster tags."
          },
          "support_ebs_volumes": {
            "type": "boolean",
            "description": "Whether this node type support EBS volumes. EBS volumes is disabled for node types that we could place multiple corresponding containers on the same hosting instance."
          },
          "support_port_forwarding": {
            "type": "boolean",
            "description": "Whether this node type supports port forwarding."
          }
        },
        "required": [
          "node_type_id",
          "memory_mb",
          "num_cores",
          "description",
          "instance_type_id",
          "category"
        ],
        "description": "A description of a Spark node type including both the dimensions of the node and the instance\n    type on which it will be hosted."
      },
      "NodeTypeFlexibility": {
        "type": "object",
        "properties": {
          "alternate_node_type_ids": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "A list of node type IDs to use as fallbacks when the primary node type is unavailable."
          }
        },
        "description": "Configuration for flexible node types, allowing fallback to alternate node types during cluster\n    launch and upscale."
      },
      "PendingInstanceError": {
        "type": "object",
        "properties": {
          "instance_id": {
            "type": "string"
          },
          "message": {
            "type": "string"
          }
        },
        "description": "Error message of a failed pending instances"
      },
      "PermanentDeleteClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "PinClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "Policy": {
        "type": "object",
        "properties": {
          "created_at_timestamp": {
            "type": "integer",
            "description": "Creation time. The timestamp (in millisecond) when this Cluster Policy was created."
          },
          "creator_user_name": {
            "type": "string",
            "description": "Creator user name. The field won't be included in the response if the user has already been deleted."
          },
          "definition": {
            "type": "string",
            "description": "Policy definition document expressed in [Databricks Cluster Policy Definition Language]. [Databricks Cluster Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html"
          },
          "description": {
            "type": "string",
            "description": "Additional human-readable description of the cluster policy."
          },
          "is_default": {
            "type": "boolean",
            "description": "If true, policy is a default policy created and managed by Databricks. Default policies cannot be deleted, and their policy families cannot be changed."
          },
          "libraries": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/Library"
            },
            "description": "A list of libraries to be installed on the next cluster restart that uses this policy. The maximum number of libraries is 500."
          },
          "max_clusters_per_user": {
            "type": "integer",
            "description": "Max number of clusters per user that can be active using this policy. If not present, there is no max limit."
          },
          "name": {
            "type": "string",
            "description": "Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100 characters."
          },
          "policy_family_definition_overrides": {
            "type": "string",
            "description": "Policy definition JSON document expressed in [Databricks Policy Definition Language]. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition. [Databricks Policy Definition Language]: https://docs.databricks.com/administration-guide/clusters/policy-definition.html"
          },
          "policy_family_id": {
            "type": "string",
            "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition."
          },
          "policy_id": {
            "type": "string",
            "description": "Canonical unique identifier for the Cluster Policy."
          }
        },
        "description": "Describes a Cluster Policy entity."
      },
      "PolicyFamily": {
        "type": "object",
        "properties": {
          "definition": {
            "type": "string"
          },
          "description": {
            "type": "string",
            "description": "Human-readable description of the purpose of the policy family."
          },
          "name": {
            "type": "string",
            "description": "Name of the policy family."
          },
          "policy_family_id": {
            "type": "string",
            "description": "Unique identifier for the policy family."
          }
        }
      },
      "PythonPyPiLibrary": {
        "type": "object",
        "properties": {
          "package": {
            "type": "string"
          },
          "repo": {
            "type": "string",
            "description": "The repository where the package can be found. If not specified, the default pip index is used."
          }
        },
        "required": [
          "package"
        ]
      },
      "RCranLibrary": {
        "type": "object",
        "properties": {
          "package": {
            "type": "string"
          },
          "repo": {
            "type": "string",
            "description": "The repository where the package can be found. If not specified, the default CRAN repo is used."
          }
        },
        "required": [
          "package"
        ]
      },
      "RemoveResponse": {
        "type": "object",
        "properties": {}
      },
      "ResizeClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "RestartClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "Results": {
        "type": "object",
        "properties": {
          "cause": {
            "type": "string"
          },
          "data": {
            "type": "object"
          },
          "fileName": {
            "type": "string",
            "description": "The image data in one of the following formats: 1. A Data URL with base64-encoded image data: `data:image/{type};base64,{base64-data}`. Example: `data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUA...` 2. A FileStore file path for large images: `/plots/{filename}.png`. Example: `/plots/b6a7ad70-fb2c-4353-8aed-3f1e015174a4.png`"
          },
          "fileNames": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of image data for multiple images. Each element follows the same format as file_name."
          },
          "isJsonSchema": {
            "type": "boolean",
            "description": "true if a JSON schema is returned instead of a string representation of the Hive type."
          },
          "pos": {
            "type": "integer",
            "description": "internal field used by SDK"
          },
          "resultType": {
            "$ref": "#/components/schemas/ResultType"
          },
          "schema": {
            "type": "array",
            "items": {
              "type": "object"
            },
            "description": "The table schema"
          },
          "summary": {
            "type": "string",
            "description": "The summary of the error"
          },
          "truncated": {
            "type": "boolean",
            "description": "true if partial results are returned."
          }
        }
      },
      "S3StorageInfo": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string",
            "description": "S3 destination, e.g. `s3://my-bucket/some-prefix` Note that logs will be delivered using cluster iam role, please make sure you set cluster iam role and the role has write access to the destination. Please also note that you cannot use AWS keys to deliver logs."
          },
          "canned_acl": {
            "type": "string",
            "description": "(Optional) Set canned access control list for the logs, e.g. `bucket-owner-full-control`. If `canned_cal` is set, please make sure the cluster iam role has `s3:PutObjectAcl` permission on the destination bucket and prefix. The full list of possible canned acl can be found at http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl. Please also note that by default only the object owner gets full controls. If you are using cross account role for writing data, you may want to set `bucket-owner-full-control` to make bucket owner able to read the logs."
          },
          "enable_encryption": {
            "type": "boolean",
            "description": "(Optional) Flag to enable server side encryption, `false` by default."
          },
          "encryption_type": {
            "type": "string",
            "description": "(Optional) The encryption type, it could be `sse-s3` or `sse-kms`. It will be used only when encryption is enabled and the default type is `sse-s3`."
          },
          "endpoint": {
            "type": "string",
            "description": "S3 endpoint, e.g. `https://s3-us-west-2.amazonaws.com`. Either region or endpoint needs to be set. If both are set, endpoint will be used."
          },
          "kms_key": {
            "type": "string",
            "description": "(Optional) Kms key which will be used if encryption is enabled and encryption type is set to `sse-kms`."
          },
          "region": {
            "type": "string",
            "description": "S3 region, e.g. `us-west-2`. Either region or endpoint needs to be set. If both are set, endpoint will be used."
          }
        },
        "required": [
          "destination"
        ],
        "description": "A storage location in Amazon S3"
      },
      "SparkNode": {
        "type": "object",
        "properties": {
          "host_private_ip": {
            "type": "string",
            "description": "The private IP address of the host instance."
          },
          "instance_id": {
            "type": "string",
            "description": "Globally unique identifier for the host instance from the cloud provider."
          },
          "node_aws_attributes": {
            "$ref": "#/components/schemas/SparkNodeAwsAttributes",
            "description": "Attributes specific to AWS for a Spark node."
          },
          "node_id": {
            "type": "string",
            "description": "Globally unique identifier for this node."
          },
          "private_ip": {
            "type": "string",
            "description": "Private IP address (typically a 10.x.x.x address) of the Spark node. Note that this is different from the private IP address of the host instance."
          },
          "public_dns": {
            "type": "string",
            "description": "Public DNS address of this node. This address can be used to access the Spark JDBC server on the driver node. To communicate with the JDBC server, traffic must be manually authorized by adding security group rules to the \"worker-unmanaged\" security group via the AWS console."
          },
          "start_timestamp": {
            "type": "integer",
            "description": "The timestamp (in millisecond) when the Spark node is launched."
          }
        },
        "description": "Describes a specific Spark driver or executor."
      },
      "SparkNodeAwsAttributes": {
        "type": "object",
        "properties": {
          "is_spot": {
            "type": "boolean",
            "description": "Whether this node is on an Amazon spot instance."
          }
        },
        "description": "Attributes specific to AWS for a Spark node."
      },
      "SparkVersion": {
        "type": "object",
        "properties": {
          "key": {
            "type": "string"
          },
          "name": {
            "type": "string",
            "description": "A descriptive name for this Spark version, for example \"Spark 2.1\"."
          }
        }
      },
      "StartClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "TerminationReason": {
        "type": "object",
        "properties": {
          "code": {
            "$ref": "#/components/schemas/TerminationReasonCode"
          },
          "parameters": {
            "type": "object",
            "description": "list of parameters that provide additional information about why the cluster was terminated"
          },
          "type": {
            "$ref": "#/components/schemas/TerminationReasonType",
            "description": "type of the termination"
          }
        }
      },
      "UninstallLibrariesResponse": {
        "type": "object",
        "properties": {}
      },
      "UnpinClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "UpdateClusterResource": {
        "type": "object",
        "properties": {
          "autoscale": {
            "$ref": "#/components/schemas/AutoScale"
          },
          "autotermination_minutes": {
            "type": "integer",
            "description": "Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination."
          },
          "aws_attributes": {
            "$ref": "#/components/schemas/AwsAttributes",
            "description": "Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used."
          },
          "azure_attributes": {
            "$ref": "#/components/schemas/AzureAttributes",
            "description": "Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used."
          },
          "cluster_log_conf": {
            "$ref": "#/components/schemas/ClusterLogConf",
            "description": "The configuration for delivering spark logs to a long-term storage destination. Three kinds of destinations (DBFS, S3 and Unity Catalog volumes) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`."
          },
          "cluster_name": {
            "type": "string",
            "description": "Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. For job clusters, the cluster name is automatically set based on the job and job run IDs."
          },
          "custom_tags": {
            "type": "object",
            "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes: - Currently, Databricks allows at most 45 custom tags - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags"
          },
          "data_security_mode": {
            "$ref": "#/components/schemas/DataSecurityMode"
          },
          "docker_image": {
            "$ref": "#/components/schemas/DockerImage",
            "description": "Custom docker image BYOC"
          },
          "driver_instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned."
          },
          "driver_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for the driver node."
          },
          "driver_node_type_id": {
            "type": "string",
            "description": "The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. This field, along with node_type_id, should not be set if virtual_cluster_size is set. If both driver_node_type_id, node_type_id, and virtual_cluster_size are specified, driver_node_type_id and node_type_id take precedence."
          },
          "enable_elastic_disk": {
            "type": "boolean",
            "description": "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space."
          },
          "enable_local_disk_encryption": {
            "type": "boolean",
            "description": "Whether to enable LUKS on cluster VMs' local disks"
          },
          "gcp_attributes": {
            "$ref": "#/components/schemas/GcpAttributes",
            "description": "Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used."
          },
          "init_scripts": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/InitScriptInfo"
            },
            "description": "The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If `cluster_log_conf` is specified, init script logs are sent to `<destination>/<cluster-ID>/init_scripts`."
          },
          "instance_pool_id": {
            "type": "string",
            "description": "The optional ID of the instance pool to which the cluster belongs."
          },
          "is_single_node": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`"
          },
          "kind": {
            "$ref": "#/components/schemas/Kind"
          },
          "node_type_id": {
            "type": "string",
            "description": "This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call."
          },
          "num_workers": {
            "type": "integer",
            "description": "Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned."
          },
          "policy_id": {
            "type": "string",
            "description": "The ID of the cluster policy used to create the cluster if applicable."
          },
          "remote_disk_throughput": {
            "type": "integer",
            "description": "If set, what the configurable throughput (in Mb/s) for the remote disk is. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "runtime_engine": {
            "$ref": "#/components/schemas/RuntimeEngine",
            "description": "Determines the cluster's runtime engine, either standard or Photon. This field is not compatible with legacy `spark_version` values that contain `-photon-`. Remove `-photon-` from the `spark_version` and set `runtime_engine` to `PHOTON`. If left unspecified, the runtime engine defaults to standard unless the spark_version contains -photon-, in which case Photon will be used."
          },
          "single_user_name": {
            "type": "string",
            "description": "Single user name if data_security_mode is `SINGLE_USER`"
          },
          "spark_conf": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively."
          },
          "spark_env_vars": {
            "type": "object",
            "description": "An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (i.e., `export X='Y'`) while launching the driver and workers. In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`"
          },
          "spark_version": {
            "type": "string",
            "description": "The Spark version of the cluster, e.g. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call."
          },
          "ssh_public_keys": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name `ubuntu` on port `2200`. Up to 10 keys can be specified."
          },
          "total_initial_remote_disk_size": {
            "type": "integer",
            "description": "If set, what the total initial volume size (in GB) of the remote disks should be. Currently only supported for GCP HYPERDISK_BALANCED disks."
          },
          "use_ml_runtime": {
            "type": "boolean",
            "description": "This field can only be used when `kind = CLASSIC_PREVIEW`. `effective_spark_version` is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is gpu node or not."
          },
          "worker_node_type_flexibility": {
            "$ref": "#/components/schemas/NodeTypeFlexibility",
            "description": "Flexible node type configuration for worker nodes."
          },
          "workload_type": {
            "$ref": "#/components/schemas/WorkloadType"
          }
        }
      },
      "UpdateClusterResponse": {
        "type": "object",
        "properties": {}
      },
      "UpdateResponse": {
        "type": "object",
        "properties": {}
      },
      "VolumesStorageInfo": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string",
            "description": "UC Volumes destination, e.g. `/Volumes/catalog/schema/vol1/init-scripts/setup-datadog.sh` or `dbfs:/Volumes/catalog/schema/vol1/init-scripts/setup-datadog.sh`"
          }
        },
        "required": [
          "destination"
        ],
        "description": "A storage location back by UC Volumes."
      },
      "WorkloadType": {
        "type": "object",
        "properties": {
          "clients": {
            "$ref": "#/components/schemas/ClientsTypes",
            "description": "defined what type of clients can use the cluster. E.g. Notebooks, Jobs"
          }
        },
        "required": [
          "clients"
        ],
        "description": "Cluster Attributes showing for clusters workload types."
      },
      "WorkspaceStorageInfo": {
        "type": "object",
        "properties": {
          "destination": {
            "type": "string",
            "description": "wsfs destination, e.g. `workspace:/cluster-init-scripts/setup-datadog.sh`"
          }
        },
        "required": [
          "destination"
        ],
        "description": "A storage location in Workspace Filesystem (WSFS)"
      },
      "AwsAvailability": {
        "type": "string",
        "enum": [
          "ON_DEMAND",
          "SPOT",
          "SPOT_WITH_FALLBACK"
        ],
        "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones.\n\nNote: If `first_on_demand` is zero, this availability type will be used for the entire cluster."
      },
      "AzureAvailability": {
        "type": "string",
        "enum": [
          "ON_DEMAND_AZURE",
          "SPOT_AZURE",
          "SPOT_WITH_FALLBACK_AZURE"
        ],
        "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones. Note: If\n`first_on_demand` is zero, this availability type will be used for the entire cluster."
      },
      "CloudProviderNodeStatus": {
        "type": "string",
        "enum": [
          "NotAvailableInRegion",
          "NotEnabledOnSubscription"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "ClusterPermissionLevel": {
        "type": "string",
        "enum": [
          "CAN_ATTACH_TO",
          "CAN_MANAGE",
          "CAN_RESTART"
        ],
        "description": "Permission level"
      },
      "ClusterPolicyPermissionLevel": {
        "type": "string",
        "enum": [
          "CAN_USE"
        ],
        "description": "Permission level"
      },
      "ClusterSource": {
        "type": "string",
        "enum": [
          "API",
          "JOB",
          "MODELS",
          "PIPELINE",
          "PIPELINE_MAINTENANCE",
          "SQL",
          "UI"
        ],
        "description": "Determines whether the cluster was created by a user through the UI, created by the Databricks\nJobs Scheduler, or through an API request. This is the same as cluster_creator, but read only."
      },
      "CommandStatus": {
        "type": "string",
        "enum": [
          "Cancelled",
          "Cancelling",
          "Error",
          "Finished",
          "Queued",
          "Running"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "ContextStatus": {
        "type": "string",
        "enum": [
          "Error",
          "Pending",
          "Running"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "DataPlaneEventDetailsEventType": {
        "type": "string",
        "enum": [
          "NODE_BLACKLISTED",
          "NODE_EXCLUDED_DECOMMISSIONED"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "DataSecurityMode": {
        "type": "string",
        "enum": [
          "DATA_SECURITY_MODE_AUTO",
          "DATA_SECURITY_MODE_DEDICATED",
          "DATA_SECURITY_MODE_STANDARD",
          "LEGACY_PASSTHROUGH",
          "LEGACY_SINGLE_USER",
          "LEGACY_SINGLE_USER_STANDARD",
          "LEGACY_TABLE_ACL",
          "NONE",
          "SINGLE_USER",
          "USER_ISOLATION"
        ],
        "description": "Data security mode decides what data governance model to use when accessing data from a cluster.\n\nThe following modes can only be used when `kind = CLASSIC_PREVIEW`. * `DATA_SECURITY_MODE_AUTO`:\nDatabricks will choose the most appropriate access mode depending on your compute configuration.\n* `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`. * `DATA_SECURITY_MODE_DEDICATED`:\nAlias for `SINGLE_USER`.\n\nThe following modes can be used regardless of `kind`. * `NONE`: No security isolation for\nmultiple users sharing the cluster. Data governance features are not available in this mode. *\n`SINGLE_USER`: A secure cluster that can only be exclusively used by a single user specified in\n`single_user_name`. Most programming languages, cluster features and data governance features\nare available in this mode. * `USER_ISOLATION`: A secure cluster that can be shared by multiple\nusers. Cluster users are fully isolated so that they cannot see each other's data and\ncredentials. Most data governance features are supported in this mode. But programming languages\nand cluster features might be limited.\n\nThe following modes are deprecated starting with Databricks Runtime 15.0 and will be removed for\nfuture Databricks Runtime versions:\n\n* `LEGACY_TABLE_ACL`: This mode is for users migrating from legacy Table ACL clusters. *\n`LEGACY_PASSTHROUGH`: This mode is for users migrating from legacy Passthrough on high\nconcurrency clusters. * `LEGACY_SINGLE_USER`: This mode is for users migrating from legacy\nPassthrough on standard clusters. * `LEGACY_SINGLE_USER_STANDARD`: This mode provides a way that\ndoesn\u2019t have UC nor passthrough enabled."
      },
      "DiskTypeAzureDiskVolumeType": {
        "type": "string",
        "enum": [
          "PREMIUM_LRS",
          "STANDARD_LRS"
        ],
        "description": "All Azure Disk types that Databricks supports. See\nhttps://docs.microsoft.com/en-us/azure/storage/storage-about-disks-and-vhds-linux#types-of-disks"
      },
      "DiskTypeEbsVolumeType": {
        "type": "string",
        "enum": [
          "GENERAL_PURPOSE_SSD",
          "THROUGHPUT_OPTIMIZED_HDD"
        ],
        "description": "All EBS volume types that Databricks supports. See https://aws.amazon.com/ebs/details/ for\ndetails."
      },
      "EbsVolumeType": {
        "type": "string",
        "enum": [
          "GENERAL_PURPOSE_SSD",
          "THROUGHPUT_OPTIMIZED_HDD"
        ],
        "description": "All EBS volume types that Databricks supports. See https://aws.amazon.com/ebs/details/ for\ndetails."
      },
      "EventDetailsCause": {
        "type": "string",
        "enum": [
          "AUTORECOVERY",
          "AUTOSCALE",
          "AUTOSCALE_V2",
          "REPLACE_BAD_NODES",
          "USER_REQUEST"
        ],
        "description": "The cause of a change in target size."
      },
      "EventType": {
        "type": "string",
        "enum": [
          "ADD_NODES_FAILED",
          "AUTOMATIC_CLUSTER_UPDATE",
          "AUTOSCALING_BACKOFF",
          "AUTOSCALING_FAILED",
          "AUTOSCALING_STATS_REPORT",
          "CLUSTER_MIGRATED",
          "CREATING",
          "DBFS_DOWN",
          "DECOMMISSION_ENDED",
          "DECOMMISSION_STARTED",
          "DID_NOT_EXPAND_DISK",
          "DRIVER_HEALTHY",
          "DRIVER_NOT_RESPONDING",
          "DRIVER_UNAVAILABLE",
          "EDITED",
          "EXPANDED_DISK",
          "FAILED_TO_EXPAND_DISK",
          "INIT_SCRIPTS_FINISHED",
          "INIT_SCRIPTS_STARTED",
          "METASTORE_DOWN",
          "NODES_LOST",
          "NODE_BLACKLISTED",
          "NODE_EXCLUDED_DECOMMISSIONED",
          "PINNED",
          "RESIZING",
          "RESTARTING",
          "RUNNING",
          "SPARK_EXCEPTION",
          "STARTING",
          "TERMINATING",
          "UC_VOLUME_MISCONFIGURED",
          "UNPINNED",
          "UPSIZE_COMPLETED"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "GcpAvailability": {
        "type": "string",
        "enum": [
          "ON_DEMAND_GCP",
          "PREEMPTIBLE_GCP",
          "PREEMPTIBLE_WITH_FALLBACK_GCP"
        ],
        "description": "This field determines whether the instance pool will contain preemptible VMs, on-demand VMs, or\npreemptible VMs with a fallback to on-demand VMs if the former is unavailable."
      },
      "GetEventsOrder": {
        "type": "string",
        "enum": [
          "ASC",
          "DESC"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "HardwareAcceleratorType": {
        "type": "string",
        "enum": [
          "GPU_1xA10",
          "GPU_8xH100"
        ],
        "description": "HardwareAcceleratorType: The type of hardware accelerator to use for compute workloads. NOTE:\nThis enum is referenced and is intended to be used by other Databricks services that need to\nspecify hardware accelerator requirements for AI compute workloads."
      },
      "InitScriptExecutionDetailsInitScriptExecutionStatus": {
        "type": "string",
        "enum": [
          "FAILED_EXECUTION",
          "FAILED_FETCH",
          "FUSE_MOUNT_FAILED",
          "NOT_EXECUTED",
          "SKIPPED",
          "SUCCEEDED",
          "UNKNOWN"
        ],
        "description": "Result of attempted script execution"
      },
      "InstancePoolAwsAttributesAvailability": {
        "type": "string",
        "enum": [
          "ON_DEMAND",
          "SPOT"
        ],
        "description": "The set of AWS availability types supported when setting up nodes for a cluster."
      },
      "InstancePoolAzureAttributesAvailability": {
        "type": "string",
        "enum": [
          "ON_DEMAND_AZURE",
          "SPOT_AZURE"
        ],
        "description": "The set of Azure availability types supported when setting up nodes for a cluster."
      },
      "InstancePoolPermissionLevel": {
        "type": "string",
        "enum": [
          "CAN_ATTACH_TO",
          "CAN_MANAGE"
        ],
        "description": "Permission level"
      },
      "InstancePoolState": {
        "type": "string",
        "enum": [
          "ACTIVE",
          "DELETED",
          "STOPPED"
        ],
        "description": "The state of a Cluster. The current allowable state transitions are as follows:\n\n- ``ACTIVE`` -> ``STOPPED`` - ``ACTIVE`` -> ``DELETED`` - ``STOPPED`` -> ``ACTIVE`` -\n``STOPPED`` -> ``DELETED``"
      },
      "Kind": {
        "type": "string",
        "enum": [
          "CLASSIC_PREVIEW"
        ],
        "description": "The kind of compute described by this compute specification.\n\nDepending on `kind`, different validations and default values will be applied.\n\nClusters with `kind = CLASSIC_PREVIEW` support the following fields, whereas clusters with no\nspecified `kind` do not. * [is_single_node](/api/workspace/clusters/create#is_single_node) *\n[use_ml_runtime](/api/workspace/clusters/create#use_ml_runtime) *\n[data_security_mode](/api/workspace/clusters/create#data_security_mode) set to\n`DATA_SECURITY_MODE_AUTO`, `DATA_SECURITY_MODE_DEDICATED`, or `DATA_SECURITY_MODE_STANDARD`\n\nBy using the [simple form], your clusters are automatically using `kind = CLASSIC_PREVIEW`.\n\n[simple form]: https://docs.databricks.com/compute/simple-form.html"
      },
      "Language": {
        "type": "string",
        "enum": [
          "python",
          "r",
          "scala",
          "sql"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "LibraryInstallStatus": {
        "type": "string",
        "enum": [
          "FAILED",
          "INSTALLED",
          "INSTALLING",
          "PENDING",
          "RESOLVING",
          "RESTORED",
          "SKIPPED",
          "UNINSTALL_ON_RESTART"
        ],
        "description": "The status of a library on a specific cluster."
      },
      "ListClustersSortByDirection": {
        "type": "string",
        "enum": [
          "ASC",
          "DESC"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "ListClustersSortByField": {
        "type": "string",
        "enum": [
          "CLUSTER_NAME",
          "DEFAULT"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "ListSortColumn": {
        "type": "string",
        "enum": [
          "POLICY_CREATION_TIME",
          "POLICY_NAME"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "ListSortOrder": {
        "type": "string",
        "enum": [
          "ASC",
          "DESC"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "ResultType": {
        "type": "string",
        "enum": [
          "error",
          "image",
          "images",
          "table",
          "text"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "RuntimeEngine": {
        "type": "string",
        "enum": [
          "NULL",
          "PHOTON",
          "STANDARD"
        ],
        "description": "Create a collection of name/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED = 1\n...     BLUE = 2\n...     GREEN = 3\n\nAccess them by:\n\n- attribute access::\n\n>>> Color.RED\n<Color.RED: 1>\n\n- value lookup:\n\n>>> Color(1)\n<Color.RED: 1>\n\n- name lookup:\n\n>>> Color['RED']\n<Color.RED: 1>\n\nEnumerations can be iterated over, and know how many members they have:\n\n>>> len(Color)\n3\n\n>>> list(Color)\n[<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\nMethods can be added to enumerations, and members can have their own\nattributes -- see the documentation for details."
      },
      "State": {
        "type": "string",
        "enum": [
          "ERROR",
          "PENDING",
          "RESIZING",
          "RESTARTING",
          "RUNNING",
          "TERMINATED",
          "TERMINATING",
          "UNKNOWN"
        ],
        "description": "The state of a Cluster. The current allowable state transitions are as follows:\n\n- `PENDING` -> `RUNNING` - `PENDING` -> `TERMINATING` - `RUNNING` -> `RESIZING` - `RUNNING` ->\n`RESTARTING` - `RUNNING` -> `TERMINATING` - `RESTARTING` -> `RUNNING` - `RESTARTING` ->\n`TERMINATING` - `RESIZING` -> `RUNNING` - `RESIZING` -> `TERMINATING` - `TERMINATING` ->\n`TERMINATED`"
      },
      "TerminationReasonCode": {
        "type": "string",
        "enum": [
          "ABUSE_DETECTED",
          "ACCESS_TOKEN_FAILURE",
          "ALLOCATION_TIMEOUT",
          "ALLOCATION_TIMEOUT_NODE_DAEMON_NOT_READY",
          "ALLOCATION_TIMEOUT_NO_HEALTHY_AND_WARMED_UP_CLUSTERS",
          "ALLOCATION_TIMEOUT_NO_HEALTHY_CLUSTERS",
          "ALLOCATION_TIMEOUT_NO_MATCHED_CLUSTERS",
          "ALLOCATION_TIMEOUT_NO_READY_CLUSTERS",
          "ALLOCATION_TIMEOUT_NO_UNALLOCATED_CLUSTERS",
          "ALLOCATION_TIMEOUT_NO_WARMED_UP_CLUSTERS",
          "ATTACH_PROJECT_FAILURE",
          "AWS_AUTHORIZATION_FAILURE",
          "AWS_INACCESSIBLE_KMS_KEY_FAILURE",
          "AWS_INSTANCE_PROFILE_UPDATE_FAILURE",
          "AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE",
          "AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE",
          "AWS_INVALID_KEY_PAIR",
          "AWS_INVALID_KMS_KEY_STATE",
          "AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE",
          "AWS_REQUEST_LIMIT_EXCEEDED",
          "AWS_RESOURCE_QUOTA_EXCEEDED",
          "AWS_UNSUPPORTED_FAILURE",
          "AZURE_BYOK_KEY_PERMISSION_FAILURE",
          "AZURE_EPHEMERAL_DISK_FAILURE",
          "AZURE_INVALID_DEPLOYMENT_TEMPLATE",
          "AZURE_OPERATION_NOT_ALLOWED_EXCEPTION",
          "AZURE_PACKED_DEPLOYMENT_PARTIAL_FAILURE",
          "AZURE_QUOTA_EXCEEDED_EXCEPTION",
          "AZURE_RESOURCE_MANAGER_THROTTLING",
          "AZURE_RESOURCE_PROVIDER_THROTTLING",
          "AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE",
          "AZURE_VM_EXTENSION_FAILURE",
          "AZURE_VNET_CONFIGURATION_FAILURE",
          "BOOTSTRAP_TIMEOUT",
          "BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION",
          "BOOTSTRAP_TIMEOUT_DUE_TO_MISCONFIG",
          "BUDGET_POLICY_LIMIT_ENFORCEMENT_ACTIVATED",
          "BUDGET_POLICY_RESOLUTION_FAILURE",
          "CLOUD_ACCOUNT_POD_QUOTA_EXCEEDED",
          "CLOUD_ACCOUNT_SETUP_FAILURE",
          "CLOUD_OPERATION_CANCELLED",
          "CLOUD_PROVIDER_DISK_SETUP_FAILURE",
          "CLOUD_PROVIDER_INSTANCE_NOT_LAUNCHED",
          "CLOUD_PROVIDER_LAUNCH_FAILURE",
          "CLOUD_PROVIDER_LAUNCH_FAILURE_DUE_TO_MISCONFIG",
          "CLOUD_PROVIDER_RESOURCE_STOCKOUT",
          "CLOUD_PROVIDER_RESOURCE_STOCKOUT_DUE_TO_MISCONFIG",
          "CLOUD_PROVIDER_SHUTDOWN",
          "CLUSTER_OPERATION_THROTTLED",
          "CLUSTER_OPERATION_TIMEOUT",
          "COMMUNICATION_LOST",
          "CONTAINER_LAUNCH_FAILURE",
          "CONTROL_PLANE_CONNECTION_FAILURE",
          "CONTROL_PLANE_CONNECTION_FAILURE_DUE_TO_MISCONFIG",
          "CONTROL_PLANE_REQUEST_FAILURE",
          "CONTROL_PLANE_REQUEST_FAILURE_DUE_TO_MISCONFIG",
          "DATABASE_CONNECTION_FAILURE",
          "DATA_ACCESS_CONFIG_CHANGED",
          "DBFS_COMPONENT_UNHEALTHY",
          "DBR_IMAGE_RESOLUTION_FAILURE",
          "DISASTER_RECOVERY_REPLICATION",
          "DNS_RESOLUTION_ERROR",
          "DOCKER_CONTAINER_CREATION_EXCEPTION",
          "DOCKER_IMAGE_PULL_FAILURE",
          "DOCKER_IMAGE_TOO_LARGE_FOR_INSTANCE_EXCEPTION",
          "DOCKER_INVALID_OS_EXCEPTION",
          "DRIVER_EVICTION",
          "DRIVER_LAUNCH_TIMEOUT",
          "DRIVER_NODE_UNREACHABLE",
          "DRIVER_OUT_OF_DISK",
          "DRIVER_OUT_OF_MEMORY",
          "DRIVER_POD_CREATION_FAILURE",
          "DRIVER_UNEXPECTED_FAILURE",
          "DRIVER_UNHEALTHY",
          "DRIVER_UNREACHABLE",
          "DRIVER_UNRESPONSIVE",
          "DYNAMIC_SPARK_CONF_SIZE_EXCEEDED",
          "EOS_SPARK_IMAGE",
          "EXECUTION_COMPONENT_UNHEALTHY",
          "EXECUTOR_POD_UNSCHEDULED",
          "GCP_API_RATE_QUOTA_EXCEEDED",
          "GCP_DENIED_BY_ORG_POLICY",
          "GCP_FORBIDDEN",
          "GCP_IAM_TIMEOUT",
          "GCP_INACCESSIBLE_KMS_KEY_FAILURE",
          "GCP_INSUFFICIENT_CAPACITY",
          "GCP_IP_SPACE_EXHAUSTED",
          "GCP_KMS_KEY_PERMISSION_DENIED",
          "GCP_NOT_FOUND",
          "GCP_QUOTA_EXCEEDED",
          "GCP_RESOURCE_QUOTA_EXCEEDED",
          "GCP_SERVICE_ACCOUNT_ACCESS_DENIED",
          "GCP_SERVICE_ACCOUNT_DELETED",
          "GCP_SERVICE_ACCOUNT_NOT_FOUND",
          "GCP_SUBNET_NOT_READY",
          "GCP_TRUSTED_IMAGE_PROJECTS_VIOLATED",
          "GKE_BASED_CLUSTER_TERMINATION",
          "GLOBAL_INIT_SCRIPT_FAILURE",
          "HIVE_METASTORE_PROVISIONING_FAILURE",
          "IMAGE_PULL_PERMISSION_DENIED",
          "INACTIVITY",
          "INIT_CONTAINER_NOT_FINISHED",
          "INIT_SCRIPT_FAILURE",
          "INSTANCE_POOL_CLUSTER_FAILURE",
          "INSTANCE_POOL_MAX_CAPACITY_REACHED",
          "INSTANCE_POOL_NOT_FOUND",
          "INSTANCE_UNREACHABLE",
          "INSTANCE_UNREACHABLE_DUE_TO_MISCONFIG",
          "INTERNAL_CAPACITY_FAILURE",
          "INTERNAL_ERROR",
          "INVALID_ARGUMENT",
          "INVALID_AWS_PARAMETER",
          "INVALID_INSTANCE_PLACEMENT_PROTOCOL",
          "INVALID_SPARK_IMAGE",
          "INVALID_WORKER_IMAGE_FAILURE",
          "IN_PENALTY_BOX",
          "IP_EXHAUSTION_FAILURE",
          "JOB_FINISHED",
          "K8S_ACTIVE_POD_QUOTA_EXCEEDED",
          "K8S_AUTOSCALING_FAILURE",
          "K8S_DBR_CLUSTER_LAUNCH_TIMEOUT",
          "LAZY_ALLOCATION_TIMEOUT",
          "MAINTENANCE_MODE",
          "METASTORE_COMPONENT_UNHEALTHY",
          "MTLS_PORT_CONNECTIVITY_FAILURE",
          "NEPHOS_RESOURCE_MANAGEMENT",
          "NETVISOR_SETUP_TIMEOUT",
          "NETWORK_CHECK_CONTROL_PLANE_FAILURE",
          "NETWORK_CHECK_CONTROL_PLANE_FAILURE_DUE_TO_MISCONFIG",
          "NETWORK_CHECK_DNS_SERVER_FAILURE",
          "NETWORK_CHECK_DNS_SERVER_FAILURE_DUE_TO_MISCONFIG",
          "NETWORK_CHECK_METADATA_ENDPOINT_FAILURE",
          "NETWORK_CHECK_METADATA_ENDPOINT_FAILURE_DUE_TO_MISCONFIG",
          "NETWORK_CHECK_MULTIPLE_COMPONENTS_FAILURE",
          "NETWORK_CHECK_MULTIPLE_COMPONENTS_FAILURE_DUE_TO_MISCONFIG",
          "NETWORK_CHECK_NIC_FAILURE",
          "NETWORK_CHECK_NIC_FAILURE_DUE_TO_MISCONFIG",
          "NETWORK_CHECK_STORAGE_FAILURE",
          "NETWORK_CHECK_STORAGE_FAILURE_DUE_TO_MISCONFIG",
          "NETWORK_CONFIGURATION_FAILURE",
          "NFS_MOUNT_FAILURE",
          "NO_MATCHED_K8S",
          "NO_MATCHED_K8S_TESTING_TAG",
          "NPIP_TUNNEL_SETUP_FAILURE",
          "NPIP_TUNNEL_TOKEN_FAILURE",
          "POD_ASSIGNMENT_FAILURE",
          "POD_SCHEDULING_FAILURE",
          "RATE_LIMITED",
          "REQUEST_REJECTED",
          "REQUEST_THROTTLED",
          "RESOURCE_USAGE_BLOCKED",
          "SECRET_CREATION_FAILURE",
          "SECRET_PERMISSION_DENIED",
          "SECRET_RESOLUTION_ERROR",
          "SECURITY_DAEMON_REGISTRATION_EXCEPTION",
          "SELF_BOOTSTRAP_FAILURE",
          "SERVERLESS_LONG_RUNNING_TERMINATED",
          "SKIPPED_SLOW_NODES",
          "SLOW_IMAGE_DOWNLOAD",
          "SPARK_ERROR",
          "SPARK_IMAGE_DOWNLOAD_FAILURE",
          "SPARK_IMAGE_DOWNLOAD_THROTTLED",
          "SPARK_IMAGE_NOT_FOUND",
          "SPARK_STARTUP_FAILURE",
          "SPOT_INSTANCE_TERMINATION",
          "SSH_BOOTSTRAP_FAILURE",
          "STORAGE_DOWNLOAD_FAILURE",
          "STORAGE_DOWNLOAD_FAILURE_DUE_TO_MISCONFIG",
          "STORAGE_DOWNLOAD_FAILURE_SLOW",
          "STORAGE_DOWNLOAD_FAILURE_THROTTLED",
          "STS_CLIENT_SETUP_FAILURE",
          "SUBNET_EXHAUSTED_FAILURE",
          "TEMPORARILY_UNAVAILABLE",
          "TRIAL_EXPIRED",
          "UNEXPECTED_LAUNCH_FAILURE",
          "UNEXPECTED_POD_RECREATION",
          "UNKNOWN",
          "UNSUPPORTED_INSTANCE_TYPE",
          "UPDATE_INSTANCE_PROFILE_FAILURE",
          "USAGE_POLICY_ENTITLEMENT_DENIED",
          "USER_INITIATED_VM_TERMINATION",
          "USER_REQUEST",
          "WORKER_SETUP_FAILURE",
          "WORKSPACE_CANCELLED_ERROR",
          "WORKSPACE_CONFIGURATION_ERROR",
          "WORKSPACE_UPDATE"
        ],
        "description": "The status code indicating why the cluster was terminated"
      },
      "TerminationReasonType": {
        "type": "string",
        "enum": [
          "CLIENT_ERROR",
          "CLOUD_FAILURE",
          "SERVICE_FAULT",
          "SUCCESS"
        ],
        "description": "type of the termination"
      }
    }
  }
}