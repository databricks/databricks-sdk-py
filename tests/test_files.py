import copy
import hashlib
import io
import json
import logging
import os
import platform
import random
import re
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from enum import Enum
from tempfile import NamedTemporaryFile
from threading import Lock
from typing import Any, Callable, Dict, List, Optional, Type, Union
from urllib.parse import parse_qs, urlparse

import pytest
import requests
import requests_mock
from requests import RequestException

from databricks.sdk import WorkspaceClient
from databricks.sdk.core import Config
from databricks.sdk.environments import Cloud, DatabricksEnvironment
from databricks.sdk.errors.platform import (AlreadyExists, BadRequest,
                                            InternalError, NotImplemented,
                                            PermissionDenied, TooManyRequests)
from databricks.sdk.mixins.files import FallbackToDownloadUsingFilesApi
from databricks.sdk.mixins.files_utils import CreateDownloadUrlResponse
from tests.clock import FakeClock

from .test_files_utils import NonSeekableBuffer, Utils

logger = logging.getLogger(__name__)


class CustomResponse:
    """Custom response allows to override the "default" response generated by the server
    with the "custom" response to simulate failure error code, unexpected response body or
    network error.

    The server is represented by the `processor` parameter in `generate_response()` call.
    """

    def __init__(
        self,
        # If False, default response is always returned.
        # If True, response is defined by the current invocation count
        # with respect to first_invocation / last_invocation / only_invocation
        enabled: bool = True,
        # Custom code to return
        code: Optional[int] = 200,
        # Custom body to return
        body: Optional[str] = None,
        # Custom exception to raise
        exception: Optional[Type[BaseException]] = None,
        # Whether exception should be raised before calling processor()
        # (so changing server state)
        exception_happened_before_processing: bool = False,
        # First invocation (1-based) at which return custom response
        first_invocation: Optional[int] = None,
        # Last invocation (1-based) at which return custom response
        last_invocation: Optional[int] = None,
        # Only invocation (1-based) at which return custom response
        only_invocation: Optional[int] = None,
        # If set, response is delayed by given number of seconds
        delayed_response_seconds: Optional[float] = None,
    ):
        self.enabled = enabled
        self.code = code
        self.body = body
        self.exception = exception
        self.exception_happened_before_processing = exception_happened_before_processing
        self.first_invocation = first_invocation
        self.last_invocation = last_invocation
        self.only_invocation = only_invocation
        self.delayed_response_seconds = delayed_response_seconds

        if self.only_invocation and (self.first_invocation or self.last_invocation):
            raise ValueError("Cannot set both only invocation and first/last invocation")

        if self.exception_happened_before_processing and not self.exception:
            raise ValueError("Exception is not defined")

        self.invocation_count = 0

    def invocation_matches(self) -> bool:
        if not self.enabled:
            return False

        self.invocation_count += 1

        if self.only_invocation:
            return self.invocation_count == self.only_invocation

        if self.first_invocation and self.invocation_count < self.first_invocation:
            return False
        if self.last_invocation and self.invocation_count > self.last_invocation:
            return False
        return True

    def generate_response(
        self, request: requests.Request, processor: Callable[[], list], stream=False
    ) -> requests.Response:
        if self.delayed_response_seconds:
            time.sleep(self.delayed_response_seconds)
        activate_for_current_invocation = self.invocation_matches()

        if activate_for_current_invocation and self.exception and self.exception_happened_before_processing:
            # if network exception is thrown while processing a request, it's not defined
            # if server actually processed the request (and so changed its state)
            raise self.exception

        custom_response = [self.code, self.body or "", {}]

        if activate_for_current_invocation:
            if self.code and 400 <= self.code < 500:
                # if server returns client error, it's not supposed to change its state,
                # so we're not calling processor()
                [code, body_or_stream, headers] = custom_response
            else:
                # we're calling processor() but override its response with the custom one
                processor()
                [code, body_or_stream, headers] = custom_response
        else:
            [code, body_or_stream, headers] = processor()

        if activate_for_current_invocation and self.exception:
            # self.exception_happened_before_processing is False
            raise self.exception

        resp = requests.Response()

        resp.request = request
        resp.status_code = code
        if stream:
            if type(body_or_stream) != bytes:
                resp.raw = io.BytesIO(body_or_stream.encode())
            else:
                resp.raw = io.BytesIO(body_or_stream)
        else:
            resp._content = body_or_stream.encode()

        for key in headers:
            resp.headers[key] = headers[key]

        return resp

    def clear_state(self):
        self.invocation_count = 0


@dataclass
class RequestData:
    offset: int
    end_byte_offset: Optional[int] = None


class DownloadMode(Enum):
    """Download mode for the test case. Used to determine how to download the file."""

    STREAM = "stream"  # download to a stream (existing behavior)
    FILE = "file"  # download to a file (new download_to behavior)


class FilesApiDownloadTestCase:

    def __init__(
        self,
        name: str,
        enable_new_client: bool,
        file_size: int,
        failure_at_absolute_offset: List[int],
        max_recovers_total: Optional[int] = None,
        max_recovers_without_progressing: Optional[int] = None,
        expected_requested_offsets: Optional[List[int]] = None,
        expected_exception: Optional[Type[BaseException]] = None,
        download_mode: DownloadMode = DownloadMode.STREAM,
        overwrite: bool = True,
        use_parallel: bool = False,
        parallelism: Optional[int] = None,
    ):
        self.name = name
        self.enable_new_client = enable_new_client
        self.file_size = file_size
        self.failure_at_absolute_offset = failure_at_absolute_offset
        self.max_recovers_total = max_recovers_total
        self.max_recovers_without_progressing = max_recovers_without_progressing
        self.expected_exception = expected_exception
        self.expected_requested_offsets = expected_requested_offsets
        self.download_mode = download_mode
        self.overwrite = overwrite
        self.use_parallel = use_parallel
        self.parallelism = parallelism

    @staticmethod
    def to_string(test_case: "FilesApiDownloadTestCase") -> str:
        return test_case.name

    def run(self, config: Config, monkeypatch) -> None:
        if self.use_parallel and platform.system() == "Windows":
            pytest.skip("Skipping parallel download tests on Windows")
        config = config.copy()
        config.disable_experimental_files_api_client = not self.enable_new_client
        config.files_ext_client_download_max_total_recovers = self.max_recovers_total
        config.files_ext_client_download_max_total_recovers_without_progressing = self.max_recovers_without_progressing
        config.enable_presigned_download_api = False

        w = WorkspaceClient(config=config)

        session = MockFilesystemSession(self)
        monkeypatch.setattr(w.files._api._api_client, "_session", session)

        if self.download_mode == DownloadMode.STREAM:
            if self.expected_exception is None:
                response = w.files.download("/test").contents
                actual_content = response.read()
                assert len(actual_content) == len(session.content)
                assert actual_content == session.content
            else:
                with pytest.raises(self.expected_exception):
                    response = w.files.download("/test").contents
                    response.read()
        elif self.download_mode == DownloadMode.FILE:  # FILE mode
            with NamedTemporaryFile(delete=False) as temp_file:
                file_path = temp_file.name

            # We can't use 'with' because Windows doesn't allow reopening the file, and `download_to` can open it
            # only after we close it here.
            try:
                if self.expected_exception is None:
                    w.files.download_to(
                        "/test",
                        file_path,
                        overwrite=self.overwrite,
                        use_parallel=self.use_parallel,
                        parallelism=self.parallelism,
                    )

                    # Verify the downloaded file content
                    with open(file_path, "rb") as f:
                        actual_content = f.read()
                    assert len(actual_content) == len(session.content)
                    assert actual_content == session.content
                else:
                    with pytest.raises(self.expected_exception):
                        w.files.download_to(
                            "/test",
                            file_path,
                            overwrite=self.overwrite,
                            use_parallel=self.use_parallel,
                            parallelism=self.parallelism,
                        )
            finally:
                if os.path.exists(file_path):
                    os.remove(file_path)

        received_requests = session.received_requests

        if self.expected_requested_offsets is not None:
            assert len(received_requests) == len(self.expected_requested_offsets)
            for idx, requested_offset in enumerate(self.expected_requested_offsets):
                assert received_requests[idx].offset == requested_offset


class MockFilesystemSession:

    def __init__(self, test_case: FilesApiDownloadTestCase):
        self.test_case: FilesApiDownloadTestCase = test_case
        self.received_requests: List[RequestData] = []
        self.content: bytes = fast_random_bytes(self.test_case.file_size)
        self.failure_pointer = 0
        self.planned_failures = copy.deepcopy(self.test_case.failure_at_absolute_offset)
        self.lock = Lock()
        self.last_modified = "Thu, 28 Nov 2024 16:39:14 GMT"

    # following the signature of Session.request()
    def request(
        self,
        method: str,
        url: str,
        params=None,
        data=None,
        headers=None,
        cookies=None,
        files=None,
        auth=None,
        timeout=None,
        allow_redirects: bool = True,
        proxies=None,
        hooks=None,
        stream: bool = None,
        verify=None,
        cert=None,
        json=None,
    ) -> "MockFilesApiDownloadResponse":

        if method == "GET":
            assert stream is True
            return self._handle_get_file(headers, url)
        elif method == "HEAD":
            return self._handle_head_file(headers, url)
        else:
            raise FallbackToDownloadUsingFilesApi("method must be HEAD or GET")

    def _handle_head_file(self, headers: Dict[str, str], url: str) -> "MockFilesApiDownloadResponse":
        if "If-Unmodified-Since" in headers:
            assert headers["If-Unmodified-Since"] == self.last_modified
        resp = MockFilesApiDownloadResponse(self, 0, None, MockFilesApiDownloadRequest(url))
        resp.content = ""
        return resp

    def _handle_get_file(self, headers: Dict[str, str], url: str) -> "MockFilesApiDownloadResponse":
        offset = 0
        end_byte_offset = None
        if "Range" in headers:
            offset, end_byte_offset = Utils.parse_range_header(headers["Range"], len(self.content))

        logger.debug("Client requested range: %s-%s", offset, end_byte_offset)

        if offset > len(self.content):
            raise Exception("Offset %s exceeds file length %s", offset, len(self.content))
        if end_byte_offset is not None and end_byte_offset >= len(self.content):
            raise Exception("End offset %s exceeds file length %s", end_byte_offset, len(self.content))
        if end_byte_offset is not None and offset > end_byte_offset:
            raise Exception("Begin offset %s exceeds end offset %s", offset, end_byte_offset)

        self.received_requests.append(RequestData(offset))
        return MockFilesApiDownloadResponse(self, offset, end_byte_offset, MockFilesApiDownloadRequest(url))

    def get_content(self, offset: int, end_byte_offset: int) -> bytes:
        with self.lock:
            for failure_after_byte in self.planned_failures:
                if offset <= failure_after_byte < end_byte_offset:
                    self.planned_failures.remove(failure_after_byte)
                    raise RequestException("Fake error")
        return self.content[offset:end_byte_offset]


# required only for correct logging
class MockFilesApiDownloadRequest:

    def __init__(self, url: str):
        self.url = url
        self.method = "GET"
        self.headers = dict()
        self.body = None


class MockFilesApiDownloadResponse:

    def __init__(
        self,
        session: MockFilesystemSession,
        offset: int,
        end_byte_offset: Optional[int],
        request: MockFilesApiDownloadRequest,
    ):
        self.session = session
        self.offset = offset
        self.end_byte_offset = end_byte_offset
        self.request = request
        self.status_code = 200
        self.reason = "OK"
        self.headers = dict()
        self.headers["Content-Length"] = (
            len(session.content) if end_byte_offset is None else end_byte_offset + 1
        ) - offset
        self.headers["Content-Type"] = "application/octet-stream"
        self.headers["Last-Modified"] = session.last_modified
        self.ok = True
        self.url = request.url

    def iter_content(self, chunk_size: int, decode_unicode: bool) -> "MockIterator":
        assert decode_unicode == False
        return MockIterator(self, chunk_size)


class MockIterator:

    def __init__(self, response: MockFilesApiDownloadResponse, chunk_size: int):
        self.response = response
        self.chunk_size = chunk_size
        self.offset = 0

    def __next__(self) -> bytes:
        start_offset = self.response.offset + self.offset

        if self.response.end_byte_offset is not None:
            end_offset = min(
                start_offset + self.chunk_size, self.response.end_byte_offset + 1
            )  # This is an exclusive index that might be out of range
        else:
            end_offset = start_offset + self.chunk_size  # This is an exclusive index that might be out of range

        if start_offset == len(self.response.session.content) or start_offset == end_offset:
            raise StopIteration

        result = self.response.session.get_content(start_offset, end_offset)
        self.offset += len(result)
        return result

    def close(self) -> None:
        pass


class _Constants:
    underlying_chunk_size = 1024 * 1024  # see ticket #832


@pytest.mark.parametrize(
    "test_case",
    [
        FilesApiDownloadTestCase(
            name="Old files client: no failures, file of 5 bytes",
            enable_new_client=False,
            file_size=5,
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="Old files client: no failures, file of 1.5 chunks",
            enable_new_client=False,
            file_size=int(1.5 * _Constants.underlying_chunk_size),
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="Old files client: failure",
            enable_new_client=False,
            file_size=1024,
            failure_at_absolute_offset=[100],
            max_recovers_total=None,  # unlimited but ignored
            max_recovers_without_progressing=None,  # unlimited but ignored
            expected_exception=RequestException,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: no failures, file of 5 bytes",
            enable_new_client=True,
            file_size=5,
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: no failures, file of 1 Kb",
            enable_new_client=True,
            file_size=1024,
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            failure_at_absolute_offset=[],
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: no failures, file of 1.5 parts",
            enable_new_client=True,
            file_size=int(1.5 * _Constants.underlying_chunk_size),
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: no failures, file of 10 parts",
            enable_new_client=True,
            file_size=10 * _Constants.underlying_chunk_size,
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: recovers are disabled, first failure leads to download abort",
            enable_new_client=True,
            file_size=10000,
            failure_at_absolute_offset=[5],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_exception=RequestException,
            expected_requested_offsets=[0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: unlimited recovers allowed",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 5,
            # causes errors on requesting the third chunk
            failure_at_absolute_offset=[
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size + 1,
                _Constants.underlying_chunk_size * 3,
            ],
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            expected_requested_offsets=[
                0,
                0,
                0,
                0,
                _Constants.underlying_chunk_size,
                _Constants.underlying_chunk_size * 3,
            ],
        ),
        FilesApiDownloadTestCase(
            name="New files client: we respect limit on total recovers when progressing",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 10,
            failure_at_absolute_offset=[
                1,
                _Constants.underlying_chunk_size + 1,  # progressing
                _Constants.underlying_chunk_size * 2 + 1,  # progressing
                _Constants.underlying_chunk_size * 3 + 1,  # progressing
            ],
            max_recovers_total=3,
            max_recovers_without_progressing=None,
            expected_exception=RequestException,
            expected_requested_offsets=[
                0,
                0,
                _Constants.underlying_chunk_size * 1,
                _Constants.underlying_chunk_size * 2,
            ],
        ),
        FilesApiDownloadTestCase(
            name="New files client: we respect limit on total recovers when not progressing",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 10,
            failure_at_absolute_offset=[1, 1, 1, 1],
            max_recovers_total=3,
            max_recovers_without_progressing=None,
            expected_exception=RequestException,
            expected_requested_offsets=[0, 0, 0, 0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: we respect limit on non-progressing recovers",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 2,
            failure_at_absolute_offset=[
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size - 1,
            ],
            max_recovers_total=None,
            max_recovers_without_progressing=3,
            expected_exception=RequestException,
            expected_requested_offsets=[0, 0, 0, 0],
        ),
        FilesApiDownloadTestCase(
            name="New files client: non-progressing recovers count is reset when progressing",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 10,
            failure_at_absolute_offset=[
                _Constants.underlying_chunk_size + 1,  # this recover is after progressing
                _Constants.underlying_chunk_size + 1,  # this is not
                _Constants.underlying_chunk_size * 2 + 1,  # this recover is after progressing
                _Constants.underlying_chunk_size * 2 + 1,  # this is not
                _Constants.underlying_chunk_size * 2 + 1,  # this is not, we abort here
            ],
            max_recovers_total=None,
            max_recovers_without_progressing=2,
            expected_exception=RequestException,
            expected_requested_offsets=[
                0,
                _Constants.underlying_chunk_size,
                _Constants.underlying_chunk_size,
                _Constants.underlying_chunk_size * 2,
                _Constants.underlying_chunk_size * 2,
            ],
        ),
        FilesApiDownloadTestCase(
            name="New files client: non-progressing recovers count is reset when progressing - 2",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 10,
            failure_at_absolute_offset=[
                1,
                _Constants.underlying_chunk_size + 1,
                _Constants.underlying_chunk_size * 2 + 1,
                _Constants.underlying_chunk_size * 3 + 1,
            ],
            max_recovers_total=None,
            max_recovers_without_progressing=1,
            expected_requested_offsets=[
                0,
                0,
                _Constants.underlying_chunk_size,
                _Constants.underlying_chunk_size * 2,
                _Constants.underlying_chunk_size * 3,
            ],
        ),
        # Test cases for download_to functionality
        FilesApiDownloadTestCase(
            name="Download to file: New files client, no failures, file of 1 Kb",
            enable_new_client=True,
            file_size=1024,
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            failure_at_absolute_offset=[],
            expected_requested_offsets=[0],
            download_mode=DownloadMode.FILE,
        ),
        FilesApiDownloadTestCase(
            name="Download to file in parallel (1 thread): New files client, no failures, file of 1 Kb",
            enable_new_client=True,
            file_size=1024,
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            failure_at_absolute_offset=[],
            expected_requested_offsets=[0],
            download_mode=DownloadMode.FILE,
            use_parallel=True,
            parallelism=1,
        ),
        FilesApiDownloadTestCase(
            name="Download to file in parallel (4 threads): New files client, no failures, file of 1 Kb",
            enable_new_client=True,
            file_size=1024,
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            failure_at_absolute_offset=[],
            download_mode=DownloadMode.FILE,
            use_parallel=True,
            parallelism=4,
        ),
        FilesApiDownloadTestCase(
            name="Download to file: New files client, no failures, file of 10 parts",
            enable_new_client=True,
            file_size=10 * _Constants.underlying_chunk_size,
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_requested_offsets=[0],
            download_mode=DownloadMode.FILE,
        ),
        FilesApiDownloadTestCase(
            name="Download to file in parallel: New files client, no failures, file of 10 parts",
            enable_new_client=True,
            file_size=10 * _Constants.underlying_chunk_size,
            failure_at_absolute_offset=[],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            download_mode=DownloadMode.FILE,
            use_parallel=True,
        ),
        FilesApiDownloadTestCase(
            name="Download to file: New files client, failure with recovery",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 5,
            failure_at_absolute_offset=[
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size + 1,
                _Constants.underlying_chunk_size * 3,
            ],
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            expected_requested_offsets=[
                0,
                0,
                _Constants.underlying_chunk_size,
                _Constants.underlying_chunk_size * 3,
            ],
            download_mode=DownloadMode.FILE,
        ),
        FilesApiDownloadTestCase(
            name="Download to file in parallel: New files client, failure with recovery",
            enable_new_client=True,
            file_size=_Constants.underlying_chunk_size * 5,
            failure_at_absolute_offset=[
                _Constants.underlying_chunk_size - 1,
                _Constants.underlying_chunk_size + 1,
                _Constants.underlying_chunk_size * 3,
            ],
            max_recovers_total=None,
            max_recovers_without_progressing=None,
            download_mode=DownloadMode.FILE,
            use_parallel=True,
            parallelism=2,
        ),
        FilesApiDownloadTestCase(
            name="Download to file: New files client, failure without recovery",
            enable_new_client=True,
            file_size=10000,
            failure_at_absolute_offset=[5],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_exception=RequestException,
            expected_requested_offsets=[0],
            download_mode=DownloadMode.FILE,
        ),
        FilesApiDownloadTestCase(
            name="Download to file in parallel: New files client, failure without recovery",
            enable_new_client=True,
            file_size=10000,
            failure_at_absolute_offset=[5],
            max_recovers_total=0,
            max_recovers_without_progressing=0,
            expected_exception=RequestException,
            download_mode=DownloadMode.FILE,
            use_parallel=True,
        ),
        FilesApiDownloadTestCase(
            name="Download to file: New files client, overwrite = False",
            enable_new_client=True,
            file_size=100,
            failure_at_absolute_offset=[5],
            expected_exception=IOError,
            download_mode=DownloadMode.FILE,
            overwrite=False,
        ),
        FilesApiDownloadTestCase(
            name="Download to file in parallel: New files client, overwrite = False",
            enable_new_client=True,
            file_size=100,
            failure_at_absolute_offset=[5],
            expected_exception=IOError,
            download_mode=DownloadMode.FILE,
            overwrite=False,
            use_parallel=True,
        ),
    ],
    ids=FilesApiDownloadTestCase.to_string,
)
def test_download_recover(config: Config, test_case: FilesApiDownloadTestCase, monkeypatch):
    test_case.run(config, monkeypatch)


class PresignedUrlDownloadServerState:
    HOSTNAME = "mock-presigned-url.com"

    def __init__(self, file_size: int, last_modified: str):
        self.file_size = file_size
        self.content = fast_random_bytes(file_size)
        self.requested = False
        self.api_used: Optional[str] = None
        self.last_modified = last_modified

    def get_presigned_url(self, path: str):
        return f"https://{PresignedUrlDownloadServerState.HOSTNAME}{path}"

    def get_content(self, request: requests.Request, api_used: str):
        self.requested = True
        self.api_used = api_used
        offset = 0
        end_byte_offset = len(self.content) - 1

        if "Range" in request.headers:
            offset, end_byte_offset = Utils.parse_range_header(request.headers["Range"], len(self.content))

        resp = self.get_header(request)
        resp.status_code = 206 if "Range" in request.headers else 200
        resp._content = self.content[offset : end_byte_offset + 1]
        resp.headers["Content_Length"] = str(len(resp._content))
        return resp

    def get_header(self, request: requests.Request) -> requests.Response:
        resp = requests.Response()
        resp.status_code = 200
        resp._content = b""
        resp.request = request
        resp.headers["Content-Length"] = str(self.file_size)
        resp.headers["Content-Type"] = "application/octet-stream"
        resp.headers["Last-Modified"] = self.last_modified
        return resp


class PresignedUrlDownloadTestCase:
    _FILE_PATH = "/testfile/remote/path"  # A fake path for the remote location of the file to be downloaded
    presigned_url_disabled_response = """
        {
          "error_code": "PERMISSION_DENIED",
          "message": "Presigned URLs API is not enabled",
          "details": [
            {
              "@type": "type.googleapis.com/google.rpc.ErrorInfo",
              "reason": "FILES_API_API_IS_NOT_ENABLED",
              "domain": "filesystem.databricks.com",
              "metadata": {
                "api_name": "Presigned URLs"
              }
            },
            {
              "@type": "type.googleapis.com/google.rpc.RequestInfo",
              "request_id": "9ccb2aa8-621e-42f7-a815-828b70653bf6",
              "serving_data": ""
            }
          ]
        }
    """

    model_serving_presigned_url_internal_error_response = """
        {
          "error_code": "INTERNAL_ERROR",
          "message": "Can't infer requester network zone.",
          "details": [
            {
              "@type": "type.googleapis.com/google.rpc.ErrorInfo",
              "reason": "FILES_API_REQUESTER_NETWORK_ZONE_UNKNOWN",
              "domain": "filesystem.databricks.com"
            },
            {
              "@type": "type.googleapis.com/google.rpc.RequestInfo",
              "request_id": "b2ffb201-ff61-41ad-93e3-50d47654e924",
              "serving_data": ""
            }
          ]
        }
    """

    expired_url_aws_response = (
        '<?xml version="1.0" encoding="utf-8"?><Error><Code>'
        "AuthenticationFailed</Code><Message>Server failed to authenticate "
        "the request. Make sure the value of Authorization header is formed "
        "correctly including the signature.\nRequestId:1abde581-601e-0028-"
        "4a6d-5c3952000000\nTime:2025-01-01T16:54:20.5343181Z</Message"
        "><AuthenticationErrorDetail>Signature not valid in the specified "
        "time frame: Start [Wed, 01 Jan 2025 16:38:41 GMT] - Expiry [Wed, "
        "01 Jan 2025 16:53:45 GMT] - Current [Wed, 01 Jan 2025 16:54:20 "
        "GMT]</AuthenticationErrorDetail></Error>"
    )

    expired_url_azure_response: str = (
        '<?xml version="1.0" encoding="UTF-8"?>\n<Error><Code>AccessDenied'
        "</Code><Message>Request has expired</Message><X-Amz-Expires>"
        "14</X-Amz-Expires><Expires>2025-01-01T17:47:13Z</Expires>"
        "<ServerTime>2025-01-01T17:48:01Z</ServerTime><RequestId>"
        "JY66KDXM4CXBZ7X2</RequestId><HostId>n8Qayqg60rbvut9P7pk0</HostId>"
        "</Error>"
    )

    def __init__(
        self,
        name: str,
        file_size: int,
        custom_response_get_file_status_api: Optional[CustomResponse] = CustomResponse(enabled=False),
        custom_response_create_presigned_url: Optional[CustomResponse] = CustomResponse(enabled=False),
        custom_response_download_from_url: Optional[CustomResponse] = CustomResponse(enabled=False),
        custom_response_download_from_files_api: Optional[CustomResponse] = CustomResponse(enabled=False),
        download_mode: Optional[Union[DownloadMode, List[DownloadMode]]] = None,
        overwrite: bool = True,
        use_parallel: Optional[Union[bool, List[bool]]] = None,
        parallelism: Optional[int] = None,
        parallel_download_min_file_size: Optional[int] = None,
        parallel_upload_part_size: Optional[int] = None,
        expected_exception_type: Optional[Type[BaseException]] = None,
        expected_download_api: Optional[str] = None,
    ):
        # Metadata
        self.name = name
        self.file_size = file_size
        self.last_modified = "Thu, 28 Nov 2024 16:39:14 GMT"

        # Function stubs to customize responses for various API calls
        self.custom_response_get_file_status_api = custom_response_get_file_status_api
        self.custom_response_create_presigned_url = custom_response_create_presigned_url
        self.custom_response_download_from_url = custom_response_download_from_url
        self.custom_response_download_from_files_api = custom_response_download_from_files_api

        # Parameters
        #  - By default, we would like to test both download modes for every test case.
        if download_mode is None:
            self.download_mode = [DownloadMode.STREAM, DownloadMode.FILE]
        elif not isinstance(download_mode, list):
            self.download_mode = [download_mode]
        else:
            self.download_mode = download_mode
        self.overwrite = overwrite
        #  - By default, we would like to test both parallel and non-parallel downloads for every test case.
        if use_parallel is None:
            self.use_parallel = [False, True]
        elif not isinstance(use_parallel, list):
            self.use_parallel = [use_parallel]
        else:
            self.use_parallel = use_parallel
        self.parallelism = parallelism

        # Expected results
        self.expected_exception_type = expected_exception_type
        self.expected_download_api = expected_download_api

        # Config overrides
        self.parallel_download_min_file_size = parallel_download_min_file_size
        self.parallel_upload_part_size = parallel_upload_part_size

    def _clear_state(self):
        self.custom_response_get_file_status_api.clear_state()
        self.custom_response_create_presigned_url.clear_state()
        self.custom_response_download_from_files_api.clear_state()
        self.custom_response_download_from_url.clear_state()

    def __str__(self) -> str:
        return self.name

    @staticmethod
    def to_string(test_case) -> str:
        return str(test_case)

    def match_request_to_response(
        self, request: requests.Request, server_state: PresignedUrlDownloadServerState
    ) -> Optional[requests.Response]:
        """Match the request to the server state and return a mock response."""
        request_url = urlparse(request.url)
        request_query = parse_qs(request_url.query)

        # Create Download URL request
        if (
            request_url.hostname == "localhost"
            and request.method == "POST"
            and request_url.path == "/api/2.0/fs/create-download-url"
        ):
            assert "path" in request_query, "Expected 'path' in query parameters"
            file_path = request_query.get("path")[0]

            def processor() -> list:
                url = server_state.get_presigned_url(file_path)
                return [200, json.dumps({"url": url, "headers": {}}), {}]

            return self.custom_response_create_presigned_url.generate_response(request, processor)

        # Get files status request
        elif (
            request_url.hostname == "localhost"
            and request.method == "HEAD"
            and request_url.path == f"/api/2.0/fs/files{self._FILE_PATH}"
        ):
            # HEAD request to check if file exists
            def processor() -> list:
                resp = server_state.get_header(request)
                return [resp.status_code, resp._content, resp.headers]

            return self.custom_response_get_file_status_api.generate_response(request, processor, stream=True)

        # Direct Files API download request
        elif (
            request_url.hostname == "localhost"
            and request.method == "GET"
            and request_url.path == f"/api/2.0/fs/files{self._FILE_PATH}"
        ):

            def processor() -> list:
                resp = server_state.get_content(request, api_used="files_api")
                return [resp.status_code, resp._content, resp.headers]

            return self.custom_response_download_from_files_api.generate_response(request, processor, stream=True)

        # Download from Presigned URL request
        elif request_url.hostname == PresignedUrlDownloadServerState.HOSTNAME and request.method == "GET":
            logger.debug(f"headers = {request.headers}")

            def processor() -> list:
                resp = server_state.get_content(request, api_used="presigned_url")
                return [resp.status_code, resp._content, resp.headers]

            return self.custom_response_download_from_url.generate_response(request, processor, stream=True)

        else:
            raise RuntimeError("Unexpected request " + str(request))

    def run_one_case(self, config: Config, monkeypatch, download_mode: DownloadMode, use_parallel: bool) -> None:
        if use_parallel and platform.system() == "Windows":
            logger.debug("Parallel download is not supported on Windows. Falling back to sequential download.")
            return
        config = config.copy()
        config.enable_presigned_download_api = True
        config._clock = FakeClock()
        if self.parallel_download_min_file_size is not None:
            config.files_ext_parallel_download_min_file_size = self.parallel_download_min_file_size
        if self.parallel_upload_part_size is not None:
            config.files_ext_parallel_upload_part_size = self.parallel_upload_part_size

        w = WorkspaceClient(config=config)
        state = PresignedUrlDownloadServerState(self.file_size, self.last_modified)

        with requests_mock.Mocker() as session_mock:

            def custom_matcher(request: requests.Request) -> Optional[requests.Response]:
                """Custom matcher to handle requests and return mock responses."""
                return self.match_request_to_response(request, state)

            session_mock.add_matcher(custom_matcher)

            if download_mode == DownloadMode.STREAM:
                if self.expected_exception_type is not None:
                    with pytest.raises(self.expected_exception_type):
                        w.files.download(PresignedUrlDownloadTestCase._FILE_PATH)
                else:
                    download_resp = w.files.download(PresignedUrlDownloadTestCase._FILE_PATH)
                    assert download_resp.content_length == self.file_size
                    assert download_resp.contents.read() == state.content
                    if self.expected_download_api is not None:
                        assert state.api_used == self.expected_download_api
            elif download_mode == DownloadMode.FILE:
                with NamedTemporaryFile(delete=False) as temp_file:
                    file_path = temp_file.name

                # We can't use 'with' because Windows doesn't allow reopening the file, and `download_to` can open it
                # only after we close it here.
                try:
                    if self.expected_exception_type is not None:
                        with pytest.raises(self.expected_exception_type):
                            w.files.download_to(
                                PresignedUrlDownloadTestCase._FILE_PATH,
                                file_path,
                                overwrite=self.overwrite,
                                use_parallel=use_parallel,
                                parallelism=self.parallelism,
                            )
                    else:
                        w.files.download_to(
                            PresignedUrlDownloadTestCase._FILE_PATH,
                            file_path,
                            overwrite=self.overwrite,
                            use_parallel=use_parallel,
                            parallelism=self.parallelism,
                        )
                        with open(file_path, "rb") as f:
                            actual_content = f.read()
                        assert len(actual_content) == len(state.content)
                        assert actual_content == state.content
                        if self.expected_download_api is not None:
                            assert state.api_used == self.expected_download_api
                finally:
                    if os.path.exists(file_path):
                        os.remove(file_path)
            else:
                raise ValueError("Unexpected download mode")

    def run(self, config: Config, monkeypatch) -> None:
        # Run all combinations of download modes and parallelism settings
        for download_mode in self.download_mode:
            for use_parallel in self.use_parallel:
                logger.info(f"Downloading {download_mode.name} with parallelism={use_parallel}")
                self.run_one_case(config, monkeypatch, download_mode, use_parallel)
                self._clear_state()


@pytest.mark.parametrize(
    "test_case",
    [
        # Happy cases
        PresignedUrlDownloadTestCase(
            name="Presigned URL download succeeds",
            file_size=100 * 1024 * 1024,
        ),
        PresignedUrlDownloadTestCase(
            name="Presigned URL download to File succeeds",
            file_size=100 * 1024 * 1024,
            download_mode=DownloadMode.FILE,
        ),
        PresignedUrlDownloadTestCase(
            name="Presigned URL download to File in parallel succeeds",
            file_size=100 * 1024 * 1024,
            download_mode=DownloadMode.FILE,
            use_parallel=True,
            parallelism=2,
        ),
        # Sad cases
        PresignedUrlDownloadTestCase(
            name="Presigned URL download fails with 403",
            file_size=100 * 1024 * 1024,
            custom_response_create_presigned_url=CustomResponse(code=403, only_invocation=1),
            expected_download_api="files_api",
        ),
        PresignedUrlDownloadTestCase(
            name="Presigned URL download fails with 500 when creating presigned URL",
            file_size=100 * 1024 * 1024,
            custom_response_create_presigned_url=CustomResponse(code=500, only_invocation=1),
            expected_download_api="files_api",
        ),
        PresignedUrlDownloadTestCase(
            name="Presigned URL download fails with 500 when downloading from URL, fallback to Files API",
            file_size=100 * 1024 * 1024,
            expected_download_api="files_api",  # 500 is no longer retried, falls back immediately
            custom_response_download_from_url=CustomResponse(code=500),
        ),
        PresignedUrlDownloadTestCase(
            name="Intermittent error fails after retry: Presigned URL expires with 403 when downloading from URL",
            file_size=100 * 1024 * 1024,
            expected_exception_type=ValueError,
            custom_response_download_from_url=CustomResponse(
                code=403, body=PresignedUrlDownloadTestCase.expired_url_aws_response
            ),
            expected_download_api="presigned_url",
            use_parallel=True,
            download_mode=DownloadMode.FILE,
        ),
        # Recoverable errors
        PresignedUrlDownloadTestCase(
            name="Intermittent error should succeed after retry: Presigned URL download fails with 502 when downloading from URL",
            file_size=100 * 1024 * 1024,
            custom_response_download_from_url=CustomResponse(code=502, only_invocation=1),
        ),
        PresignedUrlDownloadTestCase(
            name="Intermittent error should succeed after retry: Presigned URL expires with 403 when downloading from URL",
            file_size=100 * 1024 * 1024,
            custom_response_download_from_url=CustomResponse(
                code=403,
                first_invocation=2,
                last_invocation=3,
                body=PresignedUrlDownloadTestCase.expired_url_aws_response,
            ),
        ),
        # Test fallback to Files API
        PresignedUrlDownloadTestCase(
            name="Presigned URL is disabled, should fallback to Files API",
            file_size=100 * 1024 * 1024,
            expected_download_api="files_api",
            custom_response_create_presigned_url=CustomResponse(
                code=403, only_invocation=1, body=PresignedUrlDownloadTestCase.presigned_url_disabled_response
            ),
        ),
        PresignedUrlDownloadTestCase(
            name="Presigned URL is not issued because NetworkZone is not populated, should fallback to Files API",
            file_size=100 * 1024 * 1024,
            expected_download_api="files_api",
            custom_response_create_presigned_url=CustomResponse(
                code=500,
                only_invocation=1,
                body=PresignedUrlDownloadTestCase.model_serving_presigned_url_internal_error_response,
            ),
        ),
        PresignedUrlDownloadTestCase(
            name="Presigned URL fails with 403 when downloading, should fallback to Files API",
            file_size=100 * 1024 * 1024,
            expected_download_api="files_api",
            custom_response_download_from_url=CustomResponse(code=403, only_invocation=1),
        ),
    ],
    ids=PresignedUrlDownloadTestCase.to_string,
)
def test_presigned_url_download(config: Config, test_case: PresignedUrlDownloadTestCase, monkeypatch) -> None:
    test_case.run(config, monkeypatch)


class FileContent:

    def __init__(self, length: int, checksum: str):
        self._length = length
        self.checksum = checksum

    @classmethod
    def from_bytes(cls, data: bytes) -> "FileContent":
        sha256 = hashlib.sha256()
        sha256.update(data)
        return FileContent(len(data), sha256.hexdigest())

    def __repr__(self) -> str:
        return f"Length: {self._length}, checksum: {self.checksum}"

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, FileContent):
            return NotImplemented
        return self._length == other._length and self.checksum == other.checksum


class MultipartUploadServerState:
    """This server state is updated on multipart upload (AWS, Azure)"""

    upload_part_url_prefix = "https://cloud_provider.com/upload-part/"
    abort_upload_url_prefix = "https://cloud_provider.com/abort-upload/"

    def __init__(self, expected_part_size: Optional[int] = None):
        self.issued_multipart_urls = {}  # part_number -> expiration_time
        self.uploaded_parts = {}  # part_number -> [part file path, etag]
        self.session_token = "token-" + MultipartUploadServerState.randomstr()
        self.file_content = None
        self.issued_abort_url_expire_time = None
        self.aborted = False
        self.expected_part_size = expected_part_size
        self.global_lock = Lock()

    def create_upload_part_url(self, path: str, part_number: int, expire_time: datetime) -> str:
        assert not self.aborted
        # client may have requested a URL for the same part if retrying on network error
        self.issued_multipart_urls[part_number] = expire_time
        return f"{self.upload_part_url_prefix}{path}/{part_number}"

    def create_abort_url(self, path: str, expire_time: datetime) -> str:
        assert not self.aborted
        self.issued_abort_url_expire_time = expire_time
        return f"{self.abort_upload_url_prefix}{path}"

    def save_part(self, part_number: int, part_content: bytes, etag: str) -> None:
        assert not self.aborted
        assert len(part_content) > 0
        if self.expected_part_size is not None:
            assert len(part_content) <= self.expected_part_size

        logger.info(f"Saving part {part_number} of size {len(part_content)}")

        # part might already have been uploaded
        with self.global_lock:
            if part_number not in self.uploaded_parts:
                with NamedTemporaryFile(mode="wb", delete=False) as f:
                    part_file = f.name
                self.uploaded_parts[part_number] = [part_file, etag, Lock()]
            existing_part = self.uploaded_parts[part_number]
        with existing_part[2]:  # lock per part
            part_file = existing_part[0]
            with open(part_file, "wb") as f:  # overwrite
                f.write(part_content)
            existing_part[1] = etag  # update etag

    def cleanup(self) -> None:
        for [file, _, _] in self.uploaded_parts.values():
            os.remove(file)

    def get_file_content(self) -> Optional[FileContent]:
        if self.aborted:
            assert not self.file_content, "File content should not be set if upload was aborted"

        # content may be None even for a non-aborted upload,
        # in case single-shot upload was used due to small stream size.
        return self.file_content

    def upload_complete(self, etags: dict) -> None:
        assert not self.aborted
        # validate etags
        expected_etags = {}
        with self.global_lock:
            for part_number in self.uploaded_parts.keys():
                expected_etags[part_number] = self.uploaded_parts[part_number][1]
            assert etags == expected_etags

            size = 0
            sha256 = hashlib.sha256()

            sorted_parts = sorted(self.uploaded_parts.keys())
            for part_number in sorted_parts:
                part_path = self.uploaded_parts[part_number][0]
                size += os.path.getsize(part_path)
                with open(part_path, "rb") as f:
                    part_content = f.read()
                    sha256.update(part_content)

        self.file_content = FileContent(size, sha256.hexdigest())

    def abort_upload(self) -> None:
        self.aborted = True

    @staticmethod
    def randomstr() -> str:
        return f"{random.randrange(10000)}-{int(time.time())}"


class SingleShotUploadServerState:
    """This server state is updated on single-shot upload"""

    def __init__(self):
        self.file_content: Optional[FileContent] = None

    def cleanup(self) -> None:
        pass

    def upload(self, content: bytes) -> None:
        self.file_content = FileContent.from_bytes(content)

    def get_file_content(self) -> Optional[FileContent]:
        return self.file_content


class UploadTestCase:
    """Base class for upload test cases"""

    def __init__(
        self,
        name: str,
        stream_size: int,
        cloud: Cloud,
        overwrite: bool,
        source_type: List["UploadSourceType"],
        use_parallel: List[bool],
        parallelism: Optional[int],
        multipart_upload_min_stream_size: int,
        multipart_upload_part_size: Optional[int],
        sdk_retry_timeout_seconds: Optional[int],
        multipart_upload_max_retries: Optional[int],
        custom_response_on_single_shot_upload: CustomResponse,
        # exception which is expected to be thrown (so upload is expected to have failed)
        expected_exception_type: Optional[Type[BaseException]],
        # Whether abort is expected to be called for multipart/resumable upload, set to None if we don't care.
        expected_multipart_upload_aborted: Optional[bool],
        expected_single_shot_upload: bool,
    ):
        self.name = name
        self.stream_size = stream_size
        self.cloud = cloud
        self.overwrite = overwrite
        self.source_type = source_type
        self.use_parallel = use_parallel
        self.parallelism = parallelism
        self.multipart_upload_min_stream_size = multipart_upload_min_stream_size
        self.multipart_upload_part_size = multipart_upload_part_size
        self.sdk_retry_timeout_seconds = sdk_retry_timeout_seconds
        self.multipart_upload_max_retries = multipart_upload_max_retries
        self.custom_response_on_single_shot_upload = custom_response_on_single_shot_upload
        self.expected_exception_type = expected_exception_type
        self.expected_multipart_upload_aborted: Optional[bool] = expected_multipart_upload_aborted
        self.expected_single_shot_upload = expected_single_shot_upload

        self.path = "/test.txt"
        self.created_temp_files = []

    def customize_config(self, config: Config) -> None:
        pass

    def create_multipart_upload_server_state(self) -> Union[MultipartUploadServerState, "ResumableUploadServerState"]:
        raise NotImplementedError

    def clear_state(self) -> None:
        for file_path in self.created_temp_files:
            try:
                os.remove(file_path)
            except OSError:
                logger.warning("Failed to remove temp file: %s", file_path)
        self.created_temp_files = []

    def get_upload_file(
        self, content: bytes, source_type: "UploadSourceType"
    ) -> Union[str, io.BytesIO, NonSeekableBuffer]:
        """Returns a file or stream to upload based on the source type."""
        if source_type == UploadSourceType.FILE:
            with NamedTemporaryFile(mode="wb", delete=False) as f:
                f.write(content)
                file_path = f.name
            self.created_temp_files.append(file_path)
            return file_path
        elif source_type == UploadSourceType.SEEKABLE_STREAM:
            return io.BytesIO(content)
        elif source_type == UploadSourceType.NONSEEKABLE_STREAM:
            return NonSeekableBuffer(content)
        else:
            raise ValueError(f"Unknown source type: {source_type}")

    def match_request_to_response(
        self, request: requests.Request, server_state: Union[MultipartUploadServerState, "ResumableUploadServerState"]
    ) -> Optional[requests.Response]:
        raise NotImplementedError

    def run(self, config: Config) -> None:
        for source_type in self.source_type:
            for use_parallel in self.use_parallel:
                self.run_one_case(config, use_parallel, source_type)

    def run_one_case(self, config: Config, use_parallel: bool, source_type: "UploadSourceType") -> None:

        logger.debug(f"Running test case: {self.name}, source_type={source_type}, use_parallel={use_parallel}")
        config = config.copy()
        config._clock = FakeClock()

        if self.cloud:
            config.databricks_environment = DatabricksEnvironment(self.cloud, "")

        if self.sdk_retry_timeout_seconds:
            config.retry_timeout_seconds = self.sdk_retry_timeout_seconds
        if self.multipart_upload_part_size:
            config.multipart_upload_part_size = self.multipart_upload_part_size
        if self.multipart_upload_max_retries:
            config.files_ext_multipart_upload_max_retries = self.multipart_upload_max_retries

        config.files_ext_multipart_upload_min_stream_size = self.multipart_upload_min_stream_size

        pat_token = "some_pat_token"
        config._header_factory = lambda: {"Authorization": f"Bearer {pat_token}"}

        self.customize_config(config)

        multipart_server_state = self.create_multipart_upload_server_state()
        single_shot_server_state = SingleShotUploadServerState()

        file_content = fast_random_bytes(self.stream_size)
        content_or_source = self.get_upload_file(file_content, source_type)
        w = WorkspaceClient(config=config)

        try:
            with requests_mock.Mocker() as session_mock:

                def custom_matcher(request: requests.Request) -> Optional[requests.Response]:
                    # first, try to match single-shot upload
                    parsed_url = urlparse(request.url)
                    if (
                        parsed_url.hostname == "localhost"
                        and parsed_url.path == f"/api/2.0/fs/files{self.path}"
                        and request.method == "PUT"
                        and not parsed_url.params
                    ):

                        def processor() -> list:
                            body = request.body.read()
                            single_shot_server_state.upload(body)
                            return [200, "", {}]

                        return self.custom_response_on_single_shot_upload.generate_response(request, processor)

                    # otherwise fall back to specific matcher from the test case
                    return self.match_request_to_response(request, multipart_server_state)

                session_mock.add_matcher(matcher=custom_matcher)

                def upload() -> None:
                    if source_type == UploadSourceType.FILE:
                        w.files.upload_from(
                            file_path=self.path,
                            source_path=content_or_source,
                            overwrite=self.overwrite,
                            part_size=self.multipart_upload_part_size,
                            use_parallel=use_parallel,
                            parallelism=self.parallelism,
                        )
                    else:
                        w.files.upload(
                            file_path=self.path,
                            contents=content_or_source,
                            overwrite=self.overwrite,
                            part_size=self.multipart_upload_part_size,
                            use_parallel=use_parallel,
                            parallelism=self.parallelism,
                        )

                if self.expected_exception_type is not None:
                    with pytest.raises(self.expected_exception_type):
                        upload()
                    assert (
                        not single_shot_server_state.get_file_content()
                    ), "Single-shot upload should not have succeeded"
                    assert not multipart_server_state.get_file_content(), "Multipart upload should not have succeeded"
                else:
                    upload()
                    if self.expected_single_shot_upload:
                        assert single_shot_server_state.get_file_content() == FileContent.from_bytes(
                            file_content
                        ), "Single-shot upload should have succeeded"
                        assert (
                            not multipart_server_state.get_file_content()
                        ), "Multipart upload should not have succeeded"
                    else:
                        assert multipart_server_state.get_file_content() == FileContent.from_bytes(
                            file_content
                        ), "Multipart upload should have succeeded"
                        assert (
                            not single_shot_server_state.get_file_content()
                        ), "Single-shot upload should not have succeeded"

            assert (
                self.expected_multipart_upload_aborted is None
                or multipart_server_state.aborted == self.expected_multipart_upload_aborted
            ), "Multipart upload aborted state mismatch"

        finally:
            multipart_server_state.cleanup()
            self.clear_state()

    @staticmethod
    def is_auth_header_present(r: requests.Request) -> bool:
        return r.headers.get("Authorization") is not None


class UploadSourceType(Enum):
    """Source type for the upload. Used to determine how to upload the file."""

    FILE = "file"  # upload from a file on disk
    SEEKABLE_STREAM = "seekable_stream"  # upload from a seekable stream (e.g. BytesIO)
    NONSEEKABLE_STREAM = "nonseekable_stream"  # upload from a non-seekable stream (e.g. network stream)


class MultipartUploadTestCase(UploadTestCase):
    """Test case for multipart upload of a file. Multipart uploads are used on AWS and Azure.

    Multipart upload via presigned URLs involves multiple HTTP requests:
    - initiating upload (call to Databricks Files API)
    - requesting upload part URLs (calls to Databricks Files API)
    - uploading data in parts (calls to cloud storage provider or Databricks storage proxy)
    - completing the upload (call to Databricks Files API)
    - requesting abort upload URL (call to Databricks Files API)
    - aborting the upload (call to cloud storage provider or Databricks storage proxy)

    Test case uses requests-mock library to mock all these requests. Within a test, mocks use
    shared server state that tracks the upload. Mocks generate the "default" (successful) response.

    Response of each call can be modified by parameterising a respective `CustomResponse` object.
    """

    expired_url_aws_response = (
        '<?xml version="1.0" encoding="utf-8"?><Error><Code>'
        "AuthenticationFailed</Code><Message>Server failed to authenticate "
        "the request. Make sure the value of Authorization header is formed "
        "correctly including the signature.\nRequestId:1abde581-601e-0028-"
        "4a6d-5c3952000000\nTime:2025-01-01T16:54:20.5343181Z</Message"
        "><AuthenticationErrorDetail>Signature not valid in the specified "
        "time frame: Start [Wed, 01 Jan 2025 16:38:41 GMT] - Expiry [Wed, "
        "01 Jan 2025 16:53:45 GMT] - Current [Wed, 01 Jan 2025 16:54:20 "
        "GMT]</AuthenticationErrorDetail></Error>"
    )

    expired_url_azure_response: str = (
        '<?xml version="1.0" encoding="UTF-8"?>\n<Error><Code>AccessDenied'
        "</Code><Message>Request has expired</Message><X-Amz-Expires>"
        "14</X-Amz-Expires><Expires>2025-01-01T17:47:13Z</Expires>"
        "<ServerTime>2025-01-01T17:48:01Z</ServerTime><RequestId>"
        "JY66KDXM4CXBZ7X2</RequestId><HostId>n8Qayqg60rbvut9P7pk0</HostId>"
        "</Error>"
    )

    presigned_url_disabled_response = """
        {
          "error_code": "PERMISSION_DENIED",
          "message": "Presigned URLs API is not enabled",
          "details": [
            {
              "@type": "type.googleapis.com/google.rpc.ErrorInfo",
              "reason": "FILES_API_API_IS_NOT_ENABLED",
              "domain": "filesystem.databricks.com",
              "metadata": {
                "api_name": "Presigned URLs"
              }
            },
            {
              "@type": "type.googleapis.com/google.rpc.RequestInfo",
              "request_id": "9ccb2aa8-621e-42f7-a815-828b70653bf6",
              "serving_data": ""
            }
          ]
        }
    """

    model_serving_presigned_url_internal_error_response = """
        {
          "error_code": "INTERNAL_ERROR",
          "message": "Can't infer requester network zone.",
          "details": [
            {
              "@type": "type.googleapis.com/google.rpc.ErrorInfo",
              "reason": "FILES_API_REQUESTER_NETWORK_ZONE_UNKNOWN",
              "domain": "filesystem.databricks.com"
            },
            {
              "@type": "type.googleapis.com/google.rpc.RequestInfo",
              "request_id": "b2ffb201-ff61-41ad-93e3-50d47654e924",
              "serving_data": ""
            }
          ]
        }
    """

    def __init__(
        self,
        name: str,
        content_size: int,  # size of uploaded file or, technically, stream
        cloud: Cloud = Cloud.AWS,
        overwrite: bool = True,
        multipart_upload_min_stream_size: int = 0,  # disable single-shot uploads by default
        source_type: Optional[List[UploadSourceType]] = None,
        use_parallel: Optional[List[bool]] = None,
        parallelism: Optional[int] = None,
        multipart_upload_part_size: Optional[int] = None,
        sdk_retry_timeout_seconds: Optional[int] = None,
        multipart_upload_max_retries: Optional[int] = None,
        multipart_upload_batch_url_count: Optional[int] = None,
        custom_response_on_single_shot_upload: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_initiate: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_create_multipart_url: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_upload: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_complete: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_create_abort_url: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_abort: CustomResponse = CustomResponse(enabled=False),
        # exception which is expected to be thrown (so upload is expected to have failed)
        expected_exception_type: Optional[Type[BaseException]] = None,
        # if abort is expected to be called
        # expected part size
        expected_part_size: Optional[int] = None,
        expected_multipart_upload_aborted: Optional[bool] = False,
        expected_single_shot_upload: bool = False,
    ):
        super().__init__(
            name,
            content_size,
            cloud,
            overwrite,
            source_type
            or [UploadSourceType.FILE, UploadSourceType.SEEKABLE_STREAM, UploadSourceType.NONSEEKABLE_STREAM],
            use_parallel or [False, True],
            parallelism,
            multipart_upload_min_stream_size,
            multipart_upload_part_size,
            sdk_retry_timeout_seconds,
            multipart_upload_max_retries,
            custom_response_on_single_shot_upload,
            expected_exception_type,
            expected_multipart_upload_aborted,
            expected_single_shot_upload,
        )

        self.multipart_upload_batch_url_count = multipart_upload_batch_url_count
        self.custom_response_on_initiate = copy.deepcopy(custom_response_on_initiate)
        self.custom_response_on_create_multipart_url = copy.deepcopy(custom_response_on_create_multipart_url)
        self.custom_response_on_upload = copy.deepcopy(custom_response_on_upload)
        self.custom_response_on_complete = copy.deepcopy(custom_response_on_complete)
        self.custom_response_on_create_abort_url = copy.deepcopy(custom_response_on_create_abort_url)
        self.custom_response_on_abort = copy.deepcopy(custom_response_on_abort)
        self.expected_exception_type = expected_exception_type
        self.expected_part_size = expected_part_size

    def customize_config(self, config: Config) -> None:
        if self.multipart_upload_batch_url_count:
            config.files_ext_multipart_upload_batch_url_count = self.multipart_upload_batch_url_count

    def create_multipart_upload_server_state(self) -> MultipartUploadServerState:
        return MultipartUploadServerState(self.expected_part_size)

    def clear_state(self) -> None:
        super().clear_state()
        self.custom_response_on_initiate.clear_state()
        self.custom_response_on_create_multipart_url.clear_state()
        self.custom_response_on_upload.clear_state()
        self.custom_response_on_complete.clear_state()
        self.custom_response_on_create_abort_url.clear_state()
        self.custom_response_on_abort.clear_state()

    def match_request_to_response(
        self, request: requests.Request, server_state: MultipartUploadServerState
    ) -> Optional[requests.Response]:
        request_url = urlparse(request.url)
        request_query = parse_qs(request_url.query)

        # initial request
        if (
            request_url.hostname == "localhost"
            and request_url.path == f"/api/2.0/fs/files{self.path}"
            and request_query.get("action") == ["initiate-upload"]
            and request.method == "POST"
        ):

            assert UploadTestCase.is_auth_header_present(request)
            assert request.text is None

            def processor() -> list:
                response_json = {"multipart_upload": {"session_token": server_state.session_token}}
                return [200, json.dumps(response_json), {}]

            return self.custom_response_on_initiate.generate_response(request, processor)

        # multipart upload, create upload part URLs
        elif (
            request_url.hostname == "localhost"
            and request_url.path == "/api/2.0/fs/create-upload-part-urls"
            and request.method == "POST"
        ):

            assert UploadTestCase.is_auth_header_present(request)

            request_json = request.json()
            assert request_json.keys() == {"count", "expire_time", "path", "session_token", "start_part_number"}
            assert request_json["path"] == self.path
            assert request_json["session_token"] == server_state.session_token

            start_part_number = int(request_json["start_part_number"])
            count = int(request_json["count"])
            assert count >= 1

            expire_time = MultipartUploadTestCase.parse_and_validate_expire_time(request_json["expire_time"])

            def processor() -> list:
                response_nodes = []
                for part_number in range(start_part_number, start_part_number + count):
                    upload_part_url = server_state.create_upload_part_url(self.path, part_number, expire_time)
                    response_nodes.append(
                        {
                            "part_number": part_number,
                            "url": upload_part_url,
                            "headers": [{"name": "name1", "value": "value1"}],
                        }
                    )

                response_json = {"upload_part_urls": response_nodes}
                return [200, json.dumps(response_json), {}]

            return self.custom_response_on_create_multipart_url.generate_response(request, processor)

        # multipart upload, uploading part
        elif request.url.startswith(MultipartUploadServerState.upload_part_url_prefix) and request.method == "PUT":

            assert not UploadTestCase.is_auth_header_present(request)

            url_path = request.url[len(MultipartUploadServerState.upload_part_url_prefix) :]
            part_num = url_path.split("/")[-1]
            assert url_path[: -len(part_num) - 1] == self.path

            def processor() -> list:
                body = request.body.read()
                etag = "etag-" + MultipartUploadServerState.randomstr()
                server_state.save_part(int(part_num), body, etag)
                return [200, "", {"ETag": etag}]

            return self.custom_response_on_upload.generate_response(request, processor)

        # multipart upload, completion
        elif (
            request_url.hostname == "localhost"
            and request_url.path == f"/api/2.0/fs/files{self.path}"
            and request_query.get("action") == ["complete-upload"]
            and request_query.get("upload_type") == ["multipart"]
            and request.method == "POST"
        ):

            assert UploadTestCase.is_auth_header_present(request)
            assert [server_state.session_token] == request_query.get("session_token")

            def processor() -> list:
                request_json = request.json()
                etags = {}

                assert len(request_json["parts"]) > 0
                for part in request_json["parts"]:
                    etags[part["part_number"]] = part["etag"]

                server_state.upload_complete(etags)
                return [200, "", {}]

            return self.custom_response_on_complete.generate_response(request, processor)

        # create abort URL
        elif request.url == "http://localhost/api/2.0/fs/create-abort-upload-url" and request.method == "POST":
            assert UploadTestCase.is_auth_header_present(request)
            request_json = request.json()
            assert request_json["path"] == self.path
            expire_time = MultipartUploadTestCase.parse_and_validate_expire_time(request_json["expire_time"])

            def processor() -> list:
                response_json = {
                    "abort_upload_url": {
                        "url": server_state.create_abort_url(self.path, expire_time),
                        "headers": [{"name": "header1", "value": "headervalue1"}],
                    }
                }
                return [200, json.dumps(response_json), {}]

            return self.custom_response_on_create_abort_url.generate_response(request, processor)

        # abort upload
        elif request.url.startswith(MultipartUploadServerState.abort_upload_url_prefix) and request.method == "DELETE":
            assert not UploadTestCase.is_auth_header_present(request)
            assert request.url[len(MultipartUploadServerState.abort_upload_url_prefix) :] == self.path

            def processor() -> list:
                server_state.abort_upload()
                return [200, "", {}]

            return self.custom_response_on_abort.generate_response(request, processor)

        # direct upload (single-shot upload)
        elif (
            request_url.hostname == "localhost"
            and request_url.path == f"/api/2.0/fs/files{self.path}"
            and request.method == "PUT"
        ):
            assert MultipartUploadTestCase.is_auth_header_present(request)
            assert request.content is not None

            def processor():
                server_state.file_content = FileContent.from_bytes(request.content)
                return [200, "", {}]

            return self.custom_response_on_upload.generate_response(request, processor)

        return None

    @staticmethod
    def parse_and_validate_expire_time(s: str) -> datetime:
        expire_time = datetime.strptime(s, "%Y-%m-%dT%H:%M:%SZ")
        expire_time = expire_time.replace(tzinfo=timezone.utc)  # Explicitly add timezone
        now = datetime.now(timezone.utc)
        max_expiration = now + timedelta(hours=2)
        assert now < expire_time < max_expiration
        return expire_time

    def __str__(self) -> str:
        return self.name

    @staticmethod
    def to_string(test_case: "MultipartUploadTestCase") -> str:
        return str(test_case)


@pytest.mark.parametrize(
    "test_case",
    [
        # -------------------------- happy cases --------------------------
        MultipartUploadTestCase(
            "Multipart upload successful: single part because of small file",
            content_size=1024 * 1024,  # less than part size
            multipart_upload_min_stream_size=10 * 1024 * 1024,
            source_type=[
                UploadSourceType.FILE,
                UploadSourceType.SEEKABLE_STREAM,
            ],  # non-seekable streams always use multipart upload
            expected_part_size=1024 * 1024,  # chunk size is used
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Multipart upload successful: empty file or empty seekable stream",
            content_size=0,  # content with zero length
            multipart_upload_min_stream_size=100 * 1024 * 1024,  # all files smaller than 100M goes to single-shot
            expected_single_shot_upload=True,
            expected_multipart_upload_aborted=None,
        ),
        MultipartUploadTestCase(
            "Multipart upload successful: multiple parts (aligned)",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            expected_part_size=10 * 1024 * 1024,  # chunk size is used
        ),
        MultipartUploadTestCase(
            "Multipart upload successful: multiple parts (aligned), upload urls by 3",
            multipart_upload_batch_url_count=3,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            expected_part_size=10 * 1024 * 1024,  # chunk size is used
        ),
        MultipartUploadTestCase(
            "Multipart upload successful: multiple chunks (not aligned), upload urls by 1",
            content_size=100 * 1024 * 1024 + 1566,  # 14 full chunks + remainder
            multipart_upload_part_size=7 * 1024 * 1024 - 17,
            expected_part_size=7 * 1024 * 1024 - 17,  # chunk size is used
        ),
        MultipartUploadTestCase(
            "Multipart upload successful: multiple parts (not aligned), upload urls by 5",
            multipart_upload_batch_url_count=5,
            content_size=100 * 1024 * 1024 + 1566,  # 14 full parts + remainder
            multipart_upload_part_size=7 * 1024 * 1024 - 17,
        ),
        MultipartUploadTestCase(
            "Small stream, single-shot upload used",
            content_size=1024 * 1024,
            multipart_upload_min_stream_size=1024 * 1024 + 1,
            expected_multipart_upload_aborted=False,
            expected_single_shot_upload=True,
            use_parallel=[False],
        ),
        # -------------------------- failures on "initiate upload" --------------------------
        MultipartUploadTestCase(
            "Initiate: 400 response is not retried",
            content_size=1024 * 1024,
            multipart_upload_min_stream_size=1024 * 1024,  # still multipart upload is used
            custom_response_on_initiate=CustomResponse(code=400, only_invocation=1),
            expected_exception_type=BadRequest,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: 403 response is not retried",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(code=403, only_invocation=1),
            expected_exception_type=PermissionDenied,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: 500 response is not retried",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(code=500, only_invocation=1),
            expected_exception_type=InternalError,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: non-JSON response is not retried",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(body="this is not a JSON", only_invocation=1),
            expected_exception_type=requests.exceptions.JSONDecodeError,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: meaningless JSON response is not retried",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(body='{"foo": 123}', only_invocation=1),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: no session token in response is not retried",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(
                body='{"multipart_upload":{"session_token1": "token123"}}', only_invocation=1
            ),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: permanent retryable exception",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(exception=requests.ConnectionError),
            expected_exception_type=TimeoutError,  # SDK throws this if retries are taking too long
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        MultipartUploadTestCase(
            "Initiate: intermittent retryable exception",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(
                exception=requests.ConnectionError,
                # 3 calls fail, but request is successfully retried
                first_invocation=1,
                last_invocation=3,
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Initiate: intermittent retryable status code",
            content_size=1024 * 1024,
            custom_response_on_initiate=CustomResponse(
                code=429,
                # 3 calls fail, then retry succeeds
                first_invocation=1,
                last_invocation=3,
            ),
            expected_multipart_upload_aborted=False,
        ),
        # -------------------------- failures on "create upload URL" --------------------------
        MultipartUploadTestCase(
            "Create upload URL: 400 response should fallback",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                code=400,
                # 1 failure is enough
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: 403 response should fallback",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                code=403,
                # 1 failure is enough
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: internal error should fallback",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(code=500, only_invocation=1),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: non-JSON response should fallback",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(body="this is not a JSON", only_invocation=1),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: meaningless JSON response is not retried",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(body='{"foo":123}', only_invocation=1),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: meaningless JSON response is not retried 2",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(body='{"upload_part_urls":[]}', only_invocation=1),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: meaningless JSON response is not retried 3",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                body='{"upload_part_urls":[{"url":""}]}', only_invocation=1
            ),
            expected_exception_type=KeyError,
            expected_multipart_upload_aborted=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: permanent retryable exception should fallback",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(exception=requests.ConnectionError),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: intermittent retryable exception",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                exception=requests.Timeout,
                # happens only once, retry succeeds
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Create upload URL: intermittent retryable exception 2",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                exception=requests.Timeout,
                # 4th request for multipart URLs fails 3 times, then retry succeeds
                first_invocation=4,
                last_invocation=6,
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Create upload URL: intermittent retryable exception 3",
            content_size=1024 * 1024,
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                code=500,
                first_invocation=4,
                last_invocation=6,
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Create upload URL: fallback to single-shot upload when presigned URLs are disabled",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                code=403,
                body=MultipartUploadTestCase.presigned_url_disabled_response,
                # 1 failure is enough
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Create upload URL: fallback to single-shot upload when presigned URLs are not issue because of the NetworkZone is not populated to Filesystem service",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(
                code=500,
                body=MultipartUploadTestCase.model_serving_presigned_url_internal_error_response,
                # 1 failure is enough
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        # -------------------------- failures on part upload --------------------------
        MultipartUploadTestCase(
            "Upload part: 403 response will trigger fallback to single-shot upload on Azure",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                # fail only once
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Upload part: 403 response will trigger fallback to single-shot upload on AWS",
            cloud=Cloud.AWS,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                # fail only once on the first part
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Upload part: fallback to single-shot upload when Azure Firewall denies first part upload",
            cloud=Cloud.AZURE,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                # fail only once on the first part
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Upload part: 403 response on the second part on Azure causes permission denied",
            cloud=Cloud.AZURE,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                # fail only once on the second part
                only_invocation=2,
            ),
            expected_exception_type=PermissionDenied,
            expected_multipart_upload_aborted=True,
            use_parallel=[False],  # "Second part" is not well-defined when using parallel upload
        ),
        MultipartUploadTestCase(
            "Upload part: 400 response is not retried",
            content_size=100 * 1024 * 1024,  # 10 chunks
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=400,
                # fail once, but not on the first part
                only_invocation=3,
            ),
            expected_exception_type=BadRequest,
            expected_multipart_upload_aborted=True,
        ),
        MultipartUploadTestCase(
            "Upload part: expired URL is retried on AWS",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403, body=MultipartUploadTestCase.expired_url_aws_response, only_invocation=2
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Upload part: expired URL is retried on Azure",
            multipart_upload_max_retries=3,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                body=MultipartUploadTestCase.expired_url_azure_response,
                # 3 failures don't exceed multipart_upload_max_retries
                first_invocation=2,
                last_invocation=4,
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Upload part: expired URL is retried on Azure, requesting urls by 6",
            multipart_upload_max_retries=3,
            multipart_upload_batch_url_count=6,
            content_size=100 * 1024 * 1024,  # 100 chunks
            multipart_upload_part_size=1 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                body=MultipartUploadTestCase.expired_url_azure_response,
                # 3 failures don't exceed multipart_upload_max_retries
                first_invocation=2,
                last_invocation=4,
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Upload part: expired URL retry is exhausted",
            multipart_upload_max_retries=3,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                body=MultipartUploadTestCase.expired_url_azure_response,
                # 4 failures exceed multipart_upload_max_retries
                first_invocation=2,
                last_invocation=5,
            ),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=True,
            use_parallel=[False],  # to make "retry is exhausted" well-defined
        ),
        MultipartUploadTestCase(
            "Upload part in parallel: expired URL retry is exhausted",
            multipart_upload_max_retries=3,
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                code=403,
                body=MultipartUploadTestCase.expired_url_azure_response,
                first_invocation=2,  # to exhaust retries for parallel uploading, failure must happen infinitely
            ),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=True,
            use_parallel=[True],  # to make "retry is exhausted" well-defined
        ),
        MultipartUploadTestCase(
            "Upload part: permanent retryable error",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(exception=requests.ConnectionError, first_invocation=8),
            expected_exception_type=RuntimeError,
            expected_multipart_upload_aborted=True,
        ),
        MultipartUploadTestCase(
            "Upload part: permanent retryable status code",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(code=429, first_invocation=8),
            expected_exception_type=RuntimeError,
            expected_multipart_upload_aborted=True,
        ),
        MultipartUploadTestCase(
            "Upload part: intermittent retryable error",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(
                exception=requests.ConnectionError, first_invocation=2, last_invocation=3
            ),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Upload part: intermittent retryable status code 429",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(code=429, first_invocation=2, last_invocation=3),
            expected_multipart_upload_aborted=False,
        ),
        MultipartUploadTestCase(
            "Upload chunk: intermittent retryable status code 502",
            content_size=100 * 1024 * 1024,  # 10 parts
            multipart_upload_part_size=10 * 1024 * 1024,
            custom_response_on_upload=CustomResponse(code=502, first_invocation=2, last_invocation=3),
            expected_multipart_upload_aborted=False,
        ),
        # -------------------------- failures on abort --------------------------
        MultipartUploadTestCase(
            "Abort URL: intermittent retryable error",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(code=403, only_invocation=1),
            custom_response_on_create_abort_url=CustomResponse(code=429, first_invocation=1, last_invocation=3),
            expected_multipart_upload_aborted=True,  # abort successfully called after abort URL creation is retried
            expected_single_shot_upload=True,
        ),
        MultipartUploadTestCase(
            "Abort: exception",
            content_size=1024 * 1024,
            custom_response_on_create_multipart_url=CustomResponse(code=403, only_invocation=1),
            custom_response_on_abort=CustomResponse(
                exception=requests.Timeout,
                # this allows to change the server state to "aborted"
                exception_happened_before_processing=False,
            ),
            expected_multipart_upload_aborted=True,
            expected_single_shot_upload=True,
        ),
        # -------------------------- Parallel Upload for Streams --------------------------
        MultipartUploadTestCase(
            "Multipart parallel upload for stream: Upload errors are not retried but fallback",
            content_size=10 * 1024 * 1024,
            multipart_upload_part_size=1024 * 1024,
            source_type=[UploadSourceType.SEEKABLE_STREAM],
            use_parallel=[True],
            parallelism=1,
            custom_response_on_upload=CustomResponse(
                code=501,
                first_invocation=2,
                last_invocation=4,
                delayed_response_seconds=0.1,
            ),
            expected_exception_type=NotImplemented,
            expected_multipart_upload_aborted=True,
        ),
    ],
    ids=MultipartUploadTestCase.to_string,
)
def test_multipart_upload(config: Config, test_case: MultipartUploadTestCase) -> None:
    test_case.run(config)


class ResumableUploadServerState:
    """This server state is updated on resumable upload (GCP)"""

    resumable_upload_url_prefix = "https://cloud_provider.com/resumable-upload/"
    abort_upload_url_prefix = "https://cloud_provider.com/abort-upload/"

    def __init__(self, unconfirmed_delta: Union[int, list], expected_part_size: Optional[int]):
        self.unconfirmed_delta = unconfirmed_delta
        self.confirmed_last_byte: Optional[int] = None  # inclusive
        self.uploaded_parts = []
        self.session_token = "token-" + MultipartUploadServerState.randomstr()
        self.file_content: Optional[FileContent] = None
        self.aborted = False
        self.expected_part_size = expected_part_size

    def save_part(self, start_offset: int, end_offset_incl: int, part_content: bytes, file_size_s: str) -> None:
        assert not self.aborted

        assert len(part_content) > 0
        if self.expected_part_size is not None:
            assert len(part_content) <= self.expected_part_size

        if self.confirmed_last_byte:
            assert start_offset == self.confirmed_last_byte + 1
        else:
            assert start_offset == 0

        assert end_offset_incl == start_offset + len(part_content) - 1

        is_last_part = file_size_s != "*"
        if is_last_part:
            assert int(file_size_s) == end_offset_incl + 1
        else:
            assert not self.file_content  # last part should not have been uploaded yet

        if isinstance(self.unconfirmed_delta, int):
            unconfirmed_delta = self.unconfirmed_delta
        elif len(self.uploaded_parts) < len(self.unconfirmed_delta):
            unconfirmed_delta = self.unconfirmed_delta[len(self.uploaded_parts)]
        else:
            unconfirmed_delta = self.unconfirmed_delta[-1]  # take the last delta

        if unconfirmed_delta >= len(part_content):
            unconfirmed_delta = 0  # otherwise we never finish

        logger.info(
            f"Saving part {len(self.uploaded_parts) + 1} of original size {len(part_content)} with unconfirmed delta {unconfirmed_delta}. is_last_part = {is_last_part}"
        )

        if unconfirmed_delta > 0:
            part_content = part_content[:-unconfirmed_delta]

        with NamedTemporaryFile(mode="wb", delete=False) as f:
            part_file = f.name
            f.write(part_content)

        self.uploaded_parts.append(part_file)

        if is_last_part and unconfirmed_delta == 0:
            size = 0
            sha256 = hashlib.sha256()
            for part_path in self.uploaded_parts:
                size += os.path.getsize(part_path)
                with open(part_path, "rb") as f:
                    part_content = f.read()
                    sha256.update(part_content)

            assert size == end_offset_incl + 1
            self.file_content = FileContent(size, sha256.hexdigest())

        self.confirmed_last_byte = end_offset_incl - unconfirmed_delta

    def create_abort_url(self, path: str, expire_time: datetime) -> str:
        assert not self.aborted
        self.issued_abort_url_expire_time = expire_time
        return f"{self.abort_upload_url_prefix}{path}"

    def cleanup(self) -> None:
        for file in self.uploaded_parts:
            os.remove(file)

    def get_file_content(self) -> Optional[FileContent]:
        if self.aborted:
            assert not self.file_content

        # content may be None even for a non-aborted upload,
        # in case single-shot upload was used due to small stream size.
        return self.file_content

    def abort_upload(self) -> None:
        self.aborted = True


class ResumableUploadTestCase(UploadTestCase):
    """Test case for resumable upload of a file. Resumable uploads are used on GCP.

    Resumable upload involves multiple HTTP requests:
    - initiating upload (call to Databricks Files API)
    - requesting resumable upload URL (call to Databricks Files API)
    - uploading data in parts (calls to cloud storage provider or Databricks storage proxy)
    - aborting the upload (call to cloud storage provider or Databricks storage proxy)

    Test case uses requests-mock library to mock all these requests. Within a test, mocks use
    shared server state that tracks the upload. Mocks generate the "default" (successful) response.

    Response of each call can be modified by parameterising a respective `CustomResponse` object.
    """

    def __init__(
        self,
        name: str,
        stream_size: int,
        cloud: Cloud = None,
        overwrite: bool = True,
        source_type: Optional[List[UploadSourceType]] = None,
        use_parallel: Optional[List[bool]] = None,
        parallelism: Optional[int] = None,
        multipart_upload_min_stream_size: int = 0,  # disable single-shot uploads by default
        multipart_upload_part_size: Optional[int] = None,
        sdk_retry_timeout_seconds: Optional[int] = None,
        multipart_upload_max_retries: Optional[int] = None,
        # In resumable upload, when replying to part upload request, server returns
        # (confirms) last accepted byte offset for the client to resume upload after.
        #
        # `unconfirmed_delta` defines offset from the end of the part that remains
        # "unconfirmed", i.e. the last accepted offset would be (range_end - unconfirmed_delta).
        # Can be int (same for all parts) or list (individual for each part).
        unconfirmed_delta: Union[int, list] = 0,
        custom_response_on_single_shot_upload: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_create_resumable_url: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_upload: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_status_check: CustomResponse = CustomResponse(enabled=False),
        custom_response_on_abort: CustomResponse = CustomResponse(enabled=False),
        # exception which is expected to be thrown (so upload is expected to have failed)
        expected_exception_type: Optional[Type[BaseException]] = None,
        # if abort is expected to be called
        expected_multipart_upload_aborted: bool = False,
        expected_single_shot_upload: bool = False,
        expected_part_size: Optional[int] = None,
    ):
        super().__init__(
            name,
            stream_size,
            cloud,
            overwrite,
            source_type
            or [UploadSourceType.FILE, UploadSourceType.SEEKABLE_STREAM, UploadSourceType.NONSEEKABLE_STREAM],
            use_parallel
            or [True, False],  # Resumable Upload doesn't support parallel uploading of parts, but fallback should work
            parallelism,
            multipart_upload_min_stream_size,
            multipart_upload_part_size,
            sdk_retry_timeout_seconds,
            multipart_upload_max_retries,
            custom_response_on_single_shot_upload,
            expected_exception_type,
            expected_multipart_upload_aborted,
            expected_single_shot_upload,
        )

        self.unconfirmed_delta = unconfirmed_delta
        self.custom_response_on_create_resumable_url = copy.deepcopy(custom_response_on_create_resumable_url)
        self.custom_response_on_upload = copy.deepcopy(custom_response_on_upload)
        self.custom_response_on_status_check = copy.deepcopy(custom_response_on_status_check)
        self.custom_response_on_abort = copy.deepcopy(custom_response_on_abort)
        self.expected_exception_type = expected_exception_type
        self.expected_part_size = expected_part_size

    def create_multipart_upload_server_state(self) -> ResumableUploadServerState:
        return ResumableUploadServerState(self.unconfirmed_delta, self.expected_part_size)

    def clear_state(self) -> None:
        super().clear_state()
        self.custom_response_on_create_resumable_url.clear_state()
        self.custom_response_on_upload.clear_state()
        self.custom_response_on_status_check.clear_state()
        self.custom_response_on_abort.clear_state()

    def match_request_to_response(
        self, request: requests.Request, server_state: ResumableUploadServerState
    ) -> Optional[requests.Response]:
        request_url = urlparse(request.url)
        request_query = parse_qs(request_url.query)

        # initial request
        if (
            request_url.hostname == "localhost"
            and request_url.path == f"/api/2.0/fs/files{self.path}"
            and request_query.get("action") == ["initiate-upload"]
            and request.method == "POST"
        ):

            assert UploadTestCase.is_auth_header_present(request)
            assert request.text is None

            def processor() -> list:
                response_json = {"resumable_upload": {"session_token": server_state.session_token}}
                return [200, json.dumps(response_json), {}]

            # Different initiate error responses have been verified by test_multipart_upload(),
            # so we're always generating a "success" response.
            return CustomResponse(enabled=False).generate_response(request, processor)

        elif (
            request_url.hostname == "localhost"
            and request_url.path == "/api/2.0/fs/create-resumable-upload-url"
            and request.method == "POST"
        ):

            assert UploadTestCase.is_auth_header_present(request)

            request_json = request.json()
            assert request_json.keys() == {"path", "session_token"}
            assert request_json["path"] == self.path
            assert request_json["session_token"] == server_state.session_token

            def processor() -> list:
                resumable_upload_url = f"{ResumableUploadServerState.resumable_upload_url_prefix}{self.path}"

                response_json = {
                    "resumable_upload_url": {
                        "url": resumable_upload_url,
                        "headers": [{"name": "name1", "value": "value1"}],
                    }
                }
                return [200, json.dumps(response_json), {}]

            return self.custom_response_on_create_resumable_url.generate_response(request, processor)

        # resumable upload, uploading part
        elif request.url.startswith(ResumableUploadServerState.resumable_upload_url_prefix) and request.method == "PUT":

            assert not UploadTestCase.is_auth_header_present(request)
            url_path = request.url[len(ResumableUploadServerState.resumable_upload_url_prefix) :]
            assert url_path == self.path

            content_range_header = request.headers["Content-range"]
            is_status_check_request = re.match("bytes \\*/\\*", content_range_header)
            # Verify that headers from the resumable upload URL response are forwarded
            # on actual data upload requests (status check requests use minimal headers).
            if not is_status_check_request:
                assert request.headers.get("name1") == "value1"
            if is_status_check_request:
                assert not request.body
                response_customizer = self.custom_response_on_status_check
            else:
                response_customizer = self.custom_response_on_upload

            def processor() -> list:
                if not is_status_check_request:
                    body = request.body.read()

                    match = re.match("bytes (\\d+)-(\\d+)/(.+)", content_range_header)
                    [range_start_s, range_end_s, file_size_s] = match.groups()

                    server_state.save_part(int(range_start_s), int(range_end_s), body, file_size_s)

                if server_state.file_content:
                    # upload complete
                    return [200, "", {}]
                else:
                    # more data expected
                    if server_state.confirmed_last_byte:
                        headers = {"Range": f"bytes=0-{server_state.confirmed_last_byte}"}
                    else:
                        headers = {}
                    return [308, "", headers]

            return response_customizer.generate_response(request, processor)

        # abort upload
        elif (
            request.url.startswith(ResumableUploadServerState.resumable_upload_url_prefix)
            and request.method == "DELETE"
        ):

            assert not UploadTestCase.is_auth_header_present(request)
            # Verify that headers from the resumable upload URL response are forwarded.
            assert request.headers.get("name1") == "value1"
            url_path = request.url[len(ResumableUploadServerState.resumable_upload_url_prefix) :]
            assert url_path == self.path

            def processor() -> list:
                server_state.abort_upload()
                return [200, "", {}]

            return self.custom_response_on_abort.generate_response(request, processor)

        return None

    def __str__(self) -> str:
        return self.name

    @staticmethod
    def to_string(test_case: "ResumableUploadTestCase") -> str:
        return str(test_case)


@pytest.mark.parametrize(
    "test_case",
    [
        # ------------------ failures on creating resumable upload URL ------------------
        ResumableUploadTestCase(
            "Create resumable URL: 400 response is not retried and should fallback",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(
                code=400,
                # 1 failure is enough
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: 403 response is not retried and should fallback",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(code=403, only_invocation=1),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: fallback to single-shot upload when presigned URLs are disabled",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(
                code=403, body=MultipartUploadTestCase.presigned_url_disabled_response, only_invocation=1
            ),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: fallback to single-shot upload when presigned URLs are not issued because of the NetworkZone is not populated to Filesystem service",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(
                code=500,
                body=MultipartUploadTestCase.model_serving_presigned_url_internal_error_response,
                only_invocation=1,
            ),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: 500 response is not retried and should fallback",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(code=500, only_invocation=1),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: non-JSON response is not retried",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(body="Foo bar", only_invocation=1),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: meaningless JSON response is not retried",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(
                body='{"upload_part_urls":[{"url":""}]}', only_invocation=1
            ),
            expected_exception_type=ValueError,
            expected_multipart_upload_aborted=False,  # upload didn't start
        ),
        ResumableUploadTestCase(
            "Create resumable URL: permanent retryable status code",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(code=429),
            expected_multipart_upload_aborted=False,  # upload didn't start
            expected_single_shot_upload=True,
        ),
        ResumableUploadTestCase(
            "Create resumable URL: intermittent retryable exception is retried",
            stream_size=1024 * 1024,
            custom_response_on_create_resumable_url=CustomResponse(
                exception=requests.Timeout,
                # 3 failures total
                first_invocation=1,
                last_invocation=3,
            ),
            expected_multipart_upload_aborted=False,  # upload succeeds
        ),
        # ------------------ failures during upload ------------------
        ResumableUploadTestCase(
            "Upload: retryable exception after file is uploaded",
            stream_size=1024 * 1024,
            custom_response_on_upload=CustomResponse(
                exception=requests.ConnectionError,
                # this makes server state change before exception is thrown
                exception_happened_before_processing=False,
            ),
            # Despite the returned error, file has been uploaded. We'll discover that
            # on the next status check and consider upload completed.
            expected_multipart_upload_aborted=False,
        ),
        ResumableUploadTestCase(
            "Upload: retryable exception before file is uploaded, not enough retries",
            stream_size=1024 * 1024,
            multipart_upload_max_retries=3,
            custom_response_on_upload=CustomResponse(
                exception=requests.ConnectionError,
                # prevent server from saving this part
                exception_happened_before_processing=True,
                # fail 4 times, exceeding max_retries
                first_invocation=1,
                last_invocation=4,
            ),
            # File was never uploaded and we gave up retrying
            expected_exception_type=requests.ConnectionError,
            expected_multipart_upload_aborted=True,
        ),
        ResumableUploadTestCase(
            "Upload: retryable exception before file is uploaded, enough retries",
            stream_size=1024 * 1024,
            multipart_upload_max_retries=4,
            custom_response_on_upload=CustomResponse(
                exception=requests.ConnectionError,
                # prevent server from saving this part
                exception_happened_before_processing=True,
                # fail 4 times, not exceeding max_retries
                first_invocation=1,
                last_invocation=4,
            ),
            # File was uploaded after retries
            expected_multipart_upload_aborted=False,
        ),
        ResumableUploadTestCase(
            "Upload: intermittent 429 response: retried",
            stream_size=100 * 1024 * 1024,
            multipart_upload_part_size=7 * 1024 * 1024,
            multipart_upload_max_retries=3,
            custom_response_on_upload=CustomResponse(
                code=429,
                # 3 failures not exceeding max_retries
                first_invocation=2,
                last_invocation=4,
            ),
            expected_multipart_upload_aborted=False,  # upload succeeded
        ),
        ResumableUploadTestCase(
            "Upload: intermittent 429 response: retry exhausted",
            stream_size=100 * 1024 * 1024,
            multipart_upload_part_size=1 * 1024 * 1024,
            multipart_upload_max_retries=3,
            custom_response_on_upload=CustomResponse(
                code=429,
                # 4 failures exceeding max_retries
                first_invocation=2,
                last_invocation=5,
            ),
            expected_exception_type=TooManyRequests,
            expected_multipart_upload_aborted=True,
        ),
        # -------------- abort failures --------------
        ResumableUploadTestCase(
            "Abort: client error",
            stream_size=1024 * 1024,
            # prevent part from being uploaded
            custom_response_on_upload=CustomResponse(code=403),
            # internal server error does not prevent server state change
            custom_response_on_abort=CustomResponse(code=501),
            expected_exception_type=PermissionDenied,
            # abort returned error but was actually processed
            expected_multipart_upload_aborted=True,
        ),
        # -------------- file already exists --------------
        ResumableUploadTestCase(
            "File already exists",
            stream_size=1024 * 1024,
            overwrite=False,
            custom_response_on_upload=CustomResponse(code=412, only_invocation=1),
            expected_exception_type=AlreadyExists,
            expected_multipart_upload_aborted=True,
        ),
        # -------------- success cases --------------
        ResumableUploadTestCase(
            "Multiple parts, zero unconfirmed delta",
            stream_size=100 * 1024 * 1024,
            multipart_upload_part_size=7 * 1024 * 1024 + 566,
            # server accepts all the parts in full
            unconfirmed_delta=0,
            expected_multipart_upload_aborted=False,
            expected_part_size=7 * 1024 * 1024 + 566,  # chunk size is used
        ),
        ResumableUploadTestCase(
            "Multiple small parts, zero unconfirmed delta",
            stream_size=100 * 1024 * 1024,
            multipart_upload_part_size=100 * 1024,
            # server accepts all the parts in full
            unconfirmed_delta=0,
            expected_multipart_upload_aborted=False,
            expected_part_size=100 * 1024,  # chunk size is used
        ),
        ResumableUploadTestCase(
            "Multiple parts, non-zero unconfirmed delta",
            stream_size=100 * 1024 * 1024,
            multipart_upload_part_size=7 * 1024 * 1024 + 566,
            # for every part, server accepts all except last 239 bytes
            unconfirmed_delta=239,
            expected_multipart_upload_aborted=False,
            expected_part_size=7 * 1024 * 1024 + 566,  # chunk size is used
        ),
        ResumableUploadTestCase(
            "Multiple parts, variable unconfirmed delta",
            stream_size=100 * 1024 * 1024,
            multipart_upload_part_size=7 * 1024 * 1024 + 566,
            # for the first part, server accepts all except last 15Kib
            # for the second part, server accepts it all
            # for the 3rd part, server accepts all except last 25000 bytes
            # for the 4th part, server accepts all except last 7 Mb
            # for the 5th part onwards server accepts all except last 5 bytes
            unconfirmed_delta=[15 * 1024, 0, 25000, 7 * 1024 * 1024, 5],
            expected_multipart_upload_aborted=False,
            expected_part_size=7 * 1024 * 1024 + 566,  # chunk size is used
        ),
        ResumableUploadTestCase(
            "Small stream, single-shot upload used",
            stream_size=1024 * 1024,
            multipart_upload_min_stream_size=1024 * 1024 + 1,
            expected_multipart_upload_aborted=False,
            expected_single_shot_upload=True,
        ),
    ],
    ids=ResumableUploadTestCase.to_string,
)
def test_resumable_upload(config: Config, test_case: ResumableUploadTestCase) -> None:
    test_case.run(config)


@dataclass
class CreateDownloadUrlResponseTestCase:
    data: Dict[str, Any]
    expected_parsed_url: Optional[str] = None
    expected_parsed_headers: Optional[Dict[str, str]] = None
    expected_exception: Optional[Type[BaseException]] = None

    def run(self) -> None:

        if self.expected_exception:
            with pytest.raises(self.expected_exception):
                CreateDownloadUrlResponse.from_dict(self.data)
        else:
            response = CreateDownloadUrlResponse.from_dict(self.data)
            assert response.url == self.expected_parsed_url
            assert response.headers == (self.expected_parsed_headers or {})

    def __str__(self) -> str:
        return f"CreateDownloadUrlResponseTestCase(data={self.data})"


@pytest.mark.parametrize(
    "test_case",
    [
        CreateDownloadUrlResponseTestCase(
            data={
                "url": "https://example.com/download",
                "headers": [
                    {"name": "Authorization", "value": "Bearer token123"},
                    {"name": "Content-Type", "value": "application/octet-stream"},
                ],
            },
            expected_parsed_url="https://example.com/download",
            expected_parsed_headers={
                "Authorization": "Bearer token123",
                "Content-Type": "application/octet-stream",
            },
        ),
        CreateDownloadUrlResponseTestCase(
            data={"url": "https://example.com/download"},
            expected_parsed_url="https://example.com/download",
            expected_parsed_headers={},
        ),
        CreateDownloadUrlResponseTestCase(
            data={"url": "https://example.com/download", "headers": []},
            expected_parsed_url="https://example.com/download",
            expected_parsed_headers={},
        ),
        CreateDownloadUrlResponseTestCase(
            data={"headers": [{"name": "Content-Type", "value": "application/octet-stream"}]},
            expected_exception=ValueError,
        ),
    ],
    ids=str,
)
def test_create_download_url_response(test_case: CreateDownloadUrlResponseTestCase) -> None:
    """Run a test case for CreateDownloadUrlResponse."""
    test_case.run()


def fast_random_bytes(n: int, chunk_size: int = 1024) -> bytes:
    # Generate a small random chunk
    chunk = os.urandom(chunk_size)
    # Repeat it until we reach n bytes
    return (chunk * (n // chunk_size + 1))[:n]
